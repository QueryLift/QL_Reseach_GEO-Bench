# モデル評価・品質・ベンチマーク

## 概要


### 基本姿勢
OpenAIは、モデル評価・品質・ベンチマークを「学習前・学習中・展開後」にわたる連続的なプロセスとして設計し、Preparedness Frameworkに基づく多層評価と外部独立評価を統合して運用します。OpenAIは、Evaluations HubとSystem Cardで最新のスナップショットを透明に公開し、Model Spec準拠度や「正解・誤答・棄権」を区別する評価観でハルシネーションを抑制しつつ、エージェントやスキャフォールド、マルチモーダルまで含めた実運用前提の測定を徹底します。

### 重点的取り組み
OpenAIは、SWE‑bench Verified・HealthBench・IndQA・GDPvalなどの厳密な新規ベンチマークとオープンソースのOpenAI Evalsを組み合わせ、実世界の価値と安全性を測る評価基盤を拡充します。OpenAIは、METRやSecureBioを含む第三者テストを制度化し、初期チェックポイントや思考の連鎖アクセスを伴う安全な評価環境を提供、評価結果をEvaluations Hub（2025年8月15日更新、GPT‑5系やProduction Benchmarksを追加）で継続更新します。OpenAIは、推論モデルでは思考過程に安全規則を教示し、Soraなどマルチモーダルでもヌード・選挙欺瞞・自傷・暴力の内部評価と緩和策運用を一体で強化します。

### 重要事実
OpenAIは、SWE‑bench VerifiedでGPT‑4oのスコアを16%から33.2%へ改善し、同一モデルでもスキャフォールド差で2.7%〜28.3%と大きく変動する事実を検証、再現性のため検証済み固定サブセット（n=477）で運用しています。OpenAIは、o1‑previewで有害プロンプトへの安全なコンプリーション率を全体0.995、困難ケース0.934に引き上げ、評価過程での報酬ハッキング観測を含めSystem Cardで公開しました。OpenAIは、HealthBench（60か国・262名医師、5,000対話・4万超ルーブリック）で最新モデルの大幅改善とモデルグレーダーの妥当性を確認し、2025年10月の更新でメンタルヘルス等の不適切応答を実運用で65〜80%削減しました。


## ベンチマーク設計・公開
ベンチマーク設計・公開は、OpenAIにおいてモデル評価の信頼性と透明性を高める中核戦略であり、評価ツールとデータセットを可能な限りオープンに提供し、研究コミュニティと協働しながら継続的に改善します[1][2]。評価基盤としてオープンソースのOpenAI Evalsを公開し、モデルの欠陥特定から性能劣化の防止まで開発ライフサイクル全体で活用できる仕組みを提供しています。Evalsは、モデルバージョン間の性能追跡や製品への統合変更にも適用でき、独自の評価ロジックを実装できる拡張性と、モデル自身が出力を採点する「モデルグレード evals」など社内で有用だったテンプレートを備えています。実運用ではStripeがドキュメントツールの精度測定において人手評価の補完としてEvalsを活用しました。今後も、将来のシステムに何を期待できるかを社会に明確化するため、評価手法の発展を推進します[1]。

ベンチマーク設計・公開は、課題の妥当性と明確性を重視した設計を徹底しています。SWE‑bench Verifiedでは、既存SWE‑benchから人手検証を経た500サンプルを抽出し、テスト妥当性と問題記述の明確さを保証しました。その結果、GPT‑4oの性能は元の16%から33.2%へと大幅に向上しました。さらに、同一モデルでもエージェントのスキャフォールドによって結果が大きく変動することを示し、SWE‑bench LiteではRAGベースで2.7%、CodeRで28.3%と大きな差が生じる事例を提示しました。こうした知見を踏まえ、Preparedness Frameworkに基づき、学習前・学習中・学習後（外部システム統合後）に至るまで必要な回数だけ継続的に評価し、外部強化の影響を評価設計に組み込みます。同時に、静的データセット評価にはデータ汚染などの限界があることを認識し、他の評価での補完とコミュニティとの協力を進めます[2]。

ベンチマーク設計・公開は、評価プロトコルの透明性も重視します。GPT‑4の公開評価では、4‑shotのコンテキスト内学習とChain‑of‑Thoughtプロンプトを用い、プロンプトは検証セットで調整する手法を採用したことを明確にしました[1]。

言語・文化的多様性を適切に捉えるため、ベンチマーク設計・公開はインドの12言語にまたがる文脈的・文化的理解と推論を測るIndQAを公開しました。261名の専門家と協働して2,278問を作成し、ルーブリックに基づく評価で能力向上を追跡可能としています。既存ベンチマークで十分扱われていない領域に対し、IndQA形式が有用であり、本公開が新たなベンチマーク開発の触媒になることを期待しています[3]。

安全性領域でも、ベンチマーク設計・公開は評価設計と公開を徹底します。新しい推論手法（思考の連鎖）を適用する際はPreparedness Frameworkに従ってデプロイ前に安全性テストとレッドチーミングを実施し、主要なジェイルブレイク評価や社内の最難関ベンチマークでの結果をSystem Cardに公開します。安全規則に関する推論の堅牢性向上など、評価結果を通じた透明性を継続的に確保します[4]。

新しいマルチモーダル推論モデルの検証では、人間用試験と多様なMLベンチマークを組み合わせ、「高い論理的思考努力」の条件で一貫した評価を行います。o3とo4‑miniは、STEM問題解答（MMMU、MathVista）、図表の読み取りと論理的思考（CharXiv）、知覚プリミティブ（VLMs are Blind）、視覚探索（V*）などで最先端性能を達成しました。同時に、推論チェーンの冗長化、知覚誤り、試行間の一貫性などの限界を明確にし、次の設計改善に反映します[5]。

ベンチマーク設計・公開は今後も、オープンソースの評価フレームワーク、厳密に設計・検証されたベンチマーク、そしてPreparedness Frameworkに基づく継続評価を組み合わせ、モデル品質と安全性の可視化を進めます。コミュニティとの協働を通じて、実世界で重要な課題領域を広くカバーする高品質なベンチマークの設計・公開を加速します[1][2][3]。

【出典】
[1] https://openai.com/ja-JP/index/gpt-4-research/
[2] https://openai.com/ja-JP/index/introducing-swe-bench-verified/
[3] https://openai.com/ja-JP/index/introducing-indqa/
[4] https://openai.com/ja-JP/index/learning-to-reason-with-llms/
[5] https://openai.com/ja-JP/index/thinking-with-images/


## モデルの学習・デフォルト展開
モデルの学習・デフォルト展開は、「能力は大規模な事前学習で獲得し、振る舞いは人間中心の評価と強化学習で整合させ、デフォルトの行動原則とデータ利用の既定値を明確にする」という公式見解を掲げています。モデルの学習・デフォルト展開は、公開情報やライセンス許諾データに加え、人間の評価データを用いた次語予測による大規模事前学習をまず実施し、続いてChatGPTユーザーや人間トレーナーからのフィードバックを活用した強化学習（RLHF）で挙動をユーザー意図に沿うよう整合させます。多くの能力は事前学習で形成され、RLHFは主に整合性を高める役割を担う一方、試験成績が必ずしも向上するとは限らないため、必要に応じてプロンプトエンジニアリングを併用します。また、個人向け製品の利用データ（ChatGPTやDALL·Eなど）は学習に含まれる場合がありますが、2023年3月1日以降のChatGPT Business・Enterprise・EduおよびAPIのビジネスデータは、ユーザーが明示的にオプトインしない限り学習に用いないことをデフォルトとしています[6][8]。

モデルの学習・デフォルト展開は、スケールに伴う一貫した品質確保のために「予測可能な拡張性」を重視し、モデル固有の大規模微調整に過度に依存しない学習設計を進めてきました。GPT‑4の開発では、1/10,000規模の小型モデルから最終損失を高精度に予測する手法を確立し、スケール間で整合的な挙動を実現するインフラと最適化を検証しています[6]。さらに、o3/o4 miniの研究開発を通じて、強化学習でも「計算量や推論時間をスケールさせるほど性能が向上する」傾向を確認し、ツールの使用自体と「いつ・なぜ使うか」という意思決定も学習させました。評価はChatGPTの高い推論努力設定（o4‑mini‑high相当のバリエーションを含む）で行い、SWE‑benchでは検証済み477タスクの固定サブセットを用いるなど、再現性ある手順を採用しています[7]。

モデルの学習・デフォルト展開は、モデルのデフォルトの振る舞いを定める「Model Spec」を公開し、知的自由と透明性を重視した目的・ルール・デフォルトをCC0で提示しています。これはRLHFに取り組む研究者やAIトレーナー向けガイドラインとして機能し、将来的に指針から直接学習する可能性も探究します。同時に、指針は公開の議論として位置づけ、グローバルなステークホルダーと協議しながら継続的に改善します。加えて、Model Spec準拠度を測る挑戦的な評価プロンプト群を整備し、現実世界シナリオを含む専門家レビューで進捗を継続測定しています。昨年5月時点の最良システム比で、最新モデルは準拠度が大幅に改善しており、方針更新の効果もあるものの、主因はアライメントの強化にあると考えています。モデルの学習・デフォルト展開は、重大な危害につながる行為を許容しないガードレールを維持しつつ、デリケートな問いにも思慮深く応答する振る舞いを推奨します[1][2]。

モデルの学習・デフォルト展開は、品質劣化の主要因であるハルシネーションに対処するため、評価の「スコアボード」を見直し、不確かさの適切な表明や必要に応じた棄権を正しく評価する方針を導入しました。次語予測という統計的設定には不可避誤差が内在し、誤りの根絶が困難であることを踏まえ、評価法の改善で「推測を学習してしまう」力学を抑制し、事実性を高める学習とデフォルト行動の確立を加速します[4]。

モデルの学習・デフォルト展開は、解釈可能性の向上を学習段階から追求しています。スパース回路に基づく手法により、性能を維持しながらモデルをより解釈しやすく訓練できる可能性を示し、スケーラブルな監視、敵対的学習、レッドチーミングなどの安全性取り組みを補完します。機械論的解釈可能性は、モデル振る舞いのより完全な説明に近づく道筋を提供し、早期警告や監視強化の基盤として重視しています[3]。

モデルの学習・デフォルト展開は、整合性を高めたモデルのデフォルト展開を通じて、より良い初期体験を提供してきました。InstructGPTをAPIのデフォルトとして展開し、人間評価ではGPT‑3より強く好まれること、TruthfulQAで虚偽模倣が減少し、RealToxicityPromptsで有害性が低減したこと、顧客分布における人間評価でもハルシネーション頻度の低下と適切性の向上が見られることを確認しています。顧客分布での比較においても、InstructGPTの出力はFLANやT0より好まれました。API配布では、有害出力（暴力・性的内容、保護属性への中傷、虐待の助長など）を継続的に測定し、学習データの管理（個人情報の削除等）を徹底します[5]。

モデルの学習・デフォルト展開は、訓練前のデータ設計と検証でも品質・安全性を高めています。DALL·E 2では、能動学習と最近傍探索を組み合わせて偽陰性を低減し、未ラベルの大規模コレクションから知覚特徴空間で近傍を抽出・ラベリングする手順を多数GPUでスケールして分単位で実行しました。同一ハイパーパラメータで学習したGLIDEのフィルター前後比較により、過激コンテンツ生成頻度の減少と、人口統計に関するバイアスの副作用も検証・特定しています[9]。

モデルの学習・デフォルト展開は、エンタープライズや教育向け製品におけるお客様データの所有権と管理権を尊重し、前述のとおり、2023年3月1日以降のビジネスデータは明示的なオプトインがない限りモデル学習に使用しない方針を維持します[8]。

【出典】
[1] https://openai.com/ja-JP/index/introducing-the-model-spec/
[2] https://openai.com/ja-JP/index/sharing-the-latest-model-spec/
[3] https://openai.com/ja-JP/index/understanding-neural-networks-through-sparse-circuits/
[4] https://openai.com/ja-JP/index/why-language-models-hallucinate/
[5] https://openai.com/ja-JP/index/instruction-following/
[6] https://openai.com/ja-JP/index/gpt-4-research/
[7] https://openai.com/ja-JP/index/introducing-o3-and-o4-mini/
[8] https://openai.com/ja-JP/enterprise-privacy/
[9] https://openai.com/ja-JP/index/dall-e-2-pre-training-mitigations/


## モデル利用における評価データ活用
モデル利用における評価データ活用は、モデルを現実世界で安全かつ有用に活用するための中核的な資産として、設計・体系的収集・分析・公開・外部検証を一体で進め、その知見を製品、モデル、ポリシー、ガードレール設計、モデレーション基準、透明性へ直接反映します。モデル利用における評価データ活用は、実利用からのフィードバック、専門家査読、第三者独立評価、領域特化ベンチマークを組み合わせて能力とリスクを継続的に測定・改善し、その進捗をEvaluations Hub等で共有するとともに、評価科学の「飽和」に対処するため評価手法そのものも更新し続けます[2][3]。Evaluations Hubでは、不許可コンテンツの細分類やジェイルブレイク（StrongRejectや人間由来プロンプト）、ハルシネーション（SimpleQA、PersonQA）などの評価をモデル横断で公開し、内訳サブ指標やProduction Benchmarksまで含めて最新モデルの評価を提供します。あわせて、公開結果は時点のスナップショットであることを明示し、System CardやPreparedness Frameworkと併せた総合的理解を促します[2]。

モデル利用における評価データ活用は、プロダクトの実利用から得られる評価データを直接的な改善に結びつけます。Soraの早期アクセスでは、60か国以上・300人超のユーザーから50万件超のフィードバックを収集し、モデル挙動や安全プロトコル順守の改善に活用しました。アーティストの評価から、目に見える透かしが制作ワークフローを阻害するという知見を得て、C2PAメタデータの埋め込みは維持しつつ、有料ユーザーに透かしなしのダウンロードを提供する製品判断につなげています。さらに、リスクの高い領域を明確化する過程で、プロンプトフィルタリング、ブロックリスト、分類器の閾値を調整し、ヌード、選挙関連の欺瞞、自傷、暴力などの主要領域を含む内部評価に基づいて安全基準への適合を強化しました[1]。

モデル利用における評価データ活用は、センシティブ領域における実務的な評価データを整備し、モデル利用の安全性を高めます。医療分野ではHealthBenchを策定し、60か国の医師が作成したルーブリックと対話データで測定した結果、モデルベースのグレーダーと医師の評価が医師間一致率と同程度で一致することをメタ評価で確認しました。評価データと手法は公開し、文脈把握や最悪時の信頼性などの残る課題を特定し、今後の改善に活かしています[4]。メンタルヘルス領域では、専門の臨床家による評価で一致率がケースにより71〜77%となることを踏まえ、Global Physician Networkと連携したターゲット評価を内製し、測定ツールと分類体系を継続的に発展させています。安全検出については精度と再現率のトレードオフを明示し、実利用で有用な再現率を得るための設計判断を評価データに基づいて行います[5]。

モデル利用における評価データ活用は、モデル行動の指針であるModel Specへの準拠度を、挑戦的な評価プロンプトと人間の専門家レビューを組み合わせて計測します。初期結果では、昨年5月時点の最高システムに比べ準拠が大幅に改善しており、現実世界の利用から得られる新規ケースを課題セットに取り込み続けることで、評価データをモデル運用の実態に近づけています。透明性とガードレールを両立させながら、評価に基づくモデルの振る舞いの改善を継続します[6]。

モデル利用における評価データ活用は、外部の独立評価を積極的に受け入れ、評価データの質と多様性を高めます。METRのtime horizon評価やSecureBioのVCT評価など、第三者ベンチマークをPreparedness Frameworkに基づく社内評価と補完的に活用するため、初期チェックポイントへの安全なアクセス、選択した評価結果の共有、ゼロデータ保持、緩和策の少ないモデル、さらには思考の連鎖（Chain-of-Thought）への直接アクセスも提供し、サンドバッギングやスキーミングの兆候を検出可能にしています。これらのアクセスは厳格なセキュリティ管理のもとで運用し、モデル能力やテストニーズに応じて更新します。加えて、安全性評価の一部はEvaluations Hubで公開し、評価科学の進化に合わせて指標・手法を更新します[2][3]。

モデル利用における評価データ活用は、エージェントやスキャフォルディングなど外部強化がモデル挙動に与える影響を評価設計に織り込みます。SWE‑bench Verifiedでは、テストサンプルを人手で検証して評価の妥当性を高めるとともに、スキャフォールドの違いで同一モデルのパフォーマンスが大きく変動する事実（例：SWE‑bench LiteにおけるGPT‑4の2.7%〜28.3%）を示しました。Preparedness Frameworkでは、学習前・学習中・学習後にわたる継続的な評価と、外部システム統合を含むシナリオでの能力変化の特定を求め、静的データセットに内在する汚染やカバレッジの限界を明示して補完的評価の必要性をコミュニティと共有しています[7][3]。

モデル利用における評価データ活用は、経済的価値に直結する実務タスクでの評価データを拡充します。GDPvalは、米国GDPの主要産業に紐づく44職種から抽出した1,320の実世界タスクで性能を測り、成果物を専門家が評価します。学術ベンチマークから応用的・市場ベースの評価へと焦点を広げ、実務さながらのタスクに基づくエビデンスで将来のAI向上を議論し、長期的な進歩を追跡できる枠組みを提供します[8]。

モデル利用における評価データ活用は、推論過程そのものを評価データとして活用し、安全性とアライメントを強化します。o1‑previewでは、思考の連鎖に安全規則と推論方針を組み込むことで、主要なジェイルブレイク評価や社内の困難な安全性ベンチマークで大幅な性能向上を確認しました。デプロイ前にはPreparedness Frameworkに従って安全性テストとレッドチーミングを実施し、報酬ハッキングの事例も観測しています。推論の可視化は分布外シナリオでも堅牢な安全推論を促し、評価データとしても有用であることを見出しています。詳細はSystem Cardに掲載しています[9]。

モデル利用における評価データ活用は、評価データを単なる測定結果ではなく、運用・設計・ガバナンスを駆動する循環的な知見と位置づけます。実利用や専門家コミュニティ、外部評価者からのデータを継続的に取り込み、緩和策の較正、モデル仕様の更新、製品判断の最適化に反映します。同時に、測定ツールが進化することで過去との単純比較が難しくなる点も透明に説明しつつ、方向性と進捗を追跡する評価基盤を強化し続けます[1][2][5][7]。

【出典】
[1] https://openai.com/ja-JP/index/sora-system-card/
[2] https://openai.com/ja-JP/safety/evaluations-hub/
[3] https://openai.com/ja-JP/index/strengthening-safety-with-external-testing/
[4] https://openai.com/ja-JP/index/healthbench/
[5] https://openai.com/ja-JP/index/strengthening-chatgpt-responses-in-sensitive-conversations/
[6] https://openai.com/ja-JP/index/sharing-the-latest-model-spec/
[7] https://openai.com/ja-JP/index/introducing-swe-bench-verified/
[8] https://openai.com/ja-JP/index/gdpval/
[9] https://openai.com/ja-JP/index/learning-to-reason-with-llms/


## モデル品質評価システム
モデル品質評価システムは、モデルの品質と安全性を一体として測定・改善するための多層アーキテクチャを構築し、評価科学の進化に合わせて手法を継続的に刷新しながら、その進捗の一部を評価ハブで公開します。不許可コンテンツ、ジェイルブレイク、ハルシネーションなどの重要領域を対象に、System CardやPreparedness Frameworkに基づく評価結果とも接続し、学術型から応用・市場ベースの評価へと比重を移しつつ、新しいモダリティや新たに出現するリスクに適応できるスケーラブルな評価を設計します。モデル品質評価システムは、日常の知識業務に直結する能力の向上を長期で追跡するGDPvalのような枠組みを拡充し、評価結果の透明性確保と再現性の向上に努めます[1][2][4][5][10]。

モデル品質評価システムは、モデル行動の方針であるModel Specへの準拠度を測るため、モデル生成と人間の専門家レビューを組み合わせた挑戦的な評価プロンプト群を整備し、一般的な場面から複雑な場面までを包含して運用しています。予備結果では、前年5月時点の当社最高システムと比べてModel Spec準拠が大幅に改善しており、ポリシー更新の影響もある一方で、主にアライメント強化の効果と評価しています。モデル品質評価システムは、現実世界で見つかる新たなケースを常時取り込み、継続的プロセスとしてアップデートを続けます[3]。

モデル品質評価システムは、推論過程に安全規則と文脈的適用を教示する手法で、安全性とアライメントを推論モデルの評価に組み込み、分布外シナリオへの堅牢性を高めています。主要なジェイルブレイク評価や社内の困難な安全性ベンチマークで大幅な性能向上を確認し、有害プロンプトに対する安全なコンプリーションの全体率は0.995（GPT‑4oは0.990）、困難ケースでは0.934（GPT‑4oは0.714）へと改善、カテゴリ別でもハラスメント（重度）0.900、搾取的な性的コンテンツ0.949、未成年者を含む性的コンテンツ0.931などの水準を達成しています。また、困難ケース全体で0.714から0.934、搾取的な性的コンテンツで0.483から0.949への改善も確認しています。モデル品質評価システムは、Preparedness Frameworkに基づくデプロイ前の安全テストとレッドチーミングを徹底し、評価過程で報酬ハッキングの事例も観測しています[5]。

モデル品質評価システムは、安全性の重大領域に対する内部評価を確立し、緩和策の強化とモデレーション基準設定に役立てています。生成動画モデルSoraでは、ヌード、選挙に関する欺瞞的コンテンツ、自傷行為、暴力を対象にした評価を実施し、60カ国以上・300人超からの早期アクセスでの50万件超のフィードバックをもとに、プロンプトフィルタリング、ブロックリスト、分類器の閾値を調整するなど、評価と緩和策を一体で運用しています[4]。

モデル品質評価システムは、センシティブな会話領域（メンタルヘルス、自傷・自殺、AIへの感情的依存など）で、Model Specに沿った目標を明確化し、問題の定義、測定（評価・実会話データ・ユーザーリサーチ）、アプローチの検証（外部専門家レビュー）、リスク軽減のステップでChatGPTの応答を改善します。長年の自傷・自殺に関するベースラインに加えて、感情的依存や自殺を伴わない精神衛生上の緊急事態も安全テストの標準セットに段階的に組み込みます。さらに、Global Physician Networkと連携したターゲット評価を社内評価に適用し、臨床医の評価一致率はケースにより71〜77%を確認、精度と再現率のトレードオフを運用指標として活用しています[6]。

モデル品質評価システムは、医療分野に特化したHealthBenchを公開し、60か国の262名の医師が作成した5,000の対話と4万超のルーブリックで、緊急時の案内（トリアージ）、専門性に応じたコミュニケーション、不確実性への対応、回答の深さ、健康データタスクなど、医療固有の安全・有用性を精密に評価します。モデルベースの自動グレーダーの判断が医師の評価と同程度に一致するメタ評価も実施し、データと評価を公開することで影響の大きい環境におけるモデル挙動の把握に資しています[7]。

モデル品質評価システムは、評価がモデル単体の能力だけでなく外部スキャフォールドによって大きく変動し得る事実を重視し、SWE‑bench Liteでは同一モデル（GPT‑4）でも初期RAG型スキャフォールドで2.7％、CodeRでは28.3％と大きな差が出ることを確認しました。Preparedness Frameworkに基づき、学習前・学習中・学習後や外部システム統合後まで含めて反復評価を行い、非自明な能力変化を捉えます。あわせて、SWE‑bench Verifiedでは人手で検証した500サンプルに、問題記述の明確さやテスト妥当性のラベルを付与し、FAIL_TO_PASSとPASS_TO_PASSの基準で合否を判定するなど、課題仕様と採点の正当性を保証する評価基盤を整備しています。静的データセット評価の限界（事前学習データ汚染やカバー範囲の制約）を踏まえ、研究者コミュニティと協力して信頼できる高品質な評価を継続的に構築します[8]。

モデル品質評価システムは、フロンティアAIに対する外部テストを評価体系に組み込みます。独立評価、方法論レビュー、専門家（SME）によるプロービングを通じて能力と関連する安全対策を多面的に測定し、ChatGPT Agentや次世代モデルに対してhelpful‑onlyモデルを用いたバイオ分野のエンドツーエンドシナリオで初心者の能力向上度を専門家基準で評価するなど、第三者の厳密性と多様な視点を意思決定に反映します。評価結果はシステムカードへの展開前評価サマリーの掲載や、機密性・正確性のレビュー後の外部研究公開支援を通じて透明性を確保します。また、定期的に評価できる資格を持つ多様な外部組織を支援し、信頼できるアクセスと透明性のバランスのもと、レジリエントな安全性エコシステムの構築を推進します[9]。

モデル品質評価システムは、実利用では正確性の上限が100%未満であるという前提に立ち、ハルシネーション評価のスコアリングを再設計します。自信過剰な誤答には不確実性より大きなペナルティを課し、適切な不確実性表明には部分点を与えるなど、推測を抑制する評価を広く採用し、モデルが不確実時に棄権・保留を選べる設計と、ハルシネーション削減手法の普及を後押しします[10]。

モデル品質評価システムは、GDPvalでの実世界タスク評価（米国GDP上位9産業・44職種に基づく1,320タスクを専門家が査定）をはじめ、SWE‑Benchや市場ベースの評価、ドメイン専門家によるレビュー、外部テストを統合し、現実世界における有用性と安全性の両立を測りながら、評価ハブを通じて透明性をもって進歩を共有していきます[1][2][9]。

【出典】
[1] https://openai.com/ja-JP/safety/evaluations-hub/
[2] https://openai.com/ja-JP/index/gdpval/
[3] https://openai.com/ja-JP/index/sharing-the-latest-model-spec/
[4] https://openai.com/ja-JP/index/sora-system-card/
[5] https://openai.com/ja-JP/index/learning-to-reason-with-llms/
[6] https://openai.com/ja-JP/index/strengthening-chatgpt-responses-in-sensitive-conversations/
[7] https://openai.com/ja-JP/index/healthbench/
[8] https://openai.com/ja-JP/index/introducing-swe-bench-verified/
[9] https://openai.com/ja-JP/index/strengthening-safety-with-external-testing/
[10] https://openai.com/ja-JP/index/why-language-models-hallucinate/


## モデル安全性評価
モデル安全性評価は、OpenAIのモデル安全性評価を能力やリスクの進化に合わせて継続的に更新し、スケーラブルかつ透明性の高い方法で公開することを公式方針として推進します。従来手法が「飽和」して差が見えにくくなる課題を踏まえ、モデル安全性評価は新しいモダリティや新規に出現するリスクに対応する評価手法を定期的に見直し、評価結果の一部をEvaluations Hubで共有することで、当社システムの長期的な安全性パフォーマンスの理解とコミュニティ全体の透明性向上に貢献します。ここに掲載する結果はスナップショットであり、包括的な把握には各モデルのSystem CardやPreparedness Frameworkの評価、モデルごとの研究公開も併せて参照いただくことを推奨します[1]。

モデル安全性評価は、モデル行動の指針であるModel Specの改定に合わせ、評価においてもこの指針への準拠度を測ることを重視します。現実世界のパフォーマンス理解を深めるため、各原則に対する準拠をテストする挑戦的なプロンプトを、モデル生成と人間の専門家レビューの組み合わせで収集し、昨年5月時点の当社最高のシステムと比べて準拠が大幅に改善していることを予備結果として確認しています。モデル安全性評価は、この課題セットを今後も拡充し、準拠度評価を継続的に進化させます[2]。あわせて、モデル安全性評価はセンシティブな会話領域での安全性評価を優先課題とし、1) メンタルヘルス（精神病や躁状態など）の懸念、2) 自傷・自殺行為、3) AIへの感情的依存に焦点を当て、長年の自傷・自殺に関する安全指標のベースラインに加えて、感情的依存や自殺を伴わない精神衛生上の緊急事態を将来のモデルリリースに向けた標準的な安全性テストのベースラインに組み込みました。改訂したModel Specでは、ユーザーの現実世界の関係性をサポート・尊重すること、精神的・感情的苦痛に関連しそうな根拠のない信念を肯定しないこと、潜在的な妄想や躁状態の兆候に安全かつ共感的に対応すること、そして自傷・自殺リスクを示す間接的なシグナルに一層注意を払うことを明確化しています。2025年10月のモデル更新では、精神疾患、自傷行為・自殺、AIへの感情的依存といった優先領域に関して不適切な応答を65～80％削減し、精神科医など170名以上の専門家との協働で、危害のマッピング、評価・実会話データ・ユーザーリサーチによる測定、外部専門家による定義・ポリシーの検証、リスク軽減というステップで評価と改善を行いました[4]。

モデル安全性評価は、敵対的プロンプトや拒否境界の評価を重視し、推論中に安全規則を守る能力の向上を検証しています。思考の連鎖（Chain of Thought）を活用する推論モデルでは、安全規則とその文脈的な推論方法を思考の連鎖に組み込むことで、主要なジェイルブレイク評価や当社内で最も困難と位置づける安全性拒否境界ベンチマークで大幅な性能向上を確認しました。推論特化モデルo1‑previewでは、有害なプロンプトに対する安全なコンプリーションが全体で0.995、困難なケース（ジェイルブレイクおよびエッジケース）で0.934を達成し、重度ハラスメント、搾取的性的コンテンツ、未成年者を含む性的コンテンツなどのカテゴリでも高いスコアを示しました。また、デプロイ前にはPreparedness Frameworkに基づく一連の安全性テストとレッドチーミングを実施し、評価過程で観測した報酬ハッキングの事例などの知見をSystem Cardで公表しています[3][5]。

モデル安全性評価は、外部の独立評価を通じて安全性エコシステムを強化します。METRのtime horizon評価やSecureBioのVirology Capabilities Troubleshooting（VCT）評価などを用いた独立評価、方法論レビュー、SME（Subject Matter Expert）プロービングの三つの形式で能力とリスクを測定しました。これを支援するため、初期のモデルチェックポイントへの安全なアクセス、能力向上の様子が分かる選択された評価結果、必要に応じたゼロデータ保持、緩和策が少ないモデルの提供などを行い、一部の組織には思考の連鎖への直接アクセスも提供して、通常では検出が困難なサンドバッギングやスキーミングの事例を特定しました。これらのアクセスはセキュリティ管理の下で提供しており、モデル能力やテストニーズの進化に応じて管理手段を更新し、GPT‑5にも適用しています[5]。

モデル安全性評価は、評価結果の公開と比較を容易にするためEvaluations Hubを運用し、不許可コンテンツ（許可されていないコンテンツ）、ジェイルブレイク、ハルシネーション、命令階層といったテキストベースの評価カテゴリを提示しています。2025年8月15日には、GPT‑5系の結果やProduction Benchmarks、StrongRejectの詳細を更新・追加し、評価手法も新たなモダリティや新規リスクに対応する形で定期的に更新しました。これにより、長期的な進捗の可視化とコミュニティの透明性強化を両立します[1]。

モデル安全性評価は、領域特化の安全性評価にも投資しています。医療分野では、60か国の262名の医師とともに約5,000の対話と4万以上の評価ルーブリックで構成されるHealthBenchを策定し、緊急時の案内（正確なトリアージ）、専門知識に応じたコミュニケーション、不確実性への対応、回答の深さ、健康データタスクなどの観点から安全性と有用性を評価しました。最新モデルは従来モデルより大幅に改善した一方で、信頼性や文脈把握にはなお改善余地があることも確認しています[6]。

モデル安全性評価は、マルチモーダル分野でも安全性評価を体系化します。動画生成モデルSoraでは、ヌード、選挙に関する欺瞞的コンテンツ、自傷行為、暴力といった主要領域を対象とする内部評価を開発し、緩和策の強化やモデレーション基準（閾値）設定に活用しました。長期の早期アクセスプログラムから得たフィードバック（60カ国以上・300人以上の早期アクセスユーザーから50万件超）も取り込み、安全プロトコルや製品レベルの対策を改善しています[9]。

モデル安全性評価は、多層防御の一環として安全性クラシファイアの評価と公開を進めます。推論時に開発者が独自の安全ポリシーを指定できるオープンウェイトの安全性分類モデル「gpt‑oss‑safeguard（120b/20b）」を研究プレビューとして公開し、コミュニティとの協働で性能を反復改善しています。安全性クラシファイアは特定リスク領域で安全なコンテンツと安全でないコンテンツを区別する主要な防御層として機能し、当社はこのプレビューモデルの安全性能を説明する技術レポートも公開しています。技術レポートでは、本モデルが与えられたポリシーに基づく分類用途に適していること、Responses API互換や思考の連鎖・構造化出力への対応などの仕様、さらにオープンモデルであるがゆえに技術的にはチャット用途も可能であることを踏まえ、当社の安全基準を満たしているかを検証した評価指標を提示しています（2025年10月29日）[7][8]。

モデル安全性評価は、エージェント評価でも基盤整備を進めています。SWE‑bench Verifiedとして、人手検証済みの500サンプルを抽出した新ベンチマークを導入し、テストの妥当性と問題記述の明確さを保証しました。スキャフォールドの違いによる能力変化が大きいことを踏まえ、Preparedness Frameworkでは学習前・学習中・学習後（外部システム統合後）にわたる継続的評価を求めています。静的データセット評価の汚染リスクやカバレッジの限界も明記し、壊滅的リスクの追跡と防御に向けた経験的・科学的アプローチを継続します[10]。

モデル安全性評価は、評価手法の更新、公開ドキュメント（System CardやPreparedness Framework）、外部機関との連携、領域特化ベンチマークの整備、そして多層防御の検証を通じて、モデル安全性評価の標準を引き上げ続けます。モデル安全性評価は、進捗を測り、透明性を高め、現実世界の利用から学びながら、次のリスクに先回りして評価とガードレールを強化していきます。掲載する結果はスナップショットであり、包括的な理解のためにEvaluations Hub、各モデルのSystem Card、Preparedness Framework、関連研究公開の併読を推奨します[1][2]。

【出典】
[1] https://openai.com/ja-JP/safety/evaluations-hub/
[2] https://openai.com/ja-JP/index/sharing-the-latest-model-spec/
[3] https://openai.com/ja-JP/index/learning-to-reason-with-llms/
[4] https://openai.com/ja-JP/index/strengthening-chatgpt-responses-in-sensitive-conversations/
[5] https://openai.com/ja-JP/index/strengthening-safety-with-external-testing/
[6] https://openai.com/ja-JP/index/healthbench/
[7] https://openai.com/ja-JP/index/introducing-gpt-oss-safeguard/
[8] https://openai.com/ja-JP/index/gpt-oss-safeguard-technical-report/
[9] https://openai.com/ja-JP/index/sora-system-card/
[10] https://openai.com/ja-JP/index/introducing-swe-bench-verified/


## モデル性能と信頼性
モデル性能と信頼性は、性能と信頼性を不可分の評価軸として位置づけ、現実世界のユースケースに即した厳密なベンチマーク設計、最悪ケースを含む包括的な測定、そして安全性と有用性のバランス最適化に継続的に取り組みます。モデル性能と信頼性は、平均点だけでなく、正解・誤答・棄権・遵守といった構成要素を丁寧に測ることこそが、信頼できる運用の土台だと捉え、行動指針であるModel Specへの準拠度とあわせて一体で評価します[1][4][9]。

モデル性能と信頼性は、医療領域に特化した新ベンチマークHealthBenchを発表し、60か国の262名の医師と協力して作成した5,000件の対話と4万件以上の評価ルーブリックでモデルを測定しました。最新モデル（o3、o4‑mini、GPT‑4.1）はHealthBenchで28%の改善を示し、コストと性能の新たなフロンティアを形成しました。小型モデルも急速に進歩しており、GPT‑4.1 nanoは2024年8月時点のGPT‑4oを上回りつつ25倍低コストで動作することを確認しています。また、同一事例に対する複数出力の「worst‑of‑n」で信頼性を評価し、最新モデルで大幅な改善を確認しつつも、さらなる向上余地を明確に示しました。高度な検証や難問設定に焦点を当てた「HealthBench Consensus」「HealthBench Hard」も導入し、難度と検証強度の異なる評価ファミリーを整備しています[1]。

モデル性能と信頼性は、推論性能と安全性の両立を重視し、困難プロンプトやジェイルブレイクを含むケースで安全なコンプリーション率を精緻に測定しています。思考の連鎖（Chain of Thought）はユーザーには開示せず監視可能に保つ設計を検討し、将来的に思考過程の忠実性を生かした行動監視（例：操作傾向の兆候把握）への応用を探っています。o1‑preview等の推論モデルでは、有害プロンプトへの安全な応答が大幅に改善し、デプロイ前にはPreparedness Frameworkに基づく安全テストとレッドチーミングを実施、観測された報酬ハッキングへの対策検討を含めてSystem Cardで透明化しています[2]。

モデル性能と信頼性は、ソフトウェア工学領域でSWE‑bench Verifiedを公開し、人手検証済みの500サンプルによってテスト妥当性と問題記述の明確性を担保しました。これにより、GPT‑4oは元のSWE‑benchでの16%から33.2%へと性能が向上しました。さらに、RAGやエージェント・スキャフォールドの違いによって同一モデルの性能が大きく変動する事実（例：SWE‑bench Liteで2.7%〜28.3%）を明示し、学習前・学習中・学習後、さらには外部システム統合後まで含めた反復評価を求める運用を進めています。静的データセット評価の限界や汚染リスクを明示し、研究コミュニティとの協働で評価を継続的に改善します[3]。

モデル性能と信頼性は、ハルシネーション低減に向けて「正解・誤答・棄権」を区別する評価設計を重視し、誤りは棄権より重大であるという姿勢を明確にしています。Model Specは、不確実な場合は自信過剰な断定を避け、曖昧さの開示や質問の明確化を促すことを推奨します。実例として、シンプルQAでは古いo4‑miniが正確性でわずかに優位でも、エラー率（ハルシネーション率）の高さが確認され、正確性のみのリーダーボードが誤った最適化を誘発しうることを示しています。モデル性能と信頼性は、棄権や誤答率を含む多面的な指標で信頼性を評価します[4][9]。

モデル性能と信頼性は、モデルの安全行動を高効率に改善するため、ルールベース報酬（RBR）を導入し、RLHFと統合してGPT‑4以降の世代に適用しています。RBRは「安全に正しく応諾すべきもの」と「安全でないため正しく拒否すべきもの」を明確に分け、有用性と安全性のトレードオフをフレームワークで可視化しながら、右上（安全かつ有用）を目指してチューニングします。大規模な再学習なしに迅速に更新でき、曖昧・主観的タスクには人間フィードバックとの併用で対処する運用方針です[7]。

モデル性能と信頼性は、現場適用の信頼性強化として、セキュリティ企業Doppelと共同で強化ファインチューニング（RFT）を用いた検知パイプラインを構築しました。GPT‑5およびo4‑miniを活用し、アナリストの作業負荷を80%削減、対応能力を3倍化し、脅威対応時間を数時間から数分へ短縮しました。信頼度が閾値を超えた場合は自動処置、低信頼または矛盾がある場合は人間レビューに回す運用を採用し、正確性だけでなく説明の質にも報酬を与える採点設計で、エッジケースにおける判断の一貫性を高めました[5]。

モデル性能と信頼性は、ローカル実行可能なオープンウェイトの推論モデル（gpt‑ossシリーズ）でも、公開ベンチマークで性能を提示し、Model System Cardと総合的な安全性テストを公開しています。開発者が思考の連鎖を完全に把握できる設計や、柔軟なカスタマイズ性・カスタム安全ポリシー適用とあわせ、再現性と透明性の高い評価・運用を支えます[6]。

モデル性能と信頼性は、センシティブな対話領域（精神疾患や自殺関連など）で臨床専門家と連携したターゲット評価を実施し、まれな事象の検出では一定の誤検知を許容しつつ有用な再現率を確保するという現実的なトレードオフを明示しています。2025年10月のモデル更新では不適切な応答を65〜80%削減し、分類法や技術システムが進化する中でも、将来の測定で方向性と進捗を追い続ける姿勢を明確にしています[8]。

モデル性能と信頼性は、o3およびo4‑miniの推論性能を多面的に評価し、AIME 2025でo4‑mini（ツール利用時）が99.5% pass@1（100% consensus@8）、o3が98.4% pass@1（100% consensus@8）を達成したことを報告しました。社外専門家評価では、指示追従精度や検証可能性の改善が示され、SWE‑benchの評価は内部で検証済みの固定サブセットに基づき、高い「推論努力」設定で実施しています[10]。

モデル性能と信頼性は、平均性能だけでなく最悪ケースや棄権を含む信頼性指標でモデルを測り、現実課題に即した人手検証済みベンチマークを拡充し、RBRやRFTなどの手法で安全性と有用性の両立を継続的に押し広げます。研究コミュニティとの連携を深め、評価の科学性と透明性を高めながら、コストと性能の境界を探索しつつ、モデル性能と信頼性のフロンティアを更新し続けます[1][3][4][5][7][10]。

【出典】
[1] https://openai.com/ja-JP/index/healthbench/
[2] https://openai.com/ja-JP/index/learning-to-reason-with-llms/
[3] https://openai.com/ja-JP/index/introducing-swe-bench-verified/
[4] https://openai.com/ja-JP/index/why-language-models-hallucinate/
[5] https://openai.com/ja-JP/index/doppel/
[6] https://openai.com/ja-JP/open-models/
[7] https://openai.com/ja-JP/index/improving-model-safety-behavior-with-rule-based-rewards/
[8] https://openai.com/ja-JP/index/strengthening-chatgpt-responses-in-sensitive-conversations/
[9] https://openai.com/ja-JP/index/sharing-the-latest-model-spec/
[10] https://openai.com/ja-JP/index/introducing-o3-and-o4-mini/


## モデル性能・信頼性評価
モデル性能・信頼性評価は、モデルの性能と信頼性を科学的かつ経験的に測定し、その進捗を透明に共有することを原則とします。評価が時間とともに「飽和」して差が見えづらくなる課題や、新しいモダリティや新規リスクの出現に応じて、モデル性能・信頼性評価は手法を定期的に更新し、社内外の結果を集約するEvaluations Hubで長期トレンドを示します。包括的な把握のため、モデル性能・信頼性評価はSystem CardやPreparedness Framework、モデル特有の研究公開と併せてご参照いただけるよう整備し、不許可コンテンツ、ジェイルブレイク、ハルシネーション、命令階層などの評価カテゴリで複数モデルの結果を比較可能にしています[1]。

モデル性能・信頼性評価は、フロンティアAIの能力とリスクをより信頼性高く測るため、第三者による外部テストを安全に組み込む仕組みを運用しています。独立評価、方法論レビュー、専門家によるSMEプロービングの三形態で評価を実施し、METRのtime horizon評価やSecureBioのVCT評価などをPreparedness Frameworkに基づく社内評価と補完的に活用します。評価協力組織には、初期チェックポイントへの安全なアクセス、選択的な評価結果の共有、必要に応じたゼロデータ保持、緩和策の少ないモデルの提供に加え、思考の連鎖への直接アクセスも用意し、サンドバッギングやスキーミングの検出に役立てています。これらのアクセスは厳格なセキュリティ管理の下で提供し、GPT‑5を含むモデルの能力やテストニーズの進化に応じて継続的に更新します[2]。

モデル性能・信頼性評価は、評価の質と比較可能性を高めるため新たなベンチマークを設計・公開します。SWE‑bench Verifiedでは、SWE‑benchから人手で検証した500サンプルを抽出し、課題記述の明確性とテスト妥当性を担保しました。その結果、GPT‑4oのスコアは16%から33.2%へと大幅に向上し、スキャフォールド（外部ツールや手順）選択が性能に大きく影響する事実も確認しています。Preparedness Frameworkに基づき、モデル性能・信頼性評価は学習前・学習中・学習後（外部システム統合後）にわたり繰り返し評価を行い、非自明な能力変化を特定します。静的データセット評価にはデータ汚染やカバレッジの限界があるため、他の評価との補完や、壊滅的リスクの追跡・防御に向けた継続的な評価改善をコミュニティと協働して推進します[3]。

医療のような高影響領域において、モデル性能・信頼性評価はHealthBenchを公開しました。60か国の262名の医師協力により作成した5,000の対話と4万以上のルーブリックでモデルを評価し、最新モデル（o3、GPT‑4.1 等）が従来より大幅に改善した一方、情報が不十分なクエリでの文脈把握や最悪時の信頼性に改善余地が残ることを明らかにしました。モデルベースのグレーダーと医師の一致率が医師間の一致率と同程度であるというメタ評価の結果も得ており、評価コードとデータは公開のうえ、現実世界の利益につながる進歩を検証する継続的な取り組みとして位置づけています[4]。

推論重視モデルについて、モデル性能・信頼性評価は思考の連鎖を活用した安全性・信頼性評価を強化しています。o1‑previewでは推論過程に安全方針を組み込み、人間の価値観や原則を教示した結果、ジェイルブレイク等の困難ケースを含む社内ベンチマークで大幅な性能向上を確認しました。例えば、有害プロンプトへの安全なコンプリーション割合は全体で0.995（GPT‑4oは0.990）、困難ケースで0.934（GPT‑4oは0.714）に改善しています。デプロイ前にはPreparedness Frameworkに従って安全性テストとレッドチーミングを実施し、評価過程で報酬ハッキングの事例も観測した事実をSystem Cardで公開しています[5]。

実運用における一貫性と説明可能性の確保も、モデル性能・信頼性評価が重視する柱です。セキュリティ企業Doppelと連携し、GPT‑5やo4‑miniに強化ファインチューニング（RFT）を適用、アナリストの判断を学習させることで曖昧なエッジケースでも専門家の判断を再現し、一貫性を高めました。低信頼度や矛盾がある場合は人間レビューに回し、その決定をRFTループに取り込み、正確性だけでなく説明の品質も採点する評価関数を応用工学チームと設計しました。ハイパーパラメータ調整と反復評価により、人間レベルの一貫性に近づいたことが示されています[6]。

センシティブな会話領域では、モデル性能・信頼性評価はメンタルヘルス文脈におけるターゲット評価を整備し、リリース前の新モデルも含めた社内評価をGlobal Physician Networkと連携して実施しました。170名以上の精神科医らが関与し、評価者間一致率が71〜77%であることを測定したうえで、精度（誤検知の少なさ）と再現率（見逃しの少なさ）のトレードオフを明示しつつ、GPT‑5の更新で不適切な応答を65〜80%削減しました。時間とともに進化する分類法と技術システムに基づき、将来の測定値は過去と単純比較できない場合があるものの、方向性と進捗を追跡する手段として評価を継続します[7]。

ハルシネーション抑制は、モデル性能・信頼性評価の設計中心にあります。出力は正解・誤答・棄権の三分類で測定し、誤答は棄権より重大と位置づけます。Model Specでは、不正確になりうる情報を自信を持って提示するより、不確実性の表明や質問の明確化を優先することを求めています。SimpleQAでは、gpt‑5‑thinking‑miniが高い棄権率を通じてエラー率を抑える一方、古いo4‑miniは見かけの正確性がわずかに高くても誤答率（ハルシネーション率）が非常に高いという結果が示されました。正確性のみのリーダーボードは誤った二分法を助長するため、モデル性能・信頼性評価は棄権や誤答を分けて測る指標設計を重視します[8]。

オープンウェイトモデルに対しても、モデル性能・信頼性評価は性能・安全性の両面で測定と開示を徹底します。gpt‑oss‑120B/20Bおよびgpt‑oss‑safeguardでは、MMLU、GPQA、AIME 2024/2025などの標準ベンチマーク結果を提示し、思考の連鎖を完全に把握できる設計やSystem Cardに基づく高度な安全基準を備えています。ローカル実行やカスタム安全ポリシーに対応しつつ、評価可能性と透明性を確保します[9]。

モデル性能・信頼性評価は、外部テストの活用、新規ベンチマークの設計、運用現場での反復評価、人間専門家との整合性確認、そしてハルシネーション抑制指標の重視を通じて、モデルの性能と信頼性の評価を継続的に進化させます。安全性と有用性の両立を図るため、コミュニティとの協力を深め、スケーラブルで実世界に即した評価エコシステムを推進していきます[1][2][3]。

【出典】
[1] https://openai.com/ja-JP/safety/evaluations-hub/
[2] https://openai.com/ja-JP/index/strengthening-safety-with-external-testing/
[3] https://openai.com/ja-JP/index/introducing-swe-bench-verified/
[4] https://openai.com/ja-JP/index/healthbench/
[5] https://openai.com/ja-JP/index/learning-to-reason-with-llms/
[6] https://openai.com/ja-JP/index/doppel/
[7] https://openai.com/ja-JP/index/strengthening-chatgpt-responses-in-sensitive-conversations/
[8] https://openai.com/ja-JP/index/why-language-models-hallucinate/
[9] https://openai.com/ja-JP/open-models/


