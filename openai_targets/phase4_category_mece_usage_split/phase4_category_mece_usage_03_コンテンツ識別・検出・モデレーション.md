# コンテンツ識別・検出・モデレーション

## 概要


### 基本姿勢
OpenAIは、テキスト・画像・動画の生成前後を貫く多層的な安全アーキテクチャで、誤検知と見逃しの双方を最小化しつつ有用性・表現の自由・プライバシーのバランスを取ります。OpenAIは、公開ガイドラインであるModel Specと評価ハブによる測定を中核に、透明性・説明責任・外部検証を通じて方針と実装を継続的に改善します。OpenAIは、データ最小化と選択可能な学習オプトアウトを含むプライバシー・バイ・デザインを運用の基盤に据えます。

### 重点的取り組み
OpenAIは、Soraを含む生成システムでプロンプトフィルタ、ブロックリスト、分類器、レッドチーム、C2PAメタデータと透かし、動的な出力評価を統合し、リアルタイムに不適切生成をブロックします。OpenAIは、Moderation APIやSafety Reasoner、小型高再現率クラシファイアに加え、オープンウェイト安全性分類モデルgpt‑oss‑safeguardを公開し、開発者が多層防御を再利用できる環境を整えます。OpenAIは、RBR×RLHFで一貫した拒否と過剰拒否の抑制を両立し、企業向けの学習不使用設定や欧州データレジデンシーなどの運用ガードレールを強化します。

### 重要事実
OpenAIは、評価ハブで安全性・有用性を標準/チャレンジセットとオートグレード指標で継続測定し、昨年5月の最高システム比でModel Spec準拠やジェイルブレイク耐性、過剰拒否の抑制が大幅に改善しました。OpenAIは、Soraの全動画にC2PAを埋め込み、選挙関連の欺瞞コンテンツに対し再現率98.23%・フラグ率88.80%の内部検出を達成し、60カ国超・300人超のユーザーからの50万件超のフィードバックを反映しています。OpenAIは、センシティブ会話で不適切応答を65〜80%削減し、HealthBenchで最新モデルの改善を確認、2025年2月に欧州でデータレジデンシーを提供し企業データの学習不使用を明確化しました。


## コンテンツ検出モデル
コンテンツ検出モデルは、OpenAIの多層的な安全性アーキテクチャの中核として設計・運用され、テキスト・画像・動画にまたがり、生成前後の両段階で分類器やフィルタを組み合わせて機能します。とりわけ動画生成モデル Sora では、プロンプトフィルタリング、ブロックリスト、分類器の閾値調整、レッドチーム評価を統合し、製品レベルでは透かしやC2PAメタデータといった真正性シグナルを実装、ヌード、選挙に関する欺瞞、自傷、暴力といった主要リスク領域の緩和策とモデレーション基準を運用過程で継続的に強化しています[1]。

コンテンツ検出モデルは、妥当性と網羅性を可視化するため、安全性・性能評価を公開する評価ハブを運用し、標準セットに加えて難易度の高いチャレンジ評価セットを用いて継続測定します。オートグレーダーによる自動採点で、モデルが安全でない出力を生成していないか（not_unsafe）と、無害な要求を過剰に拒否していないか（not_overrefuse）を指標化し、non-violent hate、personal-data、harassment/threatening、sexual/exploitative、sexual/minors、extremism、illicit/nonviolent、illicit/violent、self-harm/intent、self-harm/instructionsといった重大カテゴリ別サブ指標、さらにジェイルブレイク、ハルシネーション、指示階層の遵守といった評価も組み込みます[2]。コンテンツ検出モデルは、API提供においても、安全でないコンプリーションを検出するコンテンツフィルターを提供し、潜在的な用途の事前レビューと継続的な悪用監視を併用する運用方針を維持します[3]。

コンテンツ検出モデルは、検出と応答の一貫性を支える行動基準としてModel Specを公開・更新し、政治・文化的にデリケートな問いへの思慮深い対応や、危害に直結する手順提供の拒否といった原則に沿って、挑戦的なプロンプト収集と専門家レビューを通じた準拠度評価を継続し、昨年5月時点の最高システムと比べて準拠が大幅に改善したことを確認しています[4]。コンテンツ検出モデルは、ルールベース報酬（RBR）をRLHFに統合し、リクエストを「断固とした拒否」「柔らかい拒否」「従うべき（無害）」の応答タイプにマッピングする設計で、安全ポリシーに沿った拒否の一貫性を保ちつつ過剰拒否を抑制し、簡潔な謝意や共感的なトーンなど望ましい応答様式を明示的に強化します[5]。さらに、メンタルヘルス、自傷・自殺、AIへの感情的依存といった高優先領域では、安全性テストのベースラインを拡張し、Global Physician Networkと連携したターゲット評価を構築して、臨床家間の評価一致率（ケースにより71〜77%）や、フラグ付け会話における「精度」と「再現率」のトレードオフなど、まれな事象のベースレート問題を踏まえた運用評価を実施しています[6]。

コンテンツ検出モデルは、画像領域でも学習データのフィルタリングを支える検出モデルを能動学習で強化しました。具体的には、多分割交差検証で何百もの分類器を学習して誤分類されがちな正例を特定し、知覚特徴空間の最近傍探索で大規模な未ラベル画像群から類似例を抽出・人手ラベル付けし、当社の計算インフラで分類器学習と最近傍探索を多数GPUにスケールして能動学習サイクルを数分で回す運用を実現。さらに、同一ハイパーパラメータでフィルタ前後のデータを用いたGLIDE比較により、フィルタ後モデルで過激コンテンツ生成の頻度が減少する一方、特定の人口統計に関するバイアスが生じうる副作用も確認しました[8]。コンテンツ検出モデルは、加えて自然言語だけで視覚分類を行うゼロショット検出モデルCLIPを開発・公開し、実環境の有名人画像で100候補トップ1精度59.2%、1000候補で43.3%など、タスク特化最適化なしの汎化的性能を示しつつ、この特定タスクでの量産モデルとの比較では競争力が限定的であることも確認しています[9]。

コンテンツ検出モデルは、困難な設定における実効性も測定し、WildChatのカテゴリーごとのModeration APIスコア、StrongREJECTを含むジェイルブレイク評価、人手によるジェイルブレイク評価、無害なエッジケースに対する過剰拒否の抑制など、複数の基準で「安全なコンプリーションの割合」や適合度を公開して、難ケースでの堅牢性向上を確認しています[2]。将来的な監視の機会として、思考の連鎖（Chain of Thought）をユーザーに開示せずに保持する設計が、操作的挙動などの兆候検出に有望であると見ており、方針遵守や嗜好を思考過程に直接埋め込まない前提で研究を進めます[7]。

コンテンツ検出モデルは、検出モデル、評価基盤、運用ガードレールを緊密に連動させることで、誤検知と見逃しの両方を縮減し、現実世界の脅威や新たな悪用手法に対して継続的に改善を重ねます。今後も評価結果と設計原則を透明に共有し、コミュニティや専門家との協働を通じて、より安全で有用なコンテンツ検出エコシステムを前進させていきます[2][4]。

【出典】
[1] https://openai.com/ja-JP/index/sora-system-card/
[2] https://openai.com/ja-JP/safety/evaluations-hub/
[3] https://openai.com/ja-JP/index/instruction-following/
[4] https://openai.com/ja-JP/index/sharing-the-latest-model-spec/
[5] https://openai.com/ja-JP/index/improving-model-safety-behavior-with-rule-based-rewards/
[6] https://openai.com/ja-JP/index/strengthening-chatgpt-responses-in-sensitive-conversations/
[7] https://openai.com/ja-JP/index/learning-to-reason-with-llms/
[8] https://openai.com/ja-JP/index/dall-e-2-pre-training-mitigations/
[9] https://openai.com/ja-JP/index/clip/


## モデル情報
モデル情報は、コンテンツの識別・検出・モデレーションに関するモデル行動の原則を、公開ガイドライン「Model Spec」と評価手法の組み合わせで明確化し、継続的に改善します。Model Specは知的自由と透明性を重視しつつ必要なガードレールを保つための方針であり、アライメント評価用プロンプトとともにCreative Commons CC0で公開し、コミュニティからのフィードバックを取り込みながら発展させます[1][2]。モデル情報は、ユーザーの意図が不明確な場合に明確化の質問を行い、事実を提示しつつもユーザーの考えを変えようとしないなど、望ましい振る舞いを具体例で示します[1]。また、爆発物の製造やプライバシー侵害の具体的手順は提供せず、政治・文化的にデリケートな質問には特定の意図を推進せず思慮深く回答するという原則を明確にしています[2]。

モデル情報は、モデル行動の「測定」を重視します。Model Specの各原則への準拠度を検証するため、現実の利用状況を反映した挑戦的なプロンプト群を整備し、モデル生成と人間の専門家レビューを組み合わせて評価しています。予備的には、昨年5月時点の当社最高システムと比べてModel Specへの準拠が大幅に改善していることを確認しており、課題セットの拡充と反復的な改善を継続します[2]。

モデル情報は、モデルの安全行動を体系的に高めるため、ルールベース報酬（Rule-Based Rewards, RBR）を導入し、RLHFに統合しました。RBRは簡潔な命題とルールで応答タイプを規定し、安全性と有用性のバランスを取りながら過剰拒否を抑える設計です。非合法なヘイトスピーチや暴力犯罪の助言、過激思想には「断固とした拒否」、自傷行為に関する助言・指示・告白には感情に配慮した「柔らかい拒否」、無害のリクエストには「従う」という望ましい応答を定め、拒否時は「短いお詫び」と「応じられない旨」を含めるなど応答スタイルも具体化しています[3]。

モデル情報は、センシティブな会話領域におけるChatGPTの回答品質を大幅に強化しました。最新のモデル更新では、精神疾患や躁状態、自傷・自殺、AIへの感情的依存といった領域で不適切な応答を65〜80%削減し、170名超の精神科医・専門家の協力を得て準拠率を改善しました。Model Specの原則に沿い、ユーザーの現実世界の関係性を尊重し、根拠のない信念を肯定せず、妄想や躁状態の兆候に安全かつ共感的に対応し、間接的な自傷・自殺リスクのシグナルにも注意を払う方針を明確化しています。問題の定義、測定、外部専門家との検証、リスク軽減というステップで、各優先ドメインの改善を継続します。あわせて、安全検出の精度と再現率のトレードオフを明示し、ケースによっては評価者間一致率が71〜77%となることも観察しています[4]。

モデル情報は、マルチモーダル生成においても多層的な安全対策を講じています。動画生成モデルSoraでは、プロンプトフィルタリング、ブロックリスト、分類器、モデレーション、透かし、C2PAメタデータなどを組み合わせ、ヌード、選挙関連の欺瞞コンテンツ、自傷行為、暴力といった主要リスク領域に対する内部評価を通じて分類器の閾値や緩和策を継続的に調整しています。さらに、60カ国以上・300人以上のユーザーから50万件超のフィードバックを得る早期アクセスプログラムの知見を踏まえ、作業フローを阻害する目に見える透かしの扱いを見直し、C2PAメタデータの埋め込みを維持したまま、有料ユーザーが透かしなしの動画をダウンロードできるようにするなど、クリエイティブ用途の柔軟性と安全性を両立しています[5]。

モデル情報は、安全かつ有用な医療領域での対話を評価するため、HealthBenchを発表しました。60か国の262名の医師協力による約5,000の対話と4万以上の評価ルーブリックを用い、「緊急時の案内」「専門性に応じたコミュニケーション」「不確実性への対応」「回答の深さ」「健康データタスク」などの軸で評価し、最新モデル（o3、GPT‑4.1等）が従来より大幅に改善した事実を確認する一方、信頼性や文脈把握などには引き続き改善余地があることも透明に示しています[6]。

モデル情報は、プライバシーと個人情報の取り扱いを安全行動の中核として重視します。学習に用いる個人情報の量を減らし、個人情報や機密情報を求めるリクエストを拒否するよう学習させ、プライベートまたはセンシティブな情報を含む回答の生成可能性を最小化する方針です。公開情報から人々のプロフィールを作成したり広告のターゲティングに利用したり、ユーザーデータを販売することはありません。モデルは問い合わせのたびに新たに文章を生成し、後で思い出すためにデータベースへ保存することもありません。消費者向けサービスでは学習利用に関する設定やオプトアウトを提供し、Temporary Chatsは学習に使用しません。APIおよびEnterpriseではデフォルトで学習に利用せず、非APIの消費者サービスで送信されたデータはモデル改善に用いられる場合があり、プライバシーリクエストポータルからオプトアウトできます[7]。

モデル情報は、これらの設計原則・評価手法・安全強化策を、モデルと製品の双方に一貫して適用し、コンテンツの識別・検出・モデレーションにおける安全性と表現の自由、そして実用性の最適なバランスを追求し続けます[2][3][5]。

【出典】
[1] https://openai.com/ja-JP/index/introducing-the-model-spec/
[2] https://openai.com/ja-JP/index/sharing-the-latest-model-spec/
[3] https://openai.com/ja-JP/index/improving-model-safety-behavior-with-rule-based-rewards/
[4] https://openai.com/ja-JP/index/strengthening-chatgpt-responses-in-sensitive-conversations/
[5] https://openai.com/ja-JP/index/sora-system-card/
[6] https://openai.com/ja-JP/index/healthbench/
[7] https://openai.com/ja-JP/consumer-privacy/


## 利用方法
利用方法は、OpenAIのテクノロジーが世界を良くするために使用されることを徹底する「利用規定」を整備し、コンテンツの識別・検出・モデレーションに関する基本姿勢と運用の枠組みを明確にします。利用方法は、この枠組みのもとで、ユーザーの皆さまが安全かつ責任あるかたちでシステムを活用できるよう、具体的なガイドと執行プロセスを提供します[1]。

利用方法は、「使用に関するポリシー」において、モデレーション上の重要領域を明示しています。具体的には、なりすまし、政治運動・ロビー活動・国内外の選挙干渉・政治参加を妨げる活動、機密性の高い領域での重大な意思決定の人間の確認のない自動化、重要インフラ、金融活動・与信、重要な行政サービス、製品安全コンポーネント、国家安全保障、法執行などを含み、これらの一般規定は製品・サービス全体に適用されるよう継続的に改訂します。違反の監視は自動と手動を組み合わせて行い、ユーザーには本指針に沿った適切な利用を求めるとともに、不正利用の報告や異議申立ての手段も提供します[2][1]。

利用方法は、規約に基づき、本サービスの提供・維持・開発・改善、適用法の遵守、規約およびポリシーの履行請求、ならびに本サービスの安全性維持のために、ユーザーのコンテンツを利用する場合があります。これはポリシーに基づく検出・モデレーションの実施やサービスの安全運用に必要な範囲で行うものであり、モデル学習への活用についてはヘルプセンターの手順に従ったオプトアウトを受け付け、オプトアウトにより特定目的に対する処理能力が制限されうる点も透明に説明します[3][4]。

利用方法は、生成AIの確率的な性質を踏まえ、アウトプットが常に正確とは限らないことを明確に伝えます。そのため、真実や事実の唯一の情報源、または専門家の助言の代替としての依拠は推奨せず、使用・共有の前に必要に応じて人による確認を行うなど、適切な人間の監督を組み合わせることを求めます。たとえば、ChatGPTのPDF機能を法的文書の要約に活用する場合でも、法的助言の代替ではなく、重要な情報は必ず確認するよう注意喚起します[3][4][6]。

利用方法は、望ましいモデルの振る舞いを定義する「Model Spec」を公開し、違法行為を促進・助長・関与しないというルールを明示しています。たとえば「万引きをうまくやる方法」といった要求には応じない一方で、小売事業者が防止の観点から学ぶための質問には適切に応答するなど、文脈に応じて安全な応答を行います。悪用が確認された場合には、利用規約に基づきアカウントに対する措置を講じる可能性があることも、利用方法は透明に伝えます[5][3]。

利用方法は、企業向けワークスペースにおいても一貫した安全基準を運用します。ChatGPT Business／Enterpriseおよび社内ナレッジ機能では、デフォルトでお客様データをモデル学習に用いない運用を実施し、アクセス権限の尊重と厳格な管理（管理者によるカスタムロールやグループ権限設定）、業界標準の暗号化、SSOやSCIMによる大規模で安全なアクセス管理、IP許可リストによるアクセス制限、そしてEnterprise Compliance APIを通じた会話ログへのアクセスにより、監査と法令遵守を支援します。加えて、SSOや多要素認証、専用ワークスペースでのユーザー管理・可視性の付与など、組織的なコンテンツ管理とモデレーション運用を実現します[8][9][10]。

利用方法は、プランにかかわらず悪用防止策を適用します。たとえばProプランにおいても、「悪用防止策の範囲内」での無制限化を明記し、利便性の最大化と安全性の確保を両立します[7]。

利用方法は、これらの利用規定と運用の実務を継続的に見直し、製品・サービス全体にわたって一貫した安全基準を適用し続けます。ユーザーの皆さまには、当社のポリシーに沿ってコンテンツの識別・検出・モデレーション機能を活用し、重要な意思決定に際しては人間の確認を組み合わせるなど、責任ある利用をお願いします[1][2][3][4]。

【出典】
[1] https://openai.com/ja-JP/policies/
[2] https://openai.com/ja-JP/policies/usage-policies/
[3] https://openai.com/ja-JP/policies/terms-of-use/
[4] https://openai.com/ja-JP/policies/row-terms-of-use/
[5] https://openai.com/ja-JP/index/introducing-the-model-spec/
[6] https://chatgpt.com/ja-JP/features/chat-with-pdfs/
[7] https://chatgpt.com/ja-JP/plans/pro/
[8] https://openai.com/ja-JP/index/introducing-company-knowledge/
[9] https://openai.com/ja-JP/business/chatgpt-pricing/
[10] https://chatgpt.com/ja-JP/business/ai-for-finance/


## 生成物識別とデータ利用透明性
生成物識別とデータ利用透明性は、OpenAIの安全で責任あるAI運用の中核です。生成物識別とデータ利用透明性は、サービス上の活動を分類器、リーズニングモデル、ハッシュマッチング、ブロックリストなどの自動化技術と人による審査を組み合わせて監視し、違反の可能性があるコンテンツに体系的に対処します。違反が確認された場合には、アカウントの制限、コンテンツ共有の制限、検索結果のブロック、GPTの公開設定の制限などを実施し、ユーザー報告に基づく措置ではその旨を通知します[1]。

生成物識別とデータ利用透明性は、Soraを含む生成システムで来歴情報の強化に投資し、識別性と透明性を高めています。Soraで生成されるすべての動画にC2PAメタデータを埋め込み、AI生成であることを示す目に見える透かしをデフォルトで付与します。さらに、生成の帰属を確実に評価するための内部逆探索ツールを導入し、選挙関連の誤解を招くコンテンツについてはLLMフィルターで意図を特定し、出力動画を1秒あたり1フレームでスキャンして違反の可能性を評価、該当する生成はブロックします（欺瞞的選挙コンテンツに対して再現率98.23%、フラグ率88.80%、合成データに基づく約500プロンプトで評価）[2]。他者を詐取・誤解させる目的の使用や他者の画像・動画の無断使用は禁止し、違反時にはコンテンツ削除やペナルティを含む対処を行います[9]。

生成物識別とデータ利用透明性は、画像生成やSora 2で動的かつ段階的な出力評価を実行し、リアルタイムで安全でない生成を識別・ブロックします。生物学や自傷行為などの領域では、Moderation APIで用いる小型・高速かつ高再現率のクラシファイアで関心ドメインを特定し、Safety Reasonerが詳細な分類法に基づいて出力を評価して最適な対応を決定します。これらはGPT‑5やChatGPTエージェントを含むシステム全体の多層的な安全対策であり、開発者が同様のアプローチを活用できるよう、オープンウェイトの安全性分類モデル「gpt‑oss‑safeguard」を研究プレビューとして公開しています[3]。

生成物識別とデータ利用透明性は、データ利用の透明性を体系的に公開しています。「ポリシー」では、サービスのコンテンツをモデルの改善・学習にどう用いるかを説明する「お客様のデータとモデルのパフォーマンス」や、法人ユーザー向けの「エンタープライズプライバシー（送信データの使用と保持）」などの指針を案内しています[5]。エンタープライズでは「お客様の組織のデータは当社のモデルの改善には一切使用されません」と明確に定め、データを暗号化、MFAやSAML SSOで保護し、GDPRやCCPA等への準拠をサポートします[6]。ChatGPT Businessでも、デフォルトで顧客コンテンツがモデル学習に使用されない設計を採用し、SSOや多要素認証などの管理機能を備えています[11]。さらに、2025年2月にヨーロッパ向けにAPI、ChatGPT Enterprise、Eduでデータレジデンシーを導入し、地域内保存を可能にしました。これらの製品では顧客コンテンツがAIモデルの学習に使用されないこと、CSA STARおよびSOC 2 Type 2に準拠したデータ保護、GDPR等に対応する包括的なDPAの提供、データの所有権が顧客に帰属することを明確化しています[10]。

生成物識別とデータ利用透明性は、プライバシーポリシーで取得する個人情報のカテゴリー、利用目的、開示の有無、保護措置を明示し、商業上合理的な技術的・運用的・組織的対策で個人データを保護します。米国の州法に基づく追加開示や未成年者の利用に関する要件も提示しています[7]。過度かつ不適切なデータ開示要求に対しては利用者のプライバシーを守る行動を取り、大量の会話データ提出要求には法的に争い、裁判所命令の対象コンテンツは安全なシステムに隔離してリーガルホールドで保護し、法的義務に必要な場合に限り監査済みの少人数チームのみがアクセスします。第三者の閲覧は厳格な法的プロトコル下の安全な環境に限定し、個人識別情報の匿名化・削除などの追加措置を講じ、重要な更新は透明性をもって継続的に共有します[8]。

生成物識別とデータ利用透明性は、透明性のあるモデレーション運用を重視し、自動検知、ユーザーからの報告、人による審査を組み合わせ、違反確認時にはアカウントやコンテンツ共有の制限、特定検索結果のブロック、GPTの公開設定の制限、フォーラム管理などの対応を実施します。ユーザー報告に基づき措置を講じる場合には、その旨を通知します[1]。また、プロダクト内の報告機能、EUデジタルサービス法（DSA）に基づく透明性レポートや子供の安全性に関する定期報告、DSA連絡先、信頼と透明性に関する各種リソース、企業向け信頼ポータル、GPTの制限に対する異議申し立て手続の案内など、社会との協働による透明性の実践を継続しています[4]。

生成物識別とデータ利用透明性は、利用規約・ポリシーの遵守を通じて、プライバシーの尊重と生成物の適正利用を徹底します。本人の同意なく人物の肖像（リアルな画像や声を含む）を用いて本物と誤認させる利用、公共空間でのリアルタイム遠隔生体認証、同意のない顔認識データベースの作成、無断の安全性テストや当社の安全対策の回避、未成年者の搾取・危害の助長などを禁止しています[9]。

生成物識別とデータ利用透明性は、モデルの能力とリスクに関する外部評価にも取り組み、第三者評価者に公開・本番運用向けの情報やモデルへのアクセスを提供し、評価サマリーはシステムカードで公開します。機密保持と正確性の確認後には、METRによるGPT‑5レポート、Apollo ResearchによるOpenAI o1レポート、IrregularによるGPT‑5評価などの調査結果の公開も並行して進め、評価者には直接支払いまたはAPIクレジットなどで報酬を提供しますが、評価結果に報酬が左右されることはありません[12][2]。生成物識別とデータ利用透明性は、生成物の識別強化とデータ利用の透明性向上を両輪として、より安全で信頼できるエコシステムの構築に取り組み続けます[2][3][4][5]。

【出典】
[1] https://openai.com/ja-JP/transparency-and-content-moderation/
[2] https://openai.com/ja-JP/index/sora-system-card/
[3] https://openai.com/ja-JP/index/introducing-gpt-oss-safeguard/
[4] https://openai.com/ja-JP/trust-and-transparency/
[5] https://openai.com/ja-JP/policies/
[6] https://openai.com/ja-JP/solutions/use-case/research/
[7] https://openai.com/ja-JP/policies/privacy-policy/
[8] https://openai.com/ja-JP/index/fighting-nyt-user-privacy-invasion/
[9] https://openai.com/ja-JP/policies/usage-policies/
[10] https://openai.com/ja-JP/index/introducing-data-residency-in-europe/
[11] https://chatgpt.com/ja-JP/business/ai-for-data-science-analytics/
[12] https://openai.com/ja-JP/index/strengthening-safety-with-external-testing/


## 識別性と透明性の確保
識別性と透明性の確保は、OpenAIのコンテンツ識別・検出・モデレーションを貫く中核原則として、技術・運用・公開レポート・ポリシー整備・第三者評価の各レイヤーで具体策を実装し、製品の安全かつ責任ある利用を支えます[1][2]。識別性と透明性の確保は、プライバシーポリシーに基づき、サービス上のアクティビティを自動化技術と人による審査で監視し、分類器・リーズニングモデル・ハッシュマッチング・ブロックリストなどの自動システムで規約やポリシーに違反する可能性のあるコンテンツを能動的に検知、リスクの識別性を担保します[1][4]。重大なリスク（人の生命や社会規模の危害など）は自動監視から人間によるレビューへ適切にエスカレーションし、従業員からも守る高度なセキュリティ機能と組み合わせて濫用の兆候を継続的に見張ります[3]。

識別性と透明性の確保は、ユーザーからの報告を受け付け、可能な限り迅速に審査し、報告に基づく措置の際は通知を行うことでプロセスの透明性を確保します[1]。違反が確認された場合、アカウントへのアクセス制限、特定コンテンツの共有禁止・無効化、検索結果のブロック、GPTの公開設定（GPTストアでの表示を含む）の制限、フォーラム投稿の削除などの施行措置を適用します[1][2]。あわせて、製品内の報告機能や関連リソースを提供し、ユーザーが状況に応じて当社製品内からコンテンツを報告できる導線を整備しています[2]。

識別性と透明性の確保は、Soraの探索フィードやChatGPT検索におけるコンテンツ表示方法と安全基準の適用、モデレーションと施行のアプローチを体系的に公開します[1][2]。さらに、EUデジタルサービス法（DSA）に基づく透明性レポート、子供の安全性に関する定期報告、DSA連絡先と通知、GPTの制限に対する異議申し立て手続き（再公開を含む）、企業向け信頼ポータルなど、説明責任を高める各種リソースを継続的に提供し、利用者と社会に対して一貫した説明と検証可能性を確保します[2]。

識別性と透明性の確保は、法的要請への対応でもプライバシー保護と説明責任を重視します。裁判所命令の対象となるコンテンツは安全なシステムで隔離保管し、監査済みの小規模な法務・セキュリティチームのみが法的義務の範囲でアクセスできるよう限定します。外部当事者の閲覧は厳格な法的プロトコル下の安全な環境に限定し、必要に応じて匿名化や個人識別情報の削除を実施し、重要な更新情報を随時共有します[5]。

識別性と透明性の確保は、多層防御の中核に安全性クラシファイアを据えます。Moderation APIなどのクラシファイアは、事前定義された安全性ポリシーに基づく多数のラベリング例から学習し、安全な出力と安全でない出力を区別します[6]。同時に、推論時に開発者が独自の安全ポリシーを指定できるオープンウェイトの安全性分類モデル「gpt‑oss‑safeguard（120b/20b）」を研究プレビューとして公開し、技術レポートやコミュニティからのフィードバック、ROOSTとの協働によるテストとドキュメント整備を通じて透明性と説明可能性を高めています[6]。

識別性と透明性の確保は、モデルがどのように振る舞うべきかを明確化するため「Model Spec」を公開し、ルールやデフォルト、具体例を提示した上で一般からのフィードバックを受け付け、定期的な更新を予告します。違法行為の助長は拒否する一方で、小売事業者が万引き防止策を学ぶ正当な文脈には適切に応答するなど、文脈に応じた安全な応答方針を示し、悪用が確認された場合には利用規約に基づくアカウントへの措置があり得ることを明確にしています[7]。この公開ガイドラインは、ユーザーと開発者が期待できるモデルの振る舞いを具体的に示し、モデレーション判断の透明性を高めます[7]。

識別性と透明性の確保は、フロンティアAIの信頼性ある第三者評価を通じて安全性の透明性を推進します。独立した評価者による報告はシステムカードの評価サマリーの公開と並行して発表され、必要に応じてhelpful‑onlyモデルや非公開情報へのアクセスを提供しつつ、厳格なセキュリティ管理下で実施します。評価者には作業対価やAPIクレジット等の報酬を提供しますが、結果によって左右されることはなく、これらの枠組みは最新モデル（GPT‑5を含む）にも適用されています[8]。

識別性と透明性の確保は、利用者に分かりやすいポリシーとコントロールを整備します。使用に関するポリシーの改訂では、無断の安全性テストや安全対策の回避の禁止、プライバシー尊重（同意のない顔認識データベースや公共空間でのリアルタイム生体認証、同意のない肖像の誤認使用の禁止など）、未成年者の安全確保に関する規定を明確化しました[9]。利用規約では、サービスの安全性維持等のためのコンテンツ利用、学習への利用を望まない場合のオプトアウト、そして適切な人による確認を前提としたアウトプットの取り扱いを求めています[10]。プライバシーポリシーでは、個人データを販売・共有せず、ターゲット広告目的で処理しないこと、利用者のアクセス・削除・修正・差別されない権利、認定代理人による権利行使や本人確認の仕組みを明示しています[4]。

識別性と透明性の確保は、パートナーとの協働を通じても、説明可能性の高い検知・判断基盤の構築を支援します。Doppelの取り組みでは、採点機能に説明の品質評価を組み込み、人間レビューの判断を学習に取り込む設計を支援し、決定理由の自然言語生成によって透明性と一貫性を高めました[11]。

識別性と透明性の確保は、検出の精度と運用プロセスの可視性を同時に高めることで、ユーザー、開発者、社会が信頼できるコンテンツモデレーションを実現します。今後も、識別性の高い安全対策と、意思決定プロセス・方針・結果の透明な開示を継続的に進めます[1][2]。

【出典】
[1] https://openai.com/ja-JP/transparency-and-content-moderation/
[2] https://openai.com/ja-JP/trust-and-transparency/
[3] https://openai.com/ja-JP/index/teen-safety-freedom-and-privacy/
[4] https://openai.com/ja-JP/policies/privacy-policy/
[5] https://openai.com/ja-JP/index/fighting-nyt-user-privacy-invasion/
[6] https://openai.com/ja-JP/index/introducing-gpt-oss-safeguard/
[7] https://openai.com/ja-JP/index/introducing-the-model-spec/
[8] https://openai.com/ja-JP/index/strengthening-safety-with-external-testing/
[9] https://openai.com/ja-JP/policies/usage-policies/
[10] https://openai.com/ja-JP/policies/terms-of-use/
[11] https://openai.com/ja-JP/index/doppel/



---


