# モデル運用・最適化・標準化

## 概要


### 基本姿勢
OpenAIはAIオペレーティングモデルを中核に、Model Spec（CC0）で行動標準を公開し、測定・外部検証・透明性を前提にモデルの運用・最適化・標準化を継続的に進めます。OpenAIは多層防御（学習・推論・システム）の安全設計と人間中心の運用を徹底し、エンタープライズでは「Secure & compliant by design」「Proven to scale」を原則にプライバシーと説明責任を両立します。

### 重点的取り組み
OpenAIはModel Specの継続改訂と挑戦的評価プロンプト、SWE‑bench Verified・HealthBench・GDPvalなどの評価基盤で準拠度と実務性能の双方を強化します。OpenAIはPreparedness Frameworkと第三者評価、RBR×RLHF、Safety Reasoner、リーズニングLLMモニターでフロンティアリスクを抑止し、未成年保護とモデレーション運用を強化します。OpenAIはSSO/MFA/SCIM/EKMやAdmin/Audit API、Scale TierのSLA、MCP/コネクタ連携に加え、gpt‑ossとgpt‑oss‑safeguardの公開で、安全かつ拡張可能な導入を支えます。

### 重要事実
OpenAIはセンシティブ会話領域で不適切応答を65〜80%削減（2025年更新）、Model Spec準拠も昨年5月時点の最高システム比で大幅改善を確認しました。OpenAIはo4‑miniがAIME 2025でPythonツール利用時pass@1=99.5%（o3=98.4%）を達成、SWE‑bench VerifiedではGPT‑4oのスコアを16%→33.2%へ改善しました。OpenAIはDNPの特許調査95%時間短縮・処理量10倍やENEOSマテリアルの「数ヶ月→数分」短縮などの実運用効果を実証し、Business/EnterpriseでSOC 2 Type 2とISO群に準拠（ビジネスデータは学習不使用）、Scale Tierの99.9%稼働率SLAと低レイテンシ提供、gpt‑oss（120B/20B）とgpt‑oss‑safeguardを公開しています。


## AIオペレーティングモデル
AIオペレーティングモデルは、安全で信頼できるAIを大規模に運用・最適化し、組織横断で標準化するためのOpenAIの中核フレームワークである。AIオペレーティングモデルは、モデルの望ましい振る舞いを定義する公開ガイドライン（Model Spec）、測定可能な評価と継続的改善、外部の独立評価を組み込んだ安全性ガバナンス、透明なドキュメンテーション、そしてエンタープライズ導入のための運用パターンで構成され、実運用で機能する標準として進化し続ける[1][4][6][7]。

AIオペレーティングモデルは、モデル行動の標準化の基盤としてModel Specを公開し、知的自由と透明性を尊重しながらも、安全なガードレールを堅持する。Model Specは、法令順守や危険情報の不提供、クリエイターの権利とプライバシーの尊重、NSFWコンテンツの抑制などのルールと、目的に沿ったデフォルト行動（明確化質問、公平性の奨励、不確実性の表現、適切ツールの活用など）を明示し、開発者とエンドユーザー双方の行動基準を標準化する。これらの原則はCreative Commons CC0で公開され、アライメント評価用プロンプトとともにコミュニティのフィードバックを取り込みながら発展させる。また、「範囲内に留まる」「共に真実を追求する」という哲学のもと、爆弾の作り方やプライバシー侵害の手順などの提供は拒否しつつ、政治・文化的にデリケートな問いには特定の意図を推進することなく思慮深く応答する方針を強化する[1][2]。

AIオペレーティングモデルは、運用最適化と品質保証のために、Model Spec準拠度を測る挑戦的な評価プロンプトを収集・拡充し、モデル生成と人間の専門家レビューを組み合わせて現実世界に近いシナリオで性能を測定する。予備結果では、昨年5月時点の最高システムと比べてModel Specへの準拠が大幅に改善しており、方針更新の影響も一部あるものの、多くはアライメント強化によるものと評価している。AIオペレーティングモデルはこの取り組みを継続的なプロセスと位置づけ、実運用で得られる新たな事例を取り込みながら課題セットと運用基準をアップデートし続ける[2][1]。

AIオペレーティングモデルは、安全性ガバナンスを柱に据える。安全学習データの再構築や、バイオリスク・マルウェア生成・ジェイルブレイク向けの新たな拒否プロンプトの導入を通じてモデルの拒否行動を強化し、システムレベルでは、人間が作成した解釈可能な安全仕様に基づいて学習したリーズニングLLMモニターを組み込むことで、フロンティアリスク領域の危険なプロンプトを高感度にフラグ付けする。バイオリスク領域では人間のレッドチーミング会話の最大99%を検知し、更新版Preparedness Frameworkに沿って過去最も厳格な安全性プログラムでストレステストを実施している[8]。さらに、AIオペレーティングモデルは信頼できる第三者評価を運用プロセスに組み込み、独立評価・方法論レビュー・専門家（SME）プロービングの3形態で能力・関連リスク・安全対策の有効性を検証し、その結果を展開判断に反映する。システムカードに展開前評価のサマリーを掲載するなどの透明性と、評価組織の多様性・成果公開の支援を通じて、安全性エコシステムのレジリエンス向上を図る[4]。

AIオペレーティングモデルは、透明性と標準ドキュメンテーションを運用の要と位置づける。オープンウェイトモデルgpt‑oss‑120B/20Bについては、オープンウェイト特有のリスク（悪意のあるファインチューニングによる安全措置回避や有害最適化など）を率直に説明したモデルカードを公開し、OpenAIが後から安全措置を付与・撤回できない前提を踏まえ、開発者・企業によるシステムレベルの保護の補完を推奨している。多様なシステムに組み込まれる前提から、当該文書は「システムカード」ではなく「モデルカード」として提供する[6]。あわせて、gpt‑ossファミリーはローカル実行や柔軟なカスタマイズに対応し、カスタム安全性ポリシーをサポートするgpt‑oss‑safeguardなど、ユースケースに合わせた安全・運用設計を可能にする[3]。

AIオペレーティングモデルは、エンタープライズ導入において「Secure and compliant by design」「Proven to scale」の原則のもと、暗号化と設定可能なポリシーでデータを保護し、実証済みのアーキテクチャパターンでパイロットから本番までを推進する。データが豊富で複雑な環境でも洞察を引き出すフロンティアモデル、APIプラットフォームやCodex、各種コネクターやMCP連携を活用し、業務全体の生産性と意思決定を強化する[7]。

AIオペレーティングモデルは、人間中心の運用を重視し、機械的な作業は自動化しつつ、重要な判断は人間に委ねる。社内では契約書データエージェントを用いて、PDFやスキャン・写真の契約条項を構造化し、非標準条項を理由付きでハイライトするワークフローを確立。レビュー時間の半減、夜間の自動解析、チームを増員せずに処理量を拡大する運用を定着させ、専門家が高付加価値業務に集中できる体制を築いている[5]。

AIオペレーティングモデルは、上記の原則を社内外で具体的に実装し、その実践知をプロダクトと標準として還元している。サポートではAgents SDK、Responses API、Realtime APIを活用し、単なるチケット処理にとどまらず、すべてのユーザーとのインタラクションを学習・改善の機会へ転換する運用へ移行。サポート専門家が知識の洗練・モデル改善・システム拡張に貢献する「システム思考者」として機能する体制を運用し、成果の測定や評価の仕組みも見直して、サポートを製品のサーフェスに織り込んでいる[9]。営業では、ChatGPT Enterprise/Businessの展開に伴うインバウンドリード急増へ対応する「インバウンド営業アシスタント」を構築し、内部コネクターで製品ドキュメントやポリシー、事例、プレイブックをモデルのコンテキストに統合。推測に依らない正確でパーソナライズされた多言語応答を数分で提供し、スレッドは文脈ごと担当者へシームレスに引き継ぐ。精度は数週間で約60%から98%以上に向上し、リードの転換率を大幅に高め、数か月で年間数百万ドル規模の定常収益創出に寄与している[10]。お客様の導入基盤としては、データサイエンス＆アナリティクス向けのChatGPT Businessソリューションで、データウェアハウスやBI、ストリーミングのコネクターとコラボレーション機能を備えた1つのハブを提供し、特徴量・前処理パイプライン生成からモデル解釈、インサイト作成までを自動化。学習にデータを用いないデフォルト設計、SSO/多要素認証、権限・ブランドガバナンス・コンプライアンスの一元管理により、信頼と統制の下で部門横断の意思決定を支援する[11]。金融領域では、セキュアでコンプライアンスに配慮した設計、パイロットから本番までのスケーリング実績、データリッチで複雑な環境に強いフロンティアモデルを備え、ChatGPT Enterpriseによる生産性向上、APIプラットフォームとCodexによる取引・リスク・コンプライアンスシステムの近代化、社内外データやMCPへの安全な接続によるプロンプト強化まで、全社的なAI活用を支える[7]。運用標準の確立に向けては、ローカル実行可能なオープンウェイト推論モデル群（gpt‑oss‑120B/20B）と、カスタム安全性ポリシーをサポートするgpt‑oss‑safeguardを公開し、寛容型ライセンス、柔軟なカスタマイズ性、チェーン・オブ・ソートの可視性、ブラウザで試せるplayground、デスクトップからデータセンターまでの多様な実行形態を提供している[3]。モデル行動の標準化については、Model Specの改定・公開と、各原則への準拠度を測定する挑戦的プロンプトセットでの評価を継続し、昨年5月時点の最高システム比での大幅な準拠改善を確認している[2]。安全性運用の高度化としては、o3およびo4‑miniでの安全学習データの再構築、新しい拒否プロンプトの追加、リーズニングLLMモニターによるフロンティアリスク検知、Preparedness Frameworkに沿った最も厳格な安全性プログラムでのストレステストを実施している[8]。

AIオペレーティングモデルは、行動基準の標準化（Model Spec）、測定と継続改善（評価プロンプトとベンチマーク）、多層的な安全性（システムレベルの緩和策、外部評価、Preparedness Framework）、透明性（モデル／システムカード）、運用スケールとカスタマイズ性（オープンウェイトとエンタープライズアーキテクチャ）、そして人間の判断を核とする運用設計を統合し、コミュニティとともに安全で有用なAIの社会実装を前進させ続ける[1][3][4][6][7][8]。

【出典】
[1] https://openai.com/ja-JP/index/introducing-the-model-spec/
[2] https://openai.com/ja-JP/index/sharing-the-latest-model-spec/
[3] https://openai.com/ja-JP/open-models/
[4] https://openai.com/ja-JP/index/strengthening-safety-with-external-testing/
[5] https://openai.com/ja-JP/index/openai-contract-data-agent/
[6] https://openai.com/ja-JP/index/gpt-oss-model-card/
[7] https://openai.com/ja-JP/solutions/industries/financial-services/
[8] https://openai.com/ja-JP/index/introducing-o3-and-o4-mini/
[9] https://openai.com/ja-JP/index/openai-support-model/
[10] https://openai.com/ja-JP/index/openai-inbound-sales-assistant/
[11] https://chatgpt.com/ja-JP/business/ai-for-data-science-analytics/


## アクセス管理・認証
アクセス管理・認証は、OpenAIにおけるモデル運用の最適化と標準化を下支えする前提として、組織規模に応じたアクセス管理・認証の強化を最優先で推進します。アクセス管理・認証は、BusinessおよびEnterpriseプランにおいて、既存のアイデンティティ基盤との安全な統合、きめ細かな権限管理、運用ガバナンスを一貫して提供し、企業導入に必要なセキュリティと管理性を確保します[1][2]。その基本方針のもと、アクセス管理・認証は、認証・認可・監査の各レイヤーを段階的に実装し、組織の標準化と継続的な運用最適化を支援します[1][2]。

アクセス管理・認証は、組織の認証要件に応えるため、BusinessとEnterpriseの両プランにSAML SSOを標準搭載し、シングルサインオンによる安全でスムーズなアクセスを実現します[1]。あわせて、専用ワークスペースとドメイン認証を両プランで提供し、組織ドメインに基づくアクセス管理とユーザー体験の一貫性を担保します[1][2][3]。これにより、各社のIdPと統合したシンプルな導入と、日常運用の負荷軽減を両立します[1][2]。

アクセス管理・認証は、ユーザーおよびグループのライフサイクル管理を標準化するため、EnterpriseでSCIMによるプロビジョニング／デプロビジョニングを提供します（Businessは未提供）[1][2]。権限設計では、Enterpriseにロールベースのアクセス制御（RBAC）を実装し、細分化されたアクセス権限の付与を可能にします[1]。一方で、Business/Enterpriseの双方に管理者ロール、メンバーの一括管理、管理コンソール、ならびにGPTの分析と管理を提供し、日々の運用・監査・最適化を支援します[1]。

アクセス管理・認証は、機密性要件に応えるため、Enterpriseでエンタープライズキー管理（EKM）に対応し、鍵管理ポリシーや規制要件に整合する運用を可能にします[1][2][3]。プライバシー面では、Enterpriseにおいてお客様コンテンツをモデルの学習に使用しないことを明確化し、機密性の高いユースケースにも適合する設計を徹底します[1]。

アクセス管理・認証は、運用ガバナンスの基盤となる一元化された請求管理をBusiness/Enterpriseで提供し、組織全体のコスト管理と可視化を容易にします[1]。さらに、両プランはSOC 2 Type 2に準拠し、ISO 27001/27017/27018/27701の認証を取得するなど、第三者評価に裏づけられたセキュリティ・プライバシー体制を整備しています[1][2][3]。これらの取り組みにより、アクセス管理・認証は、OpenAIにおけるアクセス管理のベストプラクティスを実装し、企業におけるAI活用の標準化と継続的な運用最適化を力強く推進していきます[1][2]。

【出典】
[1] https://openai.com/ja-JP/business/chatgpt-pricing/
[2] https://chatgpt.com/ja-JP/pricing?openaicom-did=8b0b5e53-6aae-44df-8fc0-8017a4ada5f5&openaicom_referred=true
[3] https://chatgpt.com/ja-JP/pricing?openaicom-did=9bc4de2a-5bae-4c2f-91ce-7e9011242554&openaicom_referred=true


## セキュア連携
セキュア連携は、OpenAIのモデルを既存のシステムや業務フローに安全に統合するため、エンタープライズグレードのセキュリティ、透明性、モデレーション、外部検証を組み合わせた設計思想を徹底しています。製品とAPIの両面でセキュアな接続・運用を支え、運用現場での信頼性と標準化を高い水準で両立させます[7][11][5][4][1]。その前提として、セキュア連携はエンタープライズプライバシーとセキュリティを重視し、組織データをモデル改善に一切使用しないポリシー、暗号化、MFA、SAML SSOを適用し、GDPRやCCPAなどのプライバシー規制への準拠を支援します。さらに、DPA（データ処理補遺）やSOC 2 Type 2の対象化、第三者による定期的な侵入テストを通じ、実運用に必要なコンプライアンスと技術的管理を備えています。詳細な管理策はセキュリティ/信頼ポータルや関連ドキュメントで随時確認いただけます[7][1]。

セキュア連携は、ID/アクセス管理を基盤に据え、SAML SSOやMFAに加えて、専用ワークスペース、ドメイン認証、管理者ロール、一元化された請求管理、GPTの分析・管理機能を提供します。Enterpriseプランでは、SCIMによるユーザーライフサイクル管理やロールベースのアクセス制御（RBAC）に対応し、大規模環境での権限設計と委任を可能にします。運用統制面では、Admin APIやAudit Logs API、プロジェクト単位のアクセス・使用・コスト制御、使用状況ダッシュボードにより、可視化とトレーサビリティを確保します[14][12][15]。また、GDPR/CCPA対応、DPA、SOC 2 Type 2のほか、Enterprise/Business製品におけるISO 27001/27017/27018/27701の認証を明示し、第三者の侵入テストの実施とともに、管理策とコンプライアンス活動の情報を継続的に提供します[7][12]。

セキュア連携の実装面では、コネクターやMCP（Model Context Protocol）により、CMS、DAM、デザインツールなど既存ツール群へ安全に接続しつつ、データをプライベートかつ保護された状態に維持します。あわせて、エージェントのサンドボックス化やネットワークアクセスの構成可能性といった製品レベルの緩和策を導入し、連携面の攻撃面を最小化するアーキテクチャを追求しています[11][8]。

セキュア連携は、ポリシーとモデレーションの両輪でガバナンスを実装します。使用ポリシーでは、脅迫、暴力、兵器、違法行為、無断の安全性テスト、当社安全対策の回避等の禁止事項を明確化し、誤執行に対する異議申立てや不正利用の報告手段を提供します[3]。運用時には、自動分類やハッシュマッチング等の能動的検知、ユーザーからの報告、人による審査を組み合わせ、違反が確認された場合はアカウント制限、コンテンツ共有の制限、検索結果のブロック、GPT公開設定の制限といった措置を実施します[5]。子どもの安全については、Soraを含む全製品でCSAMの予防・検出・報告を最優先とし、ファーストパーティおよびサードパーティ（API/Enterprise）双方の入出力を対象に厳格なスキャンを行います[10]。さらに、ポリシー適合の自動化・標準化を支援するため、開発者指定のポリシーに基づくコンテンツ分類を行うgpt‑oss‑safeguard（120b/20b）を公開し、Responses API互換、構造化出力、思考の連鎖（CoT）に対応した安全ゲーティングの実装を推奨しています[6]。

外部検証と透明性もセキュア連携の基盤です。フロンティアAIの第三者評価を制度化し、原則として公開用または本番運用向けの情報・モデルを提供しつつ、必要に応じてhelpful‑onlyモデルや非公開情報へのアクセスも認め、その際の厳格なセキュリティ管理を継続的に更新します。評価者には適切な報酬を提供し、機密性と正確性を確保したうえで評価サマリーや各機関の調査結果の公開を進めます[4]。同時に、DSA透明性レポートや子どもの安全に関するレポート、製品内の報告機能、GPTに関する異議申し立て窓口、企業向け信頼ポータルを整備し、ステークホルダーが確認・通報・対話できる仕組みを提供します[1]。

脅威環境の変化に対して、セキュア連携は知見と対策を継続的に発信・更新します。プロンプトインジェクションのようなフロンティア課題、責任ある開示、サイバー・レジリエンスの強化などのテーマで情報提供を行い[2]、エージェント型セキュリティリサーチャー「Aardvark」の提供や外部向け協調開示ポリシーの更新を通じて、ソフトウェアとサプライチェーンの長期的なレジリエンスに貢献します[9]。

導入実績として、セキュア連携はChatGPT Enterpriseの活用により、大日本印刷（DNP）における外部セキュリティ監査の前年項目との差分確認を30分から5分に短縮、暗号スイート選定を3時間から1時間に短縮、CIS Benchmarksの非準拠項目の初動調査約100件を従来の2人日から約10分で完了させるなど、セキュリティ関連業務の効率化に寄与しています[13]。

セキュア連携は、法規制・運用ポリシー・技術的緩和策・外部評価・透明性を統合し、モデルのセキュア連携における実務的な標準を高め続けます。お客様の重要なシステムやワークフローとAIをつなぐ際にも、私たちは安全性とプライバシー、説明責任を同時に満たす運用を推進します[7][11]。

【出典】
[1] https://openai.com/ja-JP/trust-and-transparency/
[2] https://openai.com/ja-JP/news/security/
[3] https://openai.com/ja-JP/policies/usage-policies/
[4] https://openai.com/ja-JP/index/strengthening-safety-with-external-testing/
[5] https://openai.com/ja-JP/transparency-and-content-moderation/
[6] https://openai.com/ja-JP/index/gpt-oss-safeguard-technical-report/
[7] https://openai.com/ja-JP/security-and-privacy/
[8] https://openai.com/ja-JP/safety/
[9] https://openai.com/ja-JP/index/introducing-aardvark/
[10] https://openai.com/ja-JP/index/sora-system-card/
[11] https://openai.com/ja-JP/solutions/use-case/content-creation/
[12] https://openai.com/ja-JP/business/chatgpt-pricing/
[13] https://openai.com/ja-JP/index/dai-nippon-printing/
[14] https://openai.com/ja-JP/business-data/
[15] https://chatgpt.com/ja-JP/business/ai-for-engineering/


## セキュリティ設計
セキュリティ設計は、OpenAIのモデル運用・最適化・標準化の中核に安全性を据え、開発から展開、運用に至る全ライフサイクルで設計原則として安全を組み込みます。「安全性に終わりはない」を前提に、データのフィルタリング、ポリシーと人間の価値観の反映、各種テスト、社内外レッドチーム、システムカード、Preparednessの評価、安全委員会、アルファ／ベータ／GA、そしてユーザーフィードバックという段階的プロセスを継続運用し、反復的に改善します。とくに文脈依存の新たなリスクが現れやすい領域では、この段階的アプローチを安全対策の設計と一体化して適用します[1][5]。

セキュリティ設計は、ゼロトラストと多層防御のアプローチで、エンタープライズグレードのデータガバナンスと統合レイヤーの保護を標準化します。ビジネスデータは保存時AES‑256、転送時TLS 1.2以上で暗号化し、Enterprise Key Managementによりお客様自身の鍵管理を可能にし、データ保持期間の制御やAPIのゼロデータ保持オプション、24時間365日のオンコール監視を提供します。組織データは当社モデルの改善には一切使用しません[6][7]。また、MFAやSAML SSO、ドメイン認証、（Enterpriseでは）SCIMやロールベースのアクセス制御、エンタープライズキー管理などを標準装備し、ChatGPT Business/EnterpriseではSOC 2 Type 2に加えてISO 27001/27017/27018/27701の認証を取得しています[9]。運用コントロールはSOC 2 Type 2の独立監査、DPA対応、第三者侵入テスト、HIPAA等の顧客要件支援を通じて外部基準と整合し、透明性レポートや信頼ポータルでの情報提供で規制・契約要件との整合性を高めます[6][5]。さらに、CMSやDAM、デザインツールなど既存ワークフローとの連携でも、データをプライベートかつ保護された状態で維持します[8]。エンジニアリングチームにはSSOとMFA、エンタープライズレベルのID管理、専用ワークスペースを備え、リポジトリ・ビルドシステム・本番環境への不正アクセスを防ぎ、ソースコードや設計資料、社内ドキュメントを組織の管理下に保持します。SDLC全体でサプライチェーンリスクも含めた対策を徹底します[10]。

セキュリティ設計は、学習データ段階でのフィルタリングにより、暴力的・露骨・ヘイトイメージ等のセンシティブなコンテンツを除外し、他の安全対策と組み合わせて有害データを多層的に排除します。展開準備では社内外レッドチームと多数の評価を重ね、18歳以上への提供、人物コンテンツの取り扱い制限、未成年関連プロンプトの厳格なモデレーションなど、動画のように文脈依存性が高い領域で使用制約を設計に組み込みます。CSAMについては生成のブロック、アップロードのフィルタリング・監視、高度な検出、確認時のNCMEC報告を行い、子どもの安全を最優先する運用を徹底します。これらはSoraやDALL·Eを含むモデル群で実績ある手法の延長です[3][4]。

セキュリティ設計は、Model Specに基づく安全要件を明確化し、メンタルヘルス、自傷・自殺、AIへの感情的依存といったセンシティブ領域で、問題定義・測定・外部専門家との検証・リスク軽減という標準プロセスでモデル応答を強化します。今後は、長年のベースラインに加えて、感情的依存や自殺を伴わない緊急事態も安全性テストの標準セットに組み込み、評価と改善の一貫性を高めます[11]。併せて、有害タスクやプロンプトインジェクションに対する専門的な安全学習などのモデルレベル対策と、エージェントのサンドボックス化やネットワークアクセスの構成可能性といったプロダクトレベル対策を併用し、多層の設計で運用上の安全性を確保します[1]。

セキュリティ設計は、ポリシー適用を標準化できる推論ベースのセーフガードにも取り組み、gpt‑oss‑safeguardをオープンウェイトで研究プレビュー公開しました。任意のポリシーを推論時に適用でき、ポリシー準拠のコンテンツ分類に適し、Chain‑of‑ThoughtやStructured Outputsに対応し、Apache 2.0で提供します。主要なエンドユーザー機能への直接適用は推奨せず、用途境界を明確にした運用を促します[13][12]。この推論アプローチは、モデルが安全ポリシーを直接学習して何が安全かを推論する「熟慮的アライメント」とも整合し、学習・推論・運用の各層における多層防御を補完します[13]。

セキュリティ設計は、Preparedness Frameworkに基づく安全性テストとレッドチーミングをデプロイ前に実施し、システムカードの公開や専門家と協働した評価で安全性の標準化を進めます。高度な推論能力に関する研究知見を取り込みながら、安全性評価の厳格化と運用標準の改善を継続します[1][14]。

セキュリティ設計は、サイバー・レジリエンスの強化、プロンプトインジェクションの理解、責任ある脆弱性開示の拡張など、最新の課題と対策を継続的に発信し、SoraやChatGPT、API製品で培った堅牢な安全システムを共通土台として、専門家との協働、レッドチーム評価、システムカードの公開を通じて、安全性の設計・運用・標準化を一体で推進します[2][3][4][1][5][8]。

【出典】
[1] https://openai.com/ja-JP/safety/
[2] https://openai.com/ja-JP/news/security/
[3] https://openai.com/ja-JP/index/sora-system-card/
[4] https://openai.com/ja-JP/index/sora-is-here/
[5] https://openai.com/ja-JP/trust-and-transparency/
[6] https://openai.com/ja-JP/security-and-privacy/
[7] https://openai.com/ja-JP/business-data/
[8] https://openai.com/ja-JP/solutions/use-case/content-creation/
[9] https://openai.com/ja-JP/business/chatgpt-pricing/
[10] https://chatgpt.com/ja-JP/business/ai-for-engineering/
[11] https://openai.com/ja-JP/index/strengthening-chatgpt-responses-in-sensitive-conversations/
[12] https://openai.com/ja-JP/index/gpt-oss-safeguard-technical-report/
[13] https://openai.com/ja-JP/index/introducing-gpt-oss-safeguard/
[14] https://openai.com/ja-JP/index/learning-to-reason-with-llms/


## プラットフォーム機能・連携・セキュリティ
プラットフォーム機能・連携・セキュリティは、企業のお客様が現場でモデルの運用・最適化・標準化を実現できるよう、プラットフォームの機能、連携性、そしてエンタープライズグレードのセキュリティを中核に据えて設計し、既存システムと安全に統合できる堅牢な土台の上で、データ保護とコンプライアンスの原則を製品全体で一貫して適用します[1][8]。この方針のもと、コンテンツやワークフローを組織の基準に合わせて標準化しながら、運用の可視化・統制を高い水準で両立します[6]。

プラットフォーム機能・連携・セキュリティは、実務システムとの連携を重視し、コネクターやMCP（Model Context Protocol）を通じてChatGPTをCMS、DAM、デザインツールに接続するほか、要件に応じたカスタムAPI統合にも柔軟に対応します。APIをCMSに接続することで、承認済みコンテンツを各地域・形式に合わせて即時に翻訳・適応させるローカリゼーションの自動化も可能です。これらの連携は「システムに安全に統合」できることを前提としており、既存ツールへの接続と同時にデータをプライベートかつ保護された状態に維持します[1]。また、ChatGPT Businessでは会社のドキュメントやCRM、チケット、データに接続して回答をパーソナライズし、独自のGPTを構築・共有して全社でベストプラクティスを展開できます。導入から拡張までを短時間で行える設計のもと、専用ワークスペースや一元化された請求管理など運用を簡素化する仕組みを提供します[10][5]。プラン体系や提供機能の詳細は公式の価格ページで随時ご確認いただけます[5][9]。

プラットフォーム機能・連携・セキュリティは、組織全体での統制・標準化を支える管理機能を幅広く提供します。ID連携と強固な認証ではSAML SSOやMFA（TOTP）に対応し、ワークスペース管理では専用ワークスペース、中央集約された請求、管理コンソール、メンバーの一括管理、GPTの分析と管理を提供します。さらに、SCIMによるアカウントの自動プロビジョニング、ロールベースのアクセス制御（RBAC）、エンタープライズキー管理（EKM）、ドメイン認証、プロジェクトとプロジェクト制限、Admin APIとAudit Logs API、使用状況ダッシュボードまで、運用の可視化と標準化を支える管理・監査機能を備えています[5][6][8]。

プラットフォーム機能・連携・セキュリティは、ビジネスデータの保護を最優先とします。ChatGPT Enterprise/Business/EduおよびAPIプラットフォームにおいて、お客様の組織データを当社モデルの学習に使用しない運用を明確にし、Enterpriseプランでは「コンテンツをモデル学習に使用しない」ことを保証します[3][5]。データは転送時にTLS 1.2以上、保存時にAES‑256で暗号化し、ゼロトラストと多層防御の設計により、エンドポイント・インフラ・ネットワーク・アプリケーション全体にセキュリティコントロールを実装しています。セキュリティチームは24時間365日のオンコール体制で監視・対応し、独立した第三者による定期的な侵入テストと、脆弱性の適切な報告を促進するバグバウンティ・プログラムを運用しています[8][7]。さらに、EKMによるお客様管理の鍵、データ保持期間の制御、APIプラットフォームにおけるゼロデータ保持オプションなど、保持・鍵管理・アクセスの統制も強化しています[6][3]。

プラットフォーム機能・連携・セキュリティは、グローバルな法令・規制対応においてもお客様を支援します。APIおよびChatGPT EnterpriseなどはSOC 2 Type 2の対象であり、BusinessおよびEnterpriseではISO 27001、27017、27018、27701の認証取得を示しています。GDPRやCCPA順守の支援に加え、データ処理補遺契約（DPA）に対応し、保護対象保健情報（PHI）を扱うお客様にはAPIでの利用に関してビジネスアソシエート契約（BAA）の締結に応じています。APIで入出力されたデータは、特定のエンドポイントを除き、サービス提供と不正利用の特定のために30日間安全に保持されます。ファインチューニングで送信されたデータはお客様の管理下で保持され、他のお客様への提供や別モデルの学習には使用されません[3][8]。

プラットフォーム機能・連携・セキュリティは、安全性の標準化を支える推論ベースの安全性分類モデル「gpt‑oss‑safeguard（120b/20b）」を研究プレビューとして公開しています。開発者は推論時に独自のポリシーを指定し、ポリシーと対象コンテンツを同時に入力することで、分類の結論とその推論を得られます。Responses APIに対応し、完全な思考の連鎖（CoT）と構造化出力をサポート、低・中・高の3段階の推論レベルを選択可能です。主用途はポリシーに基づく分類であり、エンドユーザーが直接対話する主機能としての使用は推奨しません。コミュニティからのフィードバックを取り入れつつ、モデル性能の反復改善を継続します[2][4]。

プラットフォーム機能・連携・セキュリティは、エンジニアリングチームがソースコードや設計・社内ドキュメントを非公開のまま組織管理下に保持し、SSOと多要素認証、専用ワークスペースによる一元管理でプロジェクト横断のセキュリティ・コンプライアンス・可視性を確保できるよう支援します[11][8]。また、政府からのデータ要求、児童の安全性、EUデジタルサービス法（DSA）に関する情報を含む透明性レポートや、セキュリティ・プライバシー・コンプライアンスの取り組みを確認できる企業向け信頼ポータルなどのリソースを公開し、プラットフォーム運用の透明性を高め、セキュリティ標準の継続的な改善に取り組みます[7]。

【出典】
[1] https://openai.com/ja-JP/solutions/use-case/content-creation/
[2] https://openai.com/ja-JP/index/introducing-gpt-oss-safeguard/
[3] https://openai.com/ja-JP/enterprise-privacy/
[4] https://openai.com/ja-JP/index/gpt-oss-safeguard-technical-report/
[5] https://openai.com/ja-JP/business/chatgpt-pricing/
[6] https://openai.com/ja-JP/business-data/
[7] https://openai.com/ja-JP/trust-and-transparency/
[8] https://openai.com/ja-JP/security-and-privacy/
[9] https://chatgpt.com/ja-JP/pricing?openaicom-did=42e5238f-df80-4fb0-97b4-9a29a43ea699&openaicom_referred=true
[10] https://chatgpt.com/ja-JP/business/
[11] https://chatgpt.com/ja-JP/business/ai-for-engineering/


## モデルアクセス
モデルアクセスは、開発者・組織・クリエイター・消費者が安全かつ柔軟にモデルへアクセスできる世界を実現するため、アクセス経路の多様化とガードレールの標準化を同時に推進します。その中核として、モデルアクセスはOpenAIが公開した望ましいモデルの振る舞いの基準「Model Spec」を運用の指針に据え、適用法の遵守や「API利用時における開発者の指示がユーザー指示に優先する」といった指揮系統の明確化を徹底します。Model Specは一般からのフィードバックを継続的に取り入れて更新され、変更点や関連研究の進捗が定期的に共有されます。モデルアクセスはこのプロセスに則り、標準の継続的改善と透明な情報公開を進めます[1][3]。

モデルアクセスは、アイデンティティ管理・権限管理・データ取り扱いの既定を実装し、用途とリスクに応じた複数の経路で制御可能なアクセス体験を提供します。データの取り扱いでは、OpenAI API経由で送信されたデータはモデル学習やサービス改善に使用されない一方、ChatGPTやDALL·Eなど非APIの消費者サービスに送信されたデータはモデル改善に用いられる場合があり、ユーザーはプライバシーリクエストポータルからオプトアウトできます。OpenAIはオンライン公開情報によるプロファイル作成・広告ターゲティング・ユーザーデータ販売を行わず、モデルは問い合わせのたびに新たな文章を生成するため、後で思い出すために情報を保存する設計を採用していません。さらに、学習に用いる個人情報の量の削減や、個人・機密情報の要求を拒否する学習、プライベート・センシティブ情報を含む回答の最小化を重視します[5]。企業・教育機関を含むビジネスデータについては、2023年3月1日以降、明示的なオプトインがない限りモデル学習に使用しない既定を運用し、社内アクセスは必要性が認められ承認を受けた従業員（エンジニアリングサポート・悪用調査・法令遵守）と、機密保持義務下で悪用・誤用の検証のみを行う専門の第三者請負業者に限定します[10]。加えて、ユーザーはプライバシーリクエストポータルを通じて、オプトアウトや個人データの閲覧・削除・修正・移転・異議申立て等の権利を行使できます[5][11]。

モデルアクセスは、組織のガバナンス要件に応えるアクセス管理機能を備えます。APIプラットフォームでは、多要素認証（MFA、TOTP対応）、SAML SSO、プロジェクトとプロジェクト制限、Admin API、Audit Logs API、使用状況ダッシュボードを提供し、権限・利用状況・支出の精緻な統制と監査可能性を実現します。ChatGPT Enterprise／Eduでは、SCIMによるユーザーライフサイクル管理、ロールベースアクセス制御（RBAC）、ユーザーアナリティクスを提供し、組織横断のアクセス管理と可観測性を強化します。ChatGPT BusinessはSSOと基本的な利用分析を備え、ワークスペース全体のモデル・機能利用状況を可視化するハブとして機能し、データはデフォルトで学習に使用されません。SSOや多要素認証によるアカウント保護、権限・ブランド・コンプライアンスの一元管理も可能です[12][13]。

モデルアクセスは、APIアクセスに加えてローカル実行という選択肢を重視します。OpenAIが提供するApache 2.0ライセンスのテキスト専用オープンウェイト推論モデル gpt‑oss‑120b / gpt‑oss‑20b は、Responses APIとの互換性、高い指示追従、ウェブ検索やPython実行などのツール利用を含むエージェント型ワークフローへの統合、タスクに応じた推論レベルの調整、完全な思考の連鎖（CoT）や構造化出力への対応など、開発者の実装を加速する設計です[2]。さらに、「どこでも実行できる」オープンウェイトとしてgpt‑oss（120B/20B）と、カスタム安全性ポリシーをサポートするgpt‑oss‑safeguard（120B/20B）を提供し、Hugging FaceやGitHubからダウンロード可能で、ブラウザで試せるインタラクティブなデモも用意されています[8]。

同時に、モデルアクセスはオープンモデルに固有のリスク特性と責任分担を明確化します。オープンウェイトはAPI内のシステムレベル保護とは異なり、公開後に悪意のあるファインチューニングで安全措置が回避され得る点、OpenAI側で追加の安全措置を施したりアクセスを取り消したりすることが不可能な点を率直に説明し、状況に応じて開発者や企業が自ら追加の安全対策を実装し、OpenAI製品に組み込まれている保護機能を補完する必要があることを明確にします。こうした理由から、gpt‑ossでは「システムカード」ではなく「モデルカード」として情報が提供されています[2]。

高リスク領域への段階的アクセスについて、モデルアクセスは多層防御と限定公開を組み合わせて運用します。動画生成モデルSoraでは、プロンプトフィルタリング、ブロックリスト、分類器の閾値調整など、リスク領域ごとの緩和策を適用し、早期アクセスプログラムにおいて60カ国以上・300名超のユーザーから50万件超のフィードバックを収集しました。これに基づく改善として、C2PAメタデータの埋め込みを維持しつつ、有料ユーザーに可視透かしなしで動画をダウンロードできる選択肢を提供するなど、クリエイティブな柔軟性と製品レベルの厳格な対策の両立を図りました[7]。さらに、信頼できる第三者による独立評価を展開可否の判断に組み込み、評価サマリーをシステムカードで公開したり、当社レビュー後の詳細な研究成果の公開を支援するなど、透明性の高い運用を続けています。これらの外部評価はフロンティアモデルにも適用され、GPT‑5に対しても運用されています[4]。

モデルアクセスは、Model Specへの実世界準拠度を継続的に測定・改善します。各原則の準拠をテストする挑戦的プロンプトセットを整備し、準拠の大幅な改善を確認しています。今後も、Model Specの改訂や研究進捗を定期的に共有しながら、ガードレールの標準を高めていきます[1][3]。

安全で有用な応答品質の向上に向け、モデルアクセスは学習手法の最適化にも取り組みます。ルールベース報酬（RBR）をRLHFに統合し、有害・デリケート領域における応答タイプ（断固とした拒否、柔らかい拒否、従うべき無害な要求）を明確化することで、過剰拒否を抑えつつ安全性と有用性のバランスを改善します[6]。あわせて、ChatGPTではメンタルヘルスや自傷・自殺、AIへの感情的依存といったセンシティブな会話領域で不適切応答を大幅に削減し、Model Specに沿った振る舞いを一層強化しました[9]。

モデルアクセスは今後も、Model Specの継続的改善、外部評価の活用、プライバシー保護と安全性基準の強化、そしてAPI・オープンウェイト・製品それぞれのアクセス経路に適したガードレールの整備を進め、モデルアクセスの運用・最適化・標準化を前進させます[1][3][4][5][8]。

【出典】
[1] https://openai.com/ja-JP/index/introducing-the-model-spec/
[2] https://openai.com/ja-JP/index/gpt-oss-model-card/
[3] https://openai.com/ja-JP/index/sharing-the-latest-model-spec/
[4] https://openai.com/ja-JP/index/strengthening-safety-with-external-testing/
[5] https://openai.com/ja-JP/consumer-privacy/
[6] https://openai.com/ja-JP/index/improving-model-safety-behavior-with-rule-based-rewards/
[7] https://openai.com/ja-JP/index/sora-system-card/
[8] https://openai.com/ja-JP/open-models/
[9] https://openai.com/ja-JP/index/strengthening-chatgpt-responses-in-sensitive-conversations/
[10] https://openai.com/ja-JP/enterprise-privacy/
[11] https://openai.com/ja-JP/policies/privacy-policy/
[12] https://openai.com/ja-JP/business-data/
[13] https://chatgpt.com/ja-JP/business/ai-for-data-science-analytics/


## モデル・技術基盤
モデル・技術基盤は、モデル運用・最適化・標準化のための技術基盤を「望ましいモデル行動の標準化」「安全性の継続的運用改善」「解釈可能性と効率の研究開発」「多様な実行環境での展開可能性」という柱で一体的に整備しています。まず、モデル・技術基盤はモデル行動の共通指針であるModel SpecをCC0で公開し、カスタマイズ性・透明性・知的自由を重視しつつも、爆弾の作り方やプライバシー侵害の詳細手順などの有害出力を許さないガードレールを明確に維持します。政治的・文化的にデリケートな問いには特定の意図を推進せず思慮深く答え、重大な危害が生じない限りあらゆるアイデアを議論可能とする原則を採用します。現実世界でのパフォーマンス把握に向けて、人間の専門家レビューとモデル生成を組み合わせた挑戦的な評価プロンプトを収集・運用し、昨年5月時点の当社最高システム比でModel Spec準拠が大幅に改善したことを確認しました。Model SpecはRLHFに携わる研究者・AIトレーナー向けガイドラインとしても活用し、モデルがModel Specから直接学習する可能性の探索を進めます。政策立案者、信頼される機関、分野専門家などグローバルなステークホルダーと対話し、一般からのフィードバックも受け付けながら、目的・ルール・デフォルトを見直す公開プロセスを継続します[1][2]。

モデル・技術基盤は、センシティブな会話領域での安全性運用をModel Specに基づき強化しています。最近の更新では、メンタルヘルス上の懸念、自傷・自殺行為、AIへの感情的依存に焦点を当て、これらを安全性テストの標準ベースラインに組み込みました。ユーザーの現実世界の人間関係を尊重し、精神的・感情的な苦痛に関わる根拠のない信念を肯定せず、潜在的な妄想や躁状態の兆候に安全かつ共感的に対応し、自傷・自殺リスクを示唆する間接的シグナルにも注意を払うようSpecを改訂。潜在的危害のマッピング、評価や実会話データを用いた測定、外部専門家との検証、緩和策の実装という手順で、優先ドメインごとにChatGPTの回答品質を体系的に改善しています[5]。この整合の基盤として、モデル・技術基盤は人間のデモンストレーションと出力ランク付けを用いたRLHFによりInstructGPTを学習し、1.3Bモデルが175BのGPT‑3より人間評価者に好まれる結果を得てAPIのデフォルトとして展開しました[9]。

評価・ベンチマーク基盤では、モデル・技術基盤はOpenAI Evalsをオープンソース化してコミュニティ参加による継続的改善を推進し[13]、ソフトウェア工学タスクに対しては内部インフラで検証済みの477タスクからなるSWE‑bench Verified固定サブセットで評価を実施しています。スキャフォールド（外部ツールやRAG等）により性能が大きく変動することを確認しており、Preparedness Frameworkの下で学習前・学習中・学習後（外部システム統合後を含む）にわたる継続的評価体制を運用しています[10]。医療分野では、新しいHealthBenchにより緊急時の案内、専門知識に基づくコミュニケーション、不確実性の伝達、回答の深さ、健康データタスクなどの観点でモデルの安全性・有用性を評価します[12]。加えて、推論性能の運用最適化としてo3およびo4‑miniを導入し、ツール使用時にAIME 2025で高いpass@1を達成するなど、サイズとコストに対する優れた性能を示しました。これらの評価では高い「推論努力」設定を用い、SWE‑benchは検証済み477タスクで実行しています。指示追従性と検証可能性も改善し、より自然な対話体験を実現しています[11]。

解釈可能性と効率の研究では、モデル・技術基盤は多数の重みをゼロに固定したスパースモデルを訓練し、少数の可解読な「回路（circuits）」で複雑な振る舞いを実現しながら性能を保てる可能性を示しました。抽出した回路はタスク達成に十分で、特定のエッジを削除すると動作が破綻するなど必要条件であることも確認。変数バインディングのような複雑な挙動についても予測可能性を高める部分的説明を提示しています。スパース訓練の非効率を克服するため、密結合モデルからスパース回路を抽出するアプローチや、より効率的な新訓練手法の開発を検討しており、将来のシステムをより分析・デバッグ・評価しやすくするツール群の構築を進めます[3]。

多様な実行環境での展開可能性に向けて、モデル・技術基盤はローカルからデータセンターまでどこでも動作するオープンウェイトの推論モデル群を提供しています。gpt‑oss‑120bおよびgpt‑oss‑20bはエージェント型タスクに最適化され、柔軟なカスタマイズ性と寛容なライセンス、思考の連鎖の完全可視化を備えます。gpt‑oss‑safeguardはカスタム安全性ポリシーをサポートし、導入先ごとの要件に沿ったガードレールの標準化を可能にします。Hugging FaceやGitHubで配布し、ブラウザで試せるPlayground、モデル性能の公開、モデルシステムカードや総合的な安全性テストなど、安全・品質基準の透明化も進めています[4][6]。ハードウェア面ではNVIDIA、AMD、Cerebras、Groqと連携して最適性能を確保し、MicrosoftはONNX Runtimeを用いたgpt‑oss‑20bのGPU最適化版をWindowsデバイスに導入、Foundry LocalやVS CodeのAIツールキットを通じてローカル推論を支援します。開発者はモデルを完全にカスタマイズして独自環境に展開でき、将来的なgpt‑ossのAPIサポートも検討します[6]。

新領域の映像生成でも、モデル・技術基盤は多層的な安全設計と運用標準を構築しています。Soraでは、レッドチームによる反復的な攻撃的評価を通じて入出力フィルタリングの課題を特定し、プロンプトフィルタリングやブロックリスト、分類器の閾値調整、モデレーション基準の設定、人物を含むメディアアップロードの保護強化、違反コンテンツへの加工リスク低減のための分類器強化などを重層的に実装しました。透かしやC2PAメタデータの運用を行いつつ、アーティストのワークフローに配慮して有料ユーザーの透かしなしダウンロードを可能にするなど、実運用での柔軟性とリスク低減の両立を図っています。9カ月の早期アクセスでは60カ国以上・300人超のユーザーから50万件以上のフィードバックを得て、モデル挙動や安全プロトコルの改善に反映。ヌード、選挙に関する欺瞞的コンテンツ、自傷行為、暴力といった主要領域の内部評価を整備し、緩和策の強化やモデレーション基準設定に活用しています[7]。

エンタープライズ運用面では、モデル・技術基盤はデータサイエンスとアナリティクスのためのChatGPT Business統合ワークスペースを提供し、コネクターを備えた一つのハブでチームを集約。特徴量生成や前処理パイプラインの自動化、モデル結果の解釈、インサイトの成果物化までを支援します。組織データはデフォルトで学習に使用せず、SSOや多要素認証、権限・ブランド・コンプライアンスの一元管理により、安全かつ標準化された運用を実現しています[8]。

モデル・技術基盤は、行動標準（Model Spec）、安全性運用の継続的改善、解釈可能性研究、オープンウェイトモデルと多様なハードウェア・ツールチェーンの最適化を組み合わせ、モデルの運用・最適化・標準化を前進させます。コミュニティとともに開かれた改善プロセスを継続し、より安全で透明性が高く、現実世界で信頼できるAI基盤を構築していきます[1][2][3][4][5][6][7][8][9][10][11][12][13]。

【出典】
[1] https://openai.com/ja-JP/index/sharing-the-latest-model-spec/
[2] https://openai.com/ja-JP/index/introducing-the-model-spec/
[3] https://openai.com/ja-JP/index/understanding-neural-networks-through-sparse-circuits/
[4] https://openai.com/ja-JP/open-models/
[5] https://openai.com/ja-JP/index/strengthening-chatgpt-responses-in-sensitive-conversations/
[6] https://openai.com/ja-JP/index/introducing-gpt-oss/
[7] https://openai.com/ja-JP/index/sora-system-card/
[8] https://chatgpt.com/ja-JP/business/ai-for-data-science-analytics/
[9] https://openai.com/ja-JP/index/instruction-following/
[10] https://openai.com/ja-JP/index/introducing-swe-bench-verified/
[11] https://openai.com/ja-JP/index/introducing-o3-and-o4-mini/
[12] https://openai.com/ja-JP/index/healthbench/
[13] https://openai.com/ja-JP/index/gpt-4-research/


