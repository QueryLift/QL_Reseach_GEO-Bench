# AI・データのガバナンス・安全
## 概要
### 基本姿勢
OpenAIは、長期的安全性と協力を中核に、設計・運用・開示・執行を単一の共通ルール（使用ポリシーとModel Spec）で統治し、法令遵守とプライバシー保護を基盤に据えます。OpenAIは、高リスク領域における人間の確認なき重大意思決定の自動化を認めず、取締役会のSafety & Security Committeeと外部評価を組み合わせて透明性ある監督を継続します。OpenAIは、企業・開発者・個人のすべてに適用されるガバナンス枠組みを公開し、変更ログと異議申立て手続を整え、継続的に改訂します。

### 重点的取り組み
OpenAIは、2025年10月29日に使用ポリシーを全製品共通規定として更新し、Model Spec、ルールベース報酬（RBR）、gpt‑oss‑safeguard（120b/20b）などで「ルールをモデル行動へ翻訳」する多層防御を前進させました。OpenAIは、SoraやGPT‑5系でのレッドチーミング、透かし/C2PA、未成年者保護（年齢予測・CSAMスキャン）を強化し、メンタルヘルス等のセンシティブ領域で不適切応答を体系的に低減しました。OpenAIは、データレジデンシーの拡大、EKM/SSO/SCIM/RBAC、ゼロデータ保持、SOC 2 Type 2やISO群への適合を通じてエンタープライズ級のデータ保護と運用統制を提供し、第三者評価やRed Teaming Challenge、Aardvarkなどで安全性エコシステムを拡充します。

### 重要事実
OpenAIは、2025年10月のモデル更新でメンタルヘルス領域などの不適切応答を65〜80%削減し、Model SpecとRBRを用いた安全性向上を定量化しました。OpenAIは、使用ポリシー（2025年10月29日更新）を公開し、未成年者保護・プライバシー尊重・高リスク用途の禁止を明確化するとともに、gpt‑oss‑safeguardの技術レポートを公開しました。OpenAIは、Enterprise/Business/Edu/APIをSOC 2 Type 2の監査対象とし、企業コンテンツを学習に使用しない運用、世界各地域へのデータレジデンシー拡大、METR/Apollo/IrregularによるGPT‑5・o1の外部評価公開、全入出力のCSAMスキャンとDSA透明性レポートの定期発行を継続しています。


## ガバナンス・共通ルール
ガバナンス・共通ルールは、OpenAIにおけるAI・データのガバナンスを、設計・運用・開示・執行まで一貫した実効性のある共通ルールで統一します。中核となる「使用に関するポリシー」では、なりすまし、政治運動・ロビー活動・選挙干渉、機密性の高い領域における人間の確認のない重大な意思決定の自動化を禁止し、重要インフラ、金融、行政サービス、製品安全、国家安全保障、法執行といった高リスク分野に関する指針を明確にしています。ポリシーは変更ログとともに継続的に改訂し、2025年10月29日には当社の製品・サービス全体に共通する一般規定を反映する形で更新しました。違反の報告手段や誤った執行に対する異議申立ても整備し、必要に応じてアクセス停止等の措置を講じます。これらの共通ルールはユーザー、開発者、企業顧客のすべてに適用される当社の基本的なガバナンス枠組みです[1][2]。

ガバナンス・共通ルールは、法令遵守をガバナンスの基盤に据えます。利用規約において、制裁・輸出管理を含む貿易関連法令の遵守を求め、禁輸対象国・地域や取引が制限されている個人・団体の利益となる利用、禁止された最終用途への利用、政府許可が必要な資料・情報をインプットに含める行為を禁止しています[2]。

ガバナンス・共通ルールは、モデルの振る舞いを統制するためにModel Specを公開し、違法行為を助長しないなどのルールを明文化しました。ルールやデフォルトに関する一般の方々からのフィードバックを受け付け、今後1年間にわたり変更や研究進捗の最新情報を定期的に共有します。ルールに反する悪用が確認された場合には、利用規約に基づきアカウントに対する措置が講じられる可能性を明確にしています[3][2]。

ガバナンス・共通ルールは、センシティブな領域の安全性を継続的に強化します。最新の更新では、メンタルヘルス、自傷・自殺、AIへの感情的依存といった優先領域に焦点を当て、ユーザーの関係性を尊重し、根拠のない信念を肯定せず、潜在的な妄想や躁状態の兆候に共感的かつ安全に対応する方針を明確化しました。運用では、潜在的な危害のマッピング、評価・実会話データ・ユーザーリサーチによるリスクの測定、外部の専門家によるレビュー、リスク軽減の実装という段階的プロセスで改善を進めます[4]。

ガバナンス・共通ルールは、ルールの実装と一貫した執行のために、透明性とコンテンツモデレーションの体制を運用します。プライバシーポリシーに基づく監視では、分類器やリーズニングモデル、ハッシュマッチング、ブロックリストなどの自動検知、ユーザーからの報告、人による審査を組み合わせて違反の可能性のあるコンテンツに対応し、違反が確認された場合はアカウントへのアクセス制限、ユーザーへの通知、コンテンツ共有の制限、検索結果のブロック、GPTの公開設定の制限、フォーラム管理などの措置を講じます[5]。加えて、コンテンツの報告経路、EUデジタルサービス法（DSA）に基づく透明性レポート、子どもの安全に関する定期レポート、企業向けの信頼ポータルなど、利用者と組織がガバナンスを確認・活用できるリソースを提供し、報告・異議申立て手続も案内しています[6][1]。

ガバナンス・共通ルールは、ルールをモデル行動に翻訳する技術的アプローチにも投資します。ルールベース報酬（RBR）をRLHFに統合し、簡潔な命題とルールに基づいて応答の「受容」「有用」「許容不可」などを評価する固定言語モデルのグレーダーでルール遵守度をスコアリングすることで、違法行為の助長リクエストは適切に拒否しつつ、合法的文脈（例：万引き防止の知識など）には有用な回答を提供するなど、安全性と有用性の両立を図ります[7]。さらに、開発者が自身のポリシーに基づきコンテンツを分類できるgpt‑oss‑safeguardモデル群（主用途はポリシー準拠の分類であり、エンドユーザーと直接対話する主要機能としての使用は非推奨）を公開し、Responses API互換性やチャット環境での安全性指標も検証した技術レポート（2025年10月29日）を提供しています[8]。

ガバナンス・共通ルールは、生成AIの安全スタックを強化するため、動画生成モデルSoraで包括的なレッドチーム評価を実施しました。単純・敵対的プロンプトの双方で多数のカテゴリーを検証し、画像・動画アップロードや編集ツール（ストーリーボード、リカット、リミックス、ブレンド）を含めた違反生成リスクを評価。医療関連やSF/ファンタジー設定が性的コンテンツの安全対策を弱めうるといった観測結果を踏まえ、暗示的なプロンプトや比喩など敵対的戦術による回避可能性も含めて、反復的に安全対策を改善しました[10]。

ガバナンス・共通ルールは、未成年者に対して安全性を最優先する共通ルールを適用します。ChatGPTの利用状況に基づく年齢予測システムを導入し、疑いがある場合には18歳未満向け設定をデフォルトとし、必要に応じて身分証明書の提示をお願いすることがあります。未成年ユーザーには成人と異なるルールを適用し、他者への危害の計画や社会規模の重大なインシデントに関わる内容は人間のレビューへエスカレーションします。一方、成人ユーザーについては、広い安全性の範囲内で自由と操作性を尊重します[9]。

ガバナンス・共通ルールは、組織向けの共通統制・運用ルールも整備しています。BusinessおよびEnterpriseプランでは、SAML SSO、専用ワークスペース、管理コンソール、メンバーの一括管理、ドメイン認証、GPTの分析と管理、SOC 2 Type 2、ISO 27001/27017/27018/27701などのコンプライアンスに対応。Enterpriseではロールベースのアクセス制御（RBAC）、SCIM、エンタープライズキー管理を提供し、Enterpriseプランのコンテンツはモデル学習に使用しない運用ルールを適用します[11]。

ガバナンス・共通ルールは、ポリシーの更新履歴の公開、モデル行動の原則化、監視・施行と上訴の仕組み、透明性レポート、開発者向けの安全ツールという複層的な取り組みを通じて、AI・データのガバナンスにおける共通ルールを継続的に強化します。公開された枠組みへのフィードバックを受け取り、改訂と検証を重ねながら、より安全で有用なAIの実装を推進していきます[1][3][6]。

【出典】
[1] https://openai.com/ja-JP/policies/usage-policies/
[2] https://openai.com/ja-JP/policies/terms-of-use/
[3] https://openai.com/ja-JP/index/introducing-the-model-spec/
[4] https://openai.com/ja-JP/index/strengthening-chatgpt-responses-in-sensitive-conversations/
[5] https://openai.com/ja-JP/transparency-and-content-moderation/
[6] https://openai.com/ja-JP/trust-and-transparency/
[7] https://openai.com/ja-JP/index/improving-model-safety-behavior-with-rule-based-rewards/
[8] https://openai.com/ja-JP/index/gpt-oss-safeguard-technical-report/
[9] https://openai.com/ja-JP/index/teen-safety-freedom-and-privacy/
[10] https://openai.com/ja-JP/index/sora-system-card/
[11] https://openai.com/ja-JP/business/chatgpt-pricing/


## セキュリティ技術戦略
セキュリティ技術戦略は、AIの高度化に伴って増大するリスクに先手で対応するため、モデル、製品、データ、そしてオープンなエコシステム全体にわたる多層的な防御を推進します。基盤として、国際的なプライバシー法への準拠、第三者監査、継続的な侵入テスト、DPAの提供、GDPR・CCPA適合の支援、HIPAA要件のサポートを整備し、SOC 2 Type 2の監査対象をAPIおよびChatGPT Enterprise/Team/Eduへ拡大するなど、規制・契約要件に応える体制を強化しています[1][13]。エンタープライズ環境では、ビジネスデータを既定でモデル改善に使用せず、保存時AES‑256・転送時TLS 1.2+による暗号化、Enterprise Key ManagementやAPIのゼロデータ保持オプションを含むデータ保持制御を提供し、ゼロトラストを前提にしたSDLCおよびエンドポイント・インフラ・ネットワーク・アプリケーション横断の多層コントロール、24時間365日の監視とインシデント対応を実施します。あわせて、SAML SSO・SCIM・RBAC・ドメイン認証・ISO 27001/27017/27018/27701など運用統制を支える機能群を備え、MFAや安全な統合により既存システムと確実に連携できるよう設計しています[12][1][13][2][14]。

セキュリティ技術戦略は、モデルと製品の両レイヤーで安全性を重ねる多層防御を採用します。Soraでは、事前フィルタリング、レッドチーミング、分類器・モデレーション、透かしおよびC2PA対応を組み合わせ、未成年者の安全、肖像の不正利用、誤解を招く生成といった高リスク領域に重点的な緩和策を講じています。NCMECとの連携、Thornの推奨に基づく評価、入出力のCSAMスキャンなどを運用し、実運用での検証結果を踏まえてプロンプトフィルタやブロックリスト、分類器閾値、より強力な分類器の適用を継続的に改善しています[3]。また、GPT‑5系のコーディング機能に関するシステムカード補遺で示したとおり、有害タスクやプロンプトインジェクションへの特化学習と、エージェントのサンドボックス化やネットワークアクセス制御といったプロダクト側の緩和を併用し、モデル・製品の両面で防御を重ねる方針を明確にしています[4]。

セキュリティ技術戦略は、安全性の推論を中核に据えます。オープンウェイトのgpt‑oss‑safeguard（120b/20b）を公開し、開発者が指定するポリシーに基づくコンテンツ分類を、思考の連鎖や構造化出力とともに実現します。主用途が分類であること（エンドユーザー対話での直接利用は非推奨）や評価結果を明示し、柔軟で説明可能な判断を提供します[6][5]。加えて、社内の主要リーズニングモデルでは、安全ポリシーを直接学習し「何が安全か」を自ら推論する熟慮的アライメントを採用・発展させ、従来手法より柔軟かつ強固な多層防御を実現しています[6][4]。

セキュリティ技術戦略は、外部評価を安全性エコシステムの柱として運用します。独立評価・方法論レビュー・専門家プロービングという三つの形式でフロンティアAIの能力とリスクを測定し、透明性と機密性のバランス、厳格なアクセス管理、結果に依存しない適切な報酬という原則のもとで第三者評価を継続しています。METR、Apollo Research、Irregular等によるGPT‑5やo1に関する報告の公開と並行し、モデル能力やテストニーズの進化に応じて管理手段を更新しています[7]。

セキュリティ技術戦略は、攻撃耐性とオープンモデルのリスク評価を攻守一体で進めます。gpt‑ossの公開に先立ち、生物学やサイバーセキュリティの専門データで意図的に「拒否しない」悪意ファインチューニングを行い、Preparedness Frameworkに照らした能力評価を実施した結果、当社スタックで極めて大規模に悪意ファインチューニングしても高い能力レベルには達しないことを確認し、この手法は三つの独立専門家グループのレビューを受けています。併せて、世界の研究者・開発者を対象に賞金総額50万ドルのRed Teaming Challengeを開催し、オープンエコシステムの安全性向上を促進しています[15][7]。

セキュリティ技術戦略は、エージェント時代の防御力を高めるため、GPT‑5を基盤とするエージェント型セキュリティリサーチャー「Aardvark」を展開します。コードリポジトリの継続的な解析を通じて、脆弱性の検出、悪用可能性の検証、優先順位付け、パッチ提案までを支援し、非商用のオープンソースリポジトリには無償スキャンを提供します。協調開示ポリシーも更新し、厳格な期限よりコラボレーションとスケーラブルな影響を重視する方針を採用しました[8]。

セキュリティ技術戦略は、ユーザープライバシーを中核に据えます。過度に広範なデータ要求には法的措置で対抗し、匿名化・個人識別情報の削除、厳格な法的プロトコルに基づく安全な閲覧環境の整備、継続的な透明性の確保を徹底します[9]。

セキュリティ技術戦略は、最前線の知見共有とインシデント対応の透明性を継続します。サイバー・レジリエンスの強化、プロンプトインジェクションの理解と対策、責任ある開示の拡充、ならびにインシデント時の適時な情報提供を含む技術的見解とアップデートを公開しています[10]。

セキュリティ技術戦略は、ポリシーと運用の両輪で安全性を執行します。使用ポリシーを明確化し、モデレーションツールと実践的ガイダンスを開発者に提供、監視・審査体制で遵守を促し、不正利用の通報手段を整備します。過度な制限を避けつつユーザー保護を最適化するため、ユースケースの進化に応じて規則を継続的に更新していきます[11]。

セキュリティ技術戦略は、準拠性とエンタープライズ防御、モデル/製品レイヤーの多層防御、推論に基づく安全性、第三者評価、攻守一体のツール群、プライバシー保護、ポリシー執行を統合し、AIの有用性を最大化しながら社会にとって安全な進歩を実現します[1][3][4][5][6][7][8][9][11][12][13][14][15]。

【出典】
[1] https://openai.com/ja-JP/security-and-privacy/
[2] https://openai.com/ja-JP/solutions/use-case/content-creation/
[3] https://openai.com/ja-JP/index/sora-system-card/
[4] https://openai.com/ja-JP/safety/
[5] https://openai.com/ja-JP/index/gpt-oss-safeguard-technical-report/
[6] https://openai.com/ja-JP/index/introducing-gpt-oss-safeguard/
[7] https://openai.com/ja-JP/index/strengthening-safety-with-external-testing/
[8] https://openai.com/ja-JP/index/introducing-aardvark/
[9] https://openai.com/ja-JP/index/fighting-nyt-user-privacy-invasion/
[10] https://openai.com/ja-JP/news/security/
[11] https://openai.com/ja-JP/policies/usage-policies/
[12] https://openai.com/ja-JP/business-data/
[13] https://openai.com/ja-JP/business/chatgpt-pricing/
[14] https://chatgpt.com/ja-JP/business/ai-for-engineering/
[15] https://openai.com/ja-JP/index/introducing-gpt-oss/


## データ保護ポリシー
データ保護ポリシーは、OpenAIのAI・データのガバナンスの中核として、個人データの保護と責任ある取り扱いを徹底します。当社はプライバシーポリシー、使用に関するポリシー、データ処理に関する補足事項（DPA）、エンタープライズプライバシー、共有と公開に関するポリシー、協調的脆弱性開示ポリシーなどの主要ドキュメントを「規約とポリシー」ポータルで体系的に公開し、法人・開発者・個人ユーザーを含むすべての利用者がデータの取扱いと安全対策を一元的に確認できるようにしています[2]。また、APIやChatGPTのビジネス向け製品では、GDPRやCCPAへの準拠を支援する包括的なデータ保護を実装し、CSA STARおよびSOC 2 Type 2の基準にも準拠しています[5]。

データ保護ポリシーは、プライバシーポリシーに基づき、個人データの保持期間を当社のサービス提供、紛争解決、安全・セキュリティ、法的義務の遵守など正当なビジネス目的に必要な期間に限定し、目的、情報の量・性質・機微性、不正利用等による危害リスク、法的要件、ユーザー設定などの要因を考慮して決定します。たとえば、ChatGPTの一時的なチャットは安全上の理由から履歴に表示されず、最大30日間のみ保有します。あわせて、利用者は居住地に応じて、個人データへのアクセス、削除、更新・修正、取扱い方法の制限、第三者への移転（データポータビリティ）などの権利を行使できます[1]。

データ保護ポリシーは、法人のお客様がデータ主権要件を満たせるよう、ヨーロッパ地域でのデータレジデンシーを導入し、APIプラットフォームおよびChatGPT EnterpriseやChatGPT Eduのデータを地域内で保管できる選択肢を提供します。ビジネス向け製品におけるお客様データの機密性と安全性を守るため、データの所有権はお客様に帰属し、これらのデータはAIモデルの学習には使用しません。さらに、GDPR等の規制における役割と責任を明確にする包括的なDPAを提供し、個人情報が適切かつ安全に扱われることを徹底する位置づけを明示しています[5][2]。

データ保護ポリシーは、プラットフォーム上でのプライバシー侵害を予防するため、使用に関するポリシーで「プライバシーの尊重」を明示し、本人の許可なく他者の私的・機微な情報を収集・監視・プロファイリング・配布する行為を禁止します。特に、同意のない顔認識データベースの構築、公共の場でのリアルタイム遠隔生体認証、同意のない人物の肖像（写真のようにリアルな画像や声を含む）の誤認利用、社会的スコアリングや機微属性推測を含む評価・分類、職場や教育環境での感情推測（医療・安全上の必要を除く）、特性やプロファイリングのみに基づく犯罪リスクの予測といった用途を認めません[4]。

データ保護ポリシーは、法的要請への対応においても利用者のプライバシーを最優先に保護します。特定の裁判所命令に基づくコンテンツは安全なシステムで隔離してリーガルホールド下で保管し、監査済みの小規模な法務・セキュリティチームのみが必要最小限のアクセスを行います。アクセス時には厳格な法的プロトコルと安全な閲覧環境を適用し、データの匿名化や個人識別情報の削除などの追加措置を講じます。当社は当社のプライバシー基準と一致しない要求には異議を申し立て、皆様のプライバシーをあらゆる手段で守るとともに、進捗や影響についての重要な更新情報を透明性をもって共有します[3]。

データ保護ポリシーは、透明性と説明責任の確保のため、子どもの安全性に関する半期レポートやEUデジタルサービス法（DSA）に基づく透明性レポート等の関連文書、製品内での報告機能の案内、GPTに関する異議申し立て手順、そして企業向け信頼ポータルによるデータセキュリティ・プライバシー・コンプライアンスのドキュメントやFAQへのアクセスを提供しています[6]。

データ保護ポリシーは、これらの方針と実務を通じ、最高水準のセキュリティ、プライバシー、コンプライアンスを維持しながら、世界中の組織と連携して安全で信頼できるAI活用を推進していきます[5]。

【出典】
[1] https://openai.com/ja-JP/policies/privacy-policy/
[2] https://openai.com/ja-JP/policies/
[3] https://openai.com/ja-JP/index/fighting-nyt-user-privacy-invasion/
[4] https://openai.com/ja-JP/policies/usage-policies/
[5] https://openai.com/ja-JP/index/introducing-data-residency-in-europe/
[6] https://openai.com/ja-JP/trust-and-transparency/


## データ保護方針・原則
データ保護方針・原則は、AIとデータのガバナンスにおいてプライバシー保護を中核原則とし、法令遵守、ユーザーコントロール、強固なセキュリティを統合した運用を徹底します。データ保護方針・原則は、個人データの収集・利用・共有に関する透明性を重視し、適用法に基づくユーザーの権利を尊重して運用します。あわせて、ユーザーが第三者と共有した情報は当該第三者の規約・プライバシーポリシーに従うことを明確化します[1]。

データ保護方針・原則は、個人データの保持を、サービス提供、紛争解決、安全・セキュリティ、法的義務の履行など正当なビジネス目的の範囲に限定し、目的、情報の性質や機微性、不正利用時の潜在的リスク、法的要件といった要素を考慮して最小限に保有します。ユーザー設定に応じた保持期間の制御を提供し、たとえばChatGPTの一時的なチャットは安全上の理由から履歴に表示されず最大30日間のみ保有します。また、ユーザーがアクセス、削除、更新・修正、取扱いの制限、データポータビリティなどの権利を行使できる仕組みを整備します[1]。ビジネス用途では、要件を満たす組織がAPIでゼロデータ保持を選択できるなど、保持に関する選択肢も提供します[4]。

データ保護方針・原則は、ビジネス向け製品とAPIにおいてお客様データの機密性と安全性を守り、データの所有権はお客様に帰属し、明示的な共有許可がない限りモデル学習に使用しないことを既定とします（ChatGPT Enterprise／Business／EduおよびAPIのデータは既定で学習に使用しません）[4]。さらに、欧州でのデータレジデンシーに対応し、API、ChatGPT Enterprise、ChatGPT Eduのデータを地域内に保存できる選択肢を提供します[2]。この機能は世界中のビジネス顧客にも拡大し、米国、欧州、カナダ、日本、韓国、シンガポール、インド、オーストラリア、UAEで提供を開始、会話、アップロードファイル、カスタムGPT、画像生成の成果物などのコンテンツを選択した地域内に保存できます。加えて、Enterpriseのお客様がAPIプラットフォームの高度なデータコントロールを有効化したプロジェクトでは、選択地域内でリクエストを処理し、モデルのリクエストや応答を当社サーバーに保存しない運用を実現します[3]。

データ保護方針・原則は、GDPRやCCPAなど各地域のプライバシー法への準拠を支援し、CSA STARやSOC 2 Type 2の基準に準拠した枠組み、ISO/IEC 27001・27017・27018・27701などの国際規格への適合、役割と責任を明確化する包括的なデータ処理契約（DPA）を提供します[2][3][4]。

データ保護方針・原則は、保存時のAES-256、通信時のTLS 1.2以上などの高度な暗号化を適用し、Enterprise Key Management（EKM）によりお客様自身の鍵を保存データに適用できる仕組みを提供します。ゼロトラストや多層防御、設計段階からのセキュリティ組込み、サプライチェーンリスク対策などのエンジニアリング実務を運用し、セキュリティチームが24時間365日のオンコール体制で自動アラートや手動調査により監視・対応します[4]。あわせて、AI利用におけるプライバシー権の保護を極めて重要と位置づけ、従業員によるアクセスを最小化するための高度なセキュリティ機能の開発を進める一方、深刻な乱用の検知を目的とした自動化モニタリングや、生命・他者への危害、重大なサイバーインシデントのような最大リスクに限った人間レビューへのエスカレーションといった、限定的かつ正当化された例外を明確に設けます[6]。

データ保護方針・原則は、法的手続においてもユーザーのプライバシーを守るため、過度に広範な要求には異議を唱え、必要に応じてデータの匿名化や個人識別情報の削除といった追加措置を講じます。裁判所命令の対象データは安全なシステムに隔離・保管し、リーガルホールド下で法的義務の履行以外の目的でアクセスされないようにします。アクセスは監査済みの小規模な法務・セキュリティチームに限定し、外部からの閲覧も厳格な法的プロトコル下で管理します。関連する重要な更新は透明性をもって継続的に共有します[7]。政府機関からの要請についてはKodexポータルで受け付け、適用法令を遵守しつつプライバシーと安全性を保護するために全件を精査し、透明性レポートを継続的に公開します。2025年上半期にはコンテンツ以外のリクエストを119件と公表し、児童安全に関してはNCMECに74,559件、サイバーチップラインへ75,027件の報告を行いました[8]。

データ保護方針・原則は、サービスの利用ルールとしてプライバシーの尊重を明確に定め、本人同意のない顔認識データベースの構築、公共空間でのリアルタイム遠隔生体認証、同意のない人物の肖像（写真のようにリアルな画像や声を含む）の使用、社会的スコアリングや機微属性の推測を含むプロファイリング、職場や教育現場での感情推測（医療・安全上の必要を除く）、プロファイリングのみに基づく犯罪リスクの評価・予測といった用途を禁止します。未成年者の安全保護も明確に規定し、搾取・危害・性的搾取を助長する利用は許可しません[5]。

データ保護方針・原則は、プライバシー、セキュリティ、コンプライアンスの最高水準を維持しながら、ユーザーと組織に対して選択可能なコントロール、透明性の高い運用、そして法令と倫理に適合したAIの利用環境を提供し続けます[2][3][4]。

【出典】
[1] https://openai.com/ja-JP/policies/privacy-policy/
[2] https://openai.com/ja-JP/index/introducing-data-residency-in-europe/
[3] https://openai.com/ja-JP/index/expanding-data-residency-access-to-business-customers-worldwide/
[4] https://openai.com/ja-JP/business-data/
[5] https://openai.com/ja-JP/policies/usage-policies/
[6] https://openai.com/ja-JP/index/teen-safety-freedom-and-privacy/
[7] https://openai.com/ja-JP/index/fighting-nyt-user-privacy-invasion/
[8] https://openai.com/ja-JP/trust-and-transparency/


## データ利用・セキュリティ方針
データ利用・セキュリティ方針は、AI・データのガバナンスと安全に関する明確な原則のもと、お客様の信頼を最優先に設計・運用します。ChatGPT Enterprise／Business／EduおよびAPIで取り扱うビジネスデータは既定でモデル学習に使用せず、ユーザーコンテンツの所有権はお客様に帰属します。適格な組織にはデータ保持の制御機能を提供し、APIではゼロデータ保持ポリシーを選択できます[1]。ヨーロッパでのデータレジデンシーでは、地域内での保管・処理に加え、該当データをモデル学習に使用しない方針とデータ所有権の帰属を明確化しています[3]。さらに、データレジデンシーをビジネス顧客向けに米国・欧州・カナダ・日本・韓国・シンガポール・インド・オーストラリア・UAEへ拡大し、EnterpriseではAPIダッシュボードからプロジェクト単位で保存地域を指定でき、当該プロジェクトのリクエストは地域内で処理され、モデルのリクエストや応答が当社サーバーに保存されない運用を提供します[13]。

データ利用・セキュリティ方針は、保存時にAES‑256、転送時にTLS 1.2以上の暗号化を適用し、Enterprise Key Management（EKM）によりお客様が自身の鍵を保存データ（ユーザーコンテンツ）に適用できる仕組みを提供します[1]。ゼロトラストと多層防御の原則を採用し、エンドポイントからアプリケーションまでセキュリティを組み込みます。セキュリティチームは24時間365日のオンコール体制で監視と対応を継続し、定期的な第三者侵入テストも実施しています[1][2]。コンプライアンスでは、GDPRやCCPA等への遵守を支援し、包括的なデータ処理補遺契約（DPA）を提供します。API、ChatGPT Enterprise、ChatGPT Team、ChatGPT EduはSOC 2 Type 2の評価対象であり、CSA STAR基準への準拠やHIPAAなどの規制・契約要件への対応支援も行います[2][3]。加えて、ChatGPT Business／EnterpriseはISO/IEC 27001／27017／27018／27701の認証を取得しています[4]。

データ利用・セキュリティ方針は、組織のアクセス管理とガバナンスを支える機能として、SAML SSO、ドメイン認証、管理者ロール、管理コンソール、一元化された請求管理、GPTの分析・管理を提供します。EnterpriseではSCIM、ロールベースのアクセス制御（RBAC）、エンタープライズキー管理（EKM）、アナリティクスダッシュボードを利用可能です[4]。BusinessではSSOと多要素認証に対応し、権限・ブランドガバナンス・コンプライアンスを一元的に管理できる専用ワークスペースを提供します[12]。

データ利用・セキュリティ方針は、信頼と透明性を重視し、政府からのユーザーデータへの要求、児童の安全性、コンテンツモデレーションに関する透明性レポートを継続的に公開します。EUデジタルサービス法（DSA）に基づく透明性レポートのほか、企業向けの信頼ポータルも整備しています[5]。プライバシーポリシーに基づき、個人データの紛失・不正利用・不正アクセス・漏洩・改ざん・破壊を防ぐため商業上合理的な技術的・運用的・組織的措置を講じる一方、インターネットや電子メールの伝送が常に完全に安全であるとは限らないことから、提供情報の選択にはご留意いただくようお願いしています[6]。使用に関するポリシーではプライバシー侵害行為（同意のない顔認識データベースの構築、公共空間でのリアルタイム遠隔生体認証、同意なき肖像の誤認誘発的利用、プロファイリングや社会的スコアリング等）を明確に禁止し、未成年者の安全に関する基準を定め、違反を許容しません。開発者には実践的なモデレーションツールとガイダンスを提供し、違反報告の手段を整備し、知見に応じて規則を更新します[7]。

データ利用・セキュリティ方針は、モデル開発におけるデータガバナンスを徹底します。例えばSoraでは、学習前にデータセットをフィルタリングし、暴力的・露骨・ヘイトなどの有害コンテンツを排除します。提供にあたってはレッドチーム評価と段階的アプローチを採用し、18歳以上への提供、人物の顔写真・動画の使用制限、未成年者関連のより厳格なモデレーションなどを運用します[8]。過度に広範なデータ提供要求からユーザーのプライバシーを守るため、法令遵守を前提に強い姿勢で対処し、ニューヨーク・タイムズによる2,000万件以上のプライベート会話提出要求に対しては匿名化や個人識別情報の削除、厳格な法的プロトコル下での安全な閲覧環境の確保などの追加措置を講じつつ、これに異議を申し立てています。進展については透明性をもって適宜情報を共有します[9]。また、プロンプトインジェクションやサイバー・レジリエンスなど最前線のセキュリティ課題に関する取り組みも継続的に発信します[10]。セキュリティエコシステムへの貢献として、エージェント型セキュリティリサーチャー「Aardvark」の提供や、オープンソース向けの無償スキャン、協調的な開示ポリシーの改善を進めています[11]。

データ利用・セキュリティ方針は、データを守り、安全で責任あるAI活用を実現するために、データ利用とセキュリティの両面からガバナンスを強化し続けます。ヨーロッパを含む各地域のデータ主権要件に対応するデータレジデンシーや高度な暗号化、GDPR／CCPA対応、包括的なDPAの提供とお客様によるデータコントロールの強化を一体として進化させ、実運用と規制要件に確実に応えます[1][3][13]。

【出典】
[1] https://openai.com/ja-JP/business-data/
[2] https://openai.com/ja-JP/security-and-privacy/
[3] https://openai.com/ja-JP/index/introducing-data-residency-in-europe/
[4] https://openai.com/ja-JP/business/chatgpt-pricing/
[5] https://openai.com/ja-JP/trust-and-transparency/
[6] https://openai.com/ja-JP/policies/privacy-policy/
[7] https://openai.com/ja-JP/policies/usage-policies/
[8] https://openai.com/ja-JP/index/sora-system-card/
[9] https://openai.com/ja-JP/index/fighting-nyt-user-privacy-invasion/
[10] https://openai.com/ja-JP/news/security/
[11] https://openai.com/ja-JP/index/introducing-aardvark/
[12] https://chatgpt.com/ja-JP/business/ai-for-data-science-analytics/
[13] https://openai.com/ja-JP/index/expanding-data-residency-access-to-business-customers-worldwide/


## ポリシー案内・一覧
ポリシー案内・一覧は、AI・データのガバナンスと安全を中核に据え、OpenAIの全社的な規約・ポリシーを体系的に公開し、利用者が容易に参照できるよう一元的に案内しています。ポリシー案内・一覧は、日本語サイトの「規約とポリシー」ページで、個人向けの利用規約、プライバシーポリシー、サービス利用規約、データ処理に関する補足事項、特許の扱い方、コネクターとアクションの規約、サービスクレジットに関する規約、事業・法人・開発者向けのサービス規約をはじめ、「利用規定（使用に関するポリシー）」「当社のポリシーに沿った画像と動画の作成」「当社のポリシーに沿った Operator の使用」「エンタープライズプライバシー」「共有と公開に関するポリシー」「協調的脆弱性開示に関するポリシー」「外部向けの協調開示ポリシー」「英国の税務戦略」「お客様のデータとモデルのパフォーマンス」など、製品とデータの利用に関する主要方針を横断的に示しています[1]。

ポリシー案内・一覧は、「使用に関するポリシー（利用規定）」を継続的に更新し、安全性と責任ある利用を徹底します。なりすまし、政治運動・ロビー活動・国内外の選挙干渉・政治参加を妨げる活動、人間の確認なしの重大な意思決定の自動化（重要インフラ、金融活動・与信、重要な行政サービス、製品安全コンポーネント、国家安全保障、法執行など）といった高リスク領域に関する禁止事項を明確に定め、変更ログも公開しています。2025年10月29日には製品・サービス全体に共通する一般規定の反映、2025年1月29日には適用法に基づく禁止事項の明確化、2024年1月10日には内容の明確化とサービス固有指針の追加、2023年2月15日にはユースケース/コンテンツポリシーの統合と高リスク業界の具体的指針追加を実施しました。さらに、2022年11月9日にはアプリケーション登録要件を廃止して自動・手動の違反監視へ移行し、2022年10月25日には申請後の承認不要化（ポリシー準拠が前提）、結果ベースアプローチへの移行、安全ベストプラクティスの更新などを行った経緯を公開しています[2]。

ポリシー案内・一覧は、ユーザーのプライバシーを尊重し、取得する個人情報を安全に管理する方針を明示します。2025年6月27日に更新したプライバシーポリシーでは、ウェブサイト・アプリケーション・サービスの利用に伴う個人情報の取扱いを定めるとともに、欧州経済地域・英国・スイス向けの地域版を提供しています。なお、事業者向けサービスで顧客のためにコンテンツを取り扱うような一部のB2B提供形態には本ポリシーが適用されない場合があることも明確にしています[3]。

ポリシー案内・一覧は、透明性とコンテンツモデレーションに関する専用方針を公開し、規約・ポリシー違反の可能性があるコンテンツに対して、分類器やリーズニングモデル、ハッシュマッチング、ブロックリストなどの能動的検知、ユーザーからの報告、人による審査を組み合わせて対応します。違反が確認された場合は、アカウントの制限、違反の可能性と結果の通知、コンテンツ共有の制限、検索結果のブロック、GPTの公開設定の制限、フォーラム管理などの措置を講じます（最終更新日：2025年9月18日）。Sora探索フィードやChatGPT検索におけるコンテンツ表示の考え方も併せて説明しています[4]。

ポリシー案内・一覧は、信頼と透明性を高めるため、子供の安全性に関する半期レポート、EUデジタルサービス法（DSA）に基づく透明性レポートや第16条通知、DSA連絡先、製品内での適切な報告機能、GPTに関する異議申し立ての手続き、ならびに企業顧客向けにデータセキュリティ・プライバシー・コンプライアンス情報を提供する「企業向け信頼ポータル」への導線を提供しています[5]。

ポリシー案内・一覧は、ユーザープライバシー保護の観点から、第三者による過度なデータ要求にも厳格に対応します。特定の訴訟で裁判所命令の対象となるコンテンツは安全なシステムに隔離しリーガルホールド下で保護し、法的義務の履行に必要な場合のみ、監査済みの少数の法務・セキュリティチームがアクセスします。第三者が閲覧する場合も厳格な法的プロトコル下の安全な環境に限定し、匿名化や個人識別情報の削除などの追加措置を講じ、重要な更新情報は透明性をもって随時共有します[6]。

ポリシー案内・一覧は、利用者・開発者の皆様が最新のルールを把握できるよう、「使用に関するポリシー」の更新通知にサインアップできる手段を提供しています。更新時に通知を受け取ることで、常に最新のガードレールに沿った利用を実現できます[7]。

【出典】
[1] https://openai.com/ja-JP/policies/
[2] https://openai.com/ja-JP/policies/usage-policies/
[3] https://openai.com/ja-JP/policies/privacy-policy/
[4] https://openai.com/ja-JP/transparency-and-content-moderation/
[5] https://openai.com/ja-JP/trust-and-transparency/
[6] https://openai.com/ja-JP/index/fighting-nyt-user-privacy-invasion/
[7] https://openai.com/ja-JP/form/usage-policy-update/


## リスク評価・可視化と優先順位付け
リスク評価・可視化と優先順位付けは、OpenAIのプロダクトのライフサイクル全体で「予測・評価・防止」を循環させる公式見解のもと、データのフィルタリング、ポリシー整備、人間の価値観の反映、レッドチーム、システムカード、Preparednessによる評価、安全委員会、アルファ/ベータ/GA、ユーザーフィードバックという一連のプロセスを日々運用し、継続的な改善を進めます[1]。リスク評価・可視化と優先順位付けは、評価手法そのものも定期的に更新し、スケーラブルで飽和しにくい測定を開発・共有することで、長期にわたる安全性パフォーマンスの理解とコミュニティの透明性向上を図ります[2]。

リスク評価・可視化と優先順位付けは、フロンティアAIの重大リスク領域の特定と優先順位付けにPreparedness Frameworkを適用し、説得、サイバーセキュリティ、CBRN（化学・生物・放射線・核）、モデル自律性の4カテゴリで評価します。Soraではこれらのカテゴリに関して重大なリスクの証拠は確認されず、モデルの機能範囲に照らして評価結果を明確化し、システムカードと併せて公開しています[3]。あわせて、同一モデルでもエージェントのスキャフォルディングなど外部強化で能力が変動し得ることを前提に、学習前・学習中・学習後（外部システム統合後を含む）にわたる継続評価を求め、静的データセット評価の限界（分布の偏りや汚染可能性）を補完する設計で、壊滅的リスクを経験的かつ科学的に追跡・防御します[7]。

リスク評価・可視化と優先順位付けは、安全性エコシステムの中核に外部評価を据え、独立評価・方法論レビュー・専門家によるプロービングという複数の形式で能力とリスクを測定します。METRのtime horizon評価やSecureBioのVCT評価などの第三者ベンチマークをPreparedness評価に補完的に活用し、そのために初期モデルやチェックポイントへの安全なアクセス、選択的な評価結果の共有、必要に応じたゼロデータ保持、軽度に緩和されたモデルの提供、さらには思考の連鎖への限定的アクセスを、厳格なセキュリティ管理の下で提供してサンドバッギングやスキーミングの事例特定に役立てています。これらの外部評価はGPT‑5にも適用し、モデル能力やテストニーズの進化に応じて管理手段を更新します[4]。

リスク評価・可視化と優先順位付けは、内部評価とレッドチーミングでも優先領域を明確化し、結果に基づく緩和を反復します。Soraではヌード、選挙に関する欺瞞的コンテンツ、自傷行為、暴力といった主要領域を対象に、入力プロンプトと入出力の分類器を組み合わせた評価フレームワークを構築し、アルファ段階の実使用データ、レッドチームの敵対例、GPT‑4による合成データの3経路から評価データを収集して、エッジケースの発見、評価セットの拡充、モデレーション基準設定、緩和策強化へと結びつけています[3]。さらにChatGPT‑4oでは、社会心理・偏見・公正・誤情報等の領域の70名超の外部専門家と広範なレッドチーミングを実施し、音声モダリティの新規リスクを踏まえて段階的リリースを判断、追跡対象リスクの緩和前後スコアを提示するスコアカードで可視化とガバナンスを強化しました[8]。

リスク評価・可視化と優先順位付けは、推論を活用して安全性を高めます。思考の連鎖を通じて安全規則と文脈化された推論方法を学習させることで分布外シナリオへの堅牢性を高め、主要なジェイルブレイク評価や社内の難易度が高い拒否境界ベンチマークで大幅な改善を確認し、デプロイ前にはPreparedness Frameworkに従う安全性テストとレッドチーミングを実施、評価過程で報酬ハッキングの興味深い事例も観測し、その詳細をシステムカードに整理・公開しています[5]。加えて、ルールベース報酬（RBR）をRLHFに統合し、固定グレーダーによるルール準拠度スコアをPPOの追加信号として用いることで、過剰拒否を抑えつつ安全性と一般ベンチマーク性能の両立を定量的に追跡・最適化しています[11]。また、推論時に任意の安全ポリシーを適用して柔軟・説明可能なラベリングを可能にするオープンウェイトの安全性分類モデル「gpt‑oss‑safeguard」を研究プレビューとして公開し、社内の主要推論モデルでも安全ポリシーを直接学習する「熟慮的アライメント」を採用して多層防御の実装可能性を広げています[10]。

リスク評価・可視化と優先順位付けは、センシティブな会話領域のリスクを優先的に低減します。直近のモデル更新では、メンタルヘルス、自傷・自殺、AIへの感情的依存を重点領域に位置づけ、(1) 潜在的危害のマッピング、(2) 評価・実会話データ・ユーザーリサーチによる測定の開始、(3) 外部のメンタルヘルス/安全専門家との定義・ポリシーの検証、(4) モデル・ポリシー・製品側の介入による軽減というステップで運用し、不適切な応答を65〜80%削減しました。今後は感情的依存や非自殺性の緊急事態も安全性テストのベースラインに追加し、準拠と安全性を継続的に高めます[6]。

リスク評価・可視化と優先順位付けは、評価の信頼性・妥当性を高めるためのベンチマーク整備にも取り組みます。SWE‑benchから人手で検証した500サンプルを抽出した「SWE‑bench Verified」を導入し、テストの妥当性や問題記述の明確さを保証するとともに、外部強化で性能が大きく変動し得る事実を実証してPreparednessにおける継続評価の重要性を裏付け、静的データセット評価の限界を補完する設計を組み込みました[7]。

リスク評価・可視化と優先順位付けは、組織のガバナンスと運用上の可視化を支える基盤として、Business/Enterprise向けにGPTの分析と管理、管理コンソール、ドメイン認証、ロールベースのアクセス制御、アナリティクスダッシュボード等の機能を提供し、利用状況の把握やリスク監督を可能にしています[9]。また、モデル横断の可視化を高めるため安全評価ハブを公開し、許可されていないコンテンツ、ジェイルブレイク、ハルシネーション、命令階層評価などの結果を共有、モデルの能力・モダリティの進化に伴う評価の飽和を回避するため手法を定期更新し、システムカードやPreparednessの評価と合わせて包括的な理解を提供します[2]。

リスク評価・可視化と優先順位付けは、外部評価・内部評価・推論による安全性・公開スコアカード・評価ハブ・エンタープライズのガバナンス機能を結合し、リスクを可視化しながら優先度を明確にして対処します。今後も、実証に基づく継続的評価と透明性、専門家コミュニティとの協働を通じて、安全なAIの実現に取り組みます[1][2][4][7]。

【出典】
[1] https://openai.com/ja-JP/safety/
[2] https://openai.com/ja-JP/safety/evaluations-hub/
[3] https://openai.com/ja-JP/index/sora-system-card/
[4] https://openai.com/ja-JP/index/strengthening-safety-with-external-testing/
[5] https://openai.com/ja-JP/index/learning-to-reason-with-llms/
[6] https://openai.com/ja-JP/index/strengthening-chatgpt-responses-in-sensitive-conversations/
[7] https://openai.com/ja-JP/index/introducing-swe-bench-verified/
[8] https://openai.com/ja-JP/index/hello-gpt-4o/
[9] https://openai.com/ja-JP/business/chatgpt-pricing/
[10] https://openai.com/ja-JP/index/introducing-gpt-oss-safeguard/
[11] https://openai.com/ja-JP/index/improving-model-safety-behavior-with-rule-based-rewards/


