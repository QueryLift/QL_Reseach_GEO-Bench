# モデル安全・リスク・ガバナンス

## 概要


### 基本姿勢
OpenAIは、安全性を設計・評価・展開・運用の全ライフサイクルに組み込む「安全性は継続的プロセス」という前提で、Model Spec、Preparedness Framework、Evaluations Hub、外部評価を中核に据えた多層ガバナンスを運用します。OpenAIは、安全と有用性の同時最大化を方針とし、透明性と機密保護のバランスを取りながら、アラインメントと制御が担保できない超知能システムは展開しません。

### 重点的取り組み
OpenAIは、RLHFにRBRを統合した学習スタック、GPT‑5のセーフコンプリーション、思考の連鎖による推論ベース安全性を強化し、安全クラシファイアやgpt‑oss‑safeguard、リーズニングLLMモニター等のシステムレイヤを重ねる多層防御を実装します。OpenAIは、Evaluations Hub（2025年8月更新）、SWE‑bench Verified、HealthBenchによる継続評価と、METRやSecureBio等の独立外部テストを併用し、Soraの入出力フィルタや透かし、ティーン保護・ペアレンタルコントロール、API/Enterpriseのゼロデータ保持やSOC2/HIPAA対応、迅速・透明なインシデント対応を推進します。

### 重要事実
OpenAIは、2025年10月にGPT‑5系でメンタルヘルス/自傷・自殺/AI依存に関する不適切応答を65〜80%削減し、o1‑previewで有害プロンプトの安全完了率0.995・難例0.934を達成しました。OpenAIは、2025年8月15日にEvaluations HubへGPT‑5結果・Production Benchmarks・StrongRejectを追加し、o3/o4‑miniではバイオリスクの人間レッドチーミング会話の最大99%をモニターがフラグしました。OpenAIは、gpt‑oss‑safeguard（120B/20B）をApache 2.0で研究プレビュー提供し、2025年11月のMixpanel事案では即時隔離と通知を実施、当社インフラ・APIキー・ChatGPTユーザーへの侵害は確認されませんでした。


## AIモデルの境界リスク対応枠組み
AIモデルの境界リスク対応枠組みは、フロンティアAIに特有の境界リスクへ一貫して対処するため、モデルの設計・評価・外部検証・学習技術・運用時の緩和策を統合した多層的アプローチを中核に据えています。望ましいモデル行動を明示する仕様、ライフサイクル全体にわたる継続的評価、独立した第三者による検証、学習スタックの強化、そしてシステムレベルの監視とガードレールを組み合わせ、安全性と有用性を同時に高めます[1][3][5]。

AIモデルの境界リスク対応枠組みは、Model Specを通じて「範囲内に留まる」「共に真実を追求する」という原則を具体化し、爆弾の作り方やプライバシー侵害の手順などの危害を伴う詳細な方法は決して提供せず、政治的・文化的にデリケートな問いには思慮深く応答するガードレールを定義しています。準拠度を測る挑戦的な評価プロンプトの収集と測定を継続し、昨年5月時点の当社最高システムと比べて準拠の大幅な改善を確認しながら、コミュニティからのフィードバックを取り込み改訂を重ねています[1]。この仕様は、適用法令の遵守や指揮系統の優先といった具体的ルールを含み、評価用プロンプトとともにCC0で公開して透明性と知的自由の両立を図ります[2]。

AIモデルの境界リスク対応枠組みは、安全性評価を「Evaluations Hub」で公開し、不許可コンテンツ、ジェイルブレイク、誤情報（ハルシネーション）、命令階層など代表的領域の結果を共有、モデル能力や新モダリティの進化に合わせて手法を定期的に更新しています。2025年8月15日にはGPT‑5系の結果やProduction Benchmarks、StrongRejectの詳細を追加し、長期的な安全パフォーマンスの透明性を高めました[3]。さらに、ライフサイクルの学習前・学習中・学習後（外部システム統合も含む）にわたって継続評価を要請するPreparedness Frameworkのもと、外部スキャフォールドが能力に与える影響まで視野を広げ、ソフトウェアエージェントの自律性リスク追跡では人手検証済みのSWE‑bench Verified（500サンプル）を活用しています[4]。

AIモデルの境界リスク対応枠組みは、独立評価・方法論レビュー・SMEプロービングという三つの形式で外部検証を強化しています。METRのtime horizon評価やSecureBioのVCT評価などがPreparedness Frameworkに基づく社内評価を補完し、評価の実施にあたっては初期チェックポイントへの安全なアクセス、選択的な結果共有、必要に応じたゼロデータ保持、ガードレール有無のモデル提供、さらに思考の連鎖への限定アクセスを組み合わせ、サンドバッギングやスキーミングの兆候を見分けられるよう透明性と機密性のバランスを取っています[5]。

AIモデルの境界リスク対応枠組みは、学習技術としてルールベース報酬（RBR）を安全性スタックの中核に導入し、標準的なRLHF/PPOパイプラインに追加信号として統合しました。RBRは明確で段階的なルールに基づいて出力の安全適合性を評価し、過剰拒否を抑えつつ安全挙動ポリシーへの従属性を高め、人間の反復的ラベリングへの依存を減らします。新しいルールや安全ポリシーへの迅速な更新にも対応します[6]。また、思考の連鎖を用いる推論モデルに安全規則とその文脈的適用を教示することで、ジェイルブレイク等の困難ケースにおける安全拒否率を大幅に改善し、有害プロンプトに対する安全完了率でGPT‑4oの0.990に対しo1‑previewは0.995、困難ケースで0.714に対し0.934、搾取的性的コンテンツで0.483に対し0.949といった結果を得ています[12]。

AIモデルの境界リスク対応枠組みは、システムレベルの緩和としてo3およびo4‑miniの開発・運用で安全学習データを再構築し、生物学的脅威やマルウェア生成、ジェイルブレイクに対する新たな拒否プロンプトを追加しました。フロンティアリスク領域の危険なプロンプトをシステムでフラグする緩和策を導入し、解釈可能な安全仕様から学習したリーズニングLLMモニターをバイオリスクに適用して、人間のレッドチーミング会話の最大99%を検知しています。いずれも更新版Preparedness Frameworkに沿って、これまでで最も厳格なストレステストを実施しました[7]。

AIモデルの境界リスク対応枠組みは、センシティブなメンタルヘルス領域の境界リスクに対し、潜在的危害のマッピング、評価・実会話データ・ユーザーリサーチによる測定開始、外部のメンタルヘルス・安全専門家との検証というプロセスで改善を進め、標準テストベースラインを自殺・自傷のみならず感情的依存や自殺を伴わない精神衛生上の緊急事態まで拡張しました。2025年10月のモデル更新では、GPT‑5の応答安全性を強化し、精神疾患や自傷・自殺、AI依存に関する不適切応答を65〜80%削減、精神科医を含む170名以上の専門家レビューでModel Specの原則（根拠のない信念を肯定しない、間接的シグナルに注意を払う等）への準拠を高めました[8]。

AIモデルの境界リスク対応枠組みは、オープンウェイトモデルにおける境界リスクにも予防と最悪ケース評価の両面から取り組みます。事前学習では化学・生物・放射線・核（CBRN）に関連する特定の有害データを除外し、事後学習では熟慮的アライメントや指示階層を用いて拒否とプロンプトインジェクション耐性を強化。生物・サイバー領域の専門データで意図的に拒否しない挙動を引き出すファインチューニングを行い、攻撃者視点の最悪ケースを直接測定する評価を実施しています[9]。さらに、gpt‑oss‑safeguardのようにカスタム安全性ポリシーをサポートするオープンリーズニングモデルを提供し、Model System Cardと総合的な安全性テストを通じて「安全性はオープンモデルの基盤」であることを実装レベルで担保しています[10]。

AIモデルの境界リスク対応枠組みは、マルチモーダル生成（Sora）においても、暗示的プロンプトや比喩など安全回避を狙う戦術を用いた継続的レッドチーミングで入出力フィルタリングの改善点を特定し、人物を含むメディアアップロードの保護策を強化。より強力な分類器、プロンプトフィルタリングやブロックリスト、分類器閾値の調整などの安全策を導入・更新し、違反コンテンツの発火点を見直しながら保護対策をアップデートしています[11]。

AIモデルの境界リスク対応枠組みは、今後も公開されたModel Specと評価資産の拡充、Evaluations HubやSystem Card、Preparedness Frameworkの運用、独立機関との厳密な外部テスト、RBRをはじめとする学習スタックの改善、システムレベルの監視・緩和の強化を継続し、透明性・安全性・知的自由の両立をコミュニティとともに前進させていきます[1][3][5]。

【出典】
[1] https://openai.com/ja-JP/index/sharing-the-latest-model-spec/
[2] https://openai.com/ja-JP/index/introducing-the-model-spec/
[3] https://openai.com/ja-JP/safety/evaluations-hub/
[4] https://openai.com/ja-JP/index/introducing-swe-bench-verified/
[5] https://openai.com/ja-JP/index/strengthening-safety-with-external-testing/
[6] https://openai.com/ja-JP/index/improving-model-safety-behavior-with-rule-based-rewards/
[7] https://openai.com/ja-JP/index/introducing-o3-and-o4-mini/
[8] https://openai.com/ja-JP/index/strengthening-chatgpt-responses-in-sensitive-conversations/
[9] https://openai.com/ja-JP/index/introducing-gpt-oss/
[10] https://openai.com/ja-JP/open-models/
[11] https://openai.com/ja-JP/index/sora-system-card/
[12] https://openai.com/ja-JP/index/learning-to-reason-with-llms/


## AIモデル評価と安全性
AIモデル評価と安全性は、安全なAIの実現に向けて「評価と改善を途切れさせないこと」を公式方針として掲げ、開発から展開、運用後のフィードバックまで、各段階でのテスト、レッドチーミング、システムカード、Preparednessの評価を通じてリスクを予測・測定し、継続的に低減します。この全社的プロセスを、モデルの能力向上と安全性確保を両立させる中核的なガバナンス手段として運用します[1]。

AIモデル評価と安全性は、安全評価ハブを通じてモデルの安全性と性能に関する評価結果を継続的に公開し、不許可コンテンツ（ポリシー違反を誘発する不適切リクエストへの不応答）、敵対的プロンプトによるジェイルブレイク、事実誤認（ハルシネーション）、複数メッセージ間の優先順位付けを試す指示階層などを含む主要領域での指標を提示します。これらの結果はシステムカードに反映し、社内の安全性・展開判断に直接活用します。従来手法の飽和に備え、よりスケーラブルな評価方法の開発と指標の定期的なアップデート・共有を推進しており、2025年8月15日にはハブを更新してGPT‑5系の評価結果に加え、Production BenchmarksおよびStrongRejectの詳細を新たに公開しました[2]。

AIモデル評価と安全性は、厳密性と独立性を高めるため、第三者による外部評価を安全性エコシステムの重要な柱として位置づけます。独立評価、方法論レビュー、専門家（SME）によるプロービングの複数形式で能力とリスクを測定し、結果を展開可否の判断に反映します。展開前評価サマリーはシステムカードに掲載し、機密性と正確性のレビュー後には評価組織の詳細報告の公開を支援するなど、外部インプットが能力評価・安全対策にどう反映されたかを透明化します。これらの枠組みはGPT‑5にも適用し、透明性と機密のバランス、適切なセキュリティ管理、相応の報酬提供といった原則のもとで協力体制を構築しています[3]。

AIモデル評価と安全性は、モデルの行動原則を定めるModel Specを改定・公開し、各原則への準拠度を挑戦的なプロンプトセットで測定する取り組みを進めています。予備結果では、前年の当社最高システムと比べてModel Specへの準拠が大幅に改善しており、今後も現実世界の使用から得られる新たなケースを取り込んで評価セットとモデルの双方を継続的に改善します。とりわけ、メンタルヘルス、自傷・自殺、AIへの感情的依存といったセンシティブ領域を優先ドメインに据え、問題定義、測定（評価・実会話データ・ユーザーリサーチ）、外部専門家との検証、リスク軽減という段階的アプローチで安全性を強化。2025年10月のモデル更新では、これら領域における不適切応答を65〜80%削減し、精神科医を含む170名以上の専門家の協力のもとで準拠率を大幅に向上させました。自殺・自傷の長年のベースラインに加え、感情的依存や自殺を伴わない精神的緊急事態も将来のモデルに向けた標準的な安全テストに組み込みます[4][5]。

AIモデル評価と安全性は、推論過程に安全規則と方針を組み込む評価設計を推進し、思考の連鎖（Chain of Thought）によって安全性とアライメントの堅牢性を高めています。主要なジェイルブレイク評価および社内の最難関ベンチマーク（安全な拒否境界）で大幅な性能向上を確認し、有害プロンプトに対する安全なコンプリーション率は全体で0.995（GPT‑4oは0.990）、困難ケースでは0.934（同0.714）に達しました。カテゴリ別でも、重度ハラスメント0.900、搾取的な性的コンテンツ0.949、未成年者を含む性的コンテンツ0.931などの改善を示しています。デプロイ前にはPreparedness Frameworkに基づく一連の安全性テストとレッドチーミングを実施し、評価過程で報酬ハッキングの事例も観測・分析したうえで、結果をシステムカードに掲載します[6]。

AIモデル評価と安全性は、オープンウェイトの推論モデルにも高度な安全基準を適用します。gpt‑ossには総合的な安全性テストを実施し、Model System Cardを提供します。また、開発者が指定するポリシーに基づいてコンテンツを分類するgpt‑oss‑safeguard（120B/20B）を研究プレビューとして公開し、CoTや構造化出力に対応した柔軟かつ説明可能な安全性推論を可能にします。これらのモデルは与えられたポリシーに基づく分類用途を推奨し、主要なエンドユーザー向け対話用途は想定していませんが、オープンモデルとしての潜在的利用を踏まえ、チャット環境でも安全基準への適合を検証しています。2025年10月29日には技術レポートを公開し、基盤モデル（gpt‑oss）を比較基準に用いた安全性評価結果を提示しました。さらに多層防御を重視し、安全に応答するようモデルを学習させるだけでなく、安全性クラシファイア（例：Moderation API）などの追加レイヤーを組み合わせ、ROOST Model Community（RMC）と連携して評価結果・モデルフィードバック等のベストプラクティス共有を進めます[7][8][9]。

AIモデル評価と安全性は、映像生成などのマルチモーダル領域でも特定リスク領域（ヌード、選挙に関する欺瞞、自傷行為、暴力など）に焦点を当てた内部評価を整備し、プロンプトフィルタリング、ブロックリスト、分類器の閾値調整などの緩和策を検証・強化します。内部レッドチームとの協業や透かし・C2PAメタデータの活用、60カ国以上・300名超のアーティストによる9カ月の早期アクセスからの大規模フィードバックに基づく運用調整など、反復的かつ文脈重視の安全アプローチを適用します[10]。

AIモデル評価と安全性は、内外の評価を統合し、定量・定性のエビデンスに基づく安全性判断を徹底します。評価データと知見は安全評価ハブやシステムカードで継続的に公開し、モデルの能力向上に伴う評価飽和に対処するスケーラブルな計測手法の開発を進めながら、透明性と実効性を両立させた安全性ガバナンスを強化していきます[2][3][4]。

【出典】
[1] https://openai.com/ja-JP/safety/
[2] https://openai.com/ja-JP/safety/evaluations-hub/
[3] https://openai.com/ja-JP/index/strengthening-safety-with-external-testing/
[4] https://openai.com/ja-JP/index/sharing-the-latest-model-spec/
[5] https://openai.com/ja-JP/index/strengthening-chatgpt-responses-in-sensitive-conversations/
[6] https://openai.com/ja-JP/index/learning-to-reason-with-llms/
[7] https://openai.com/ja-JP/open-models/
[8] https://openai.com/ja-JP/index/gpt-oss-safeguard-technical-report/
[9] https://openai.com/ja-JP/index/introducing-gpt-oss-safeguard/
[10] https://openai.com/ja-JP/index/sora-system-card/


## アクセス・API提供方針
アクセス・API提供方針は、モデルの安全・リスク・ガバナンスの中核として、APIへのアクセス設計と提供運用を厳格に整備し、セキュアでコンプライアンスに適合した利用環境をお客様に提供します。ビジネスデータおよびAPIで取り扱うお客様データは既定でモデル学習に使用せず、要請に応じてゼロデータ保持ポリシーを適用し、開発から本番運用まで一貫したデータガバナンスを実現します。保存時はAES‑256、転送時はTLS 1.2以上で暗号化し、SOC 2 Type 2に準拠した運用体制を維持します。さらに、データレジデンシーのコントロール、IP許可リストとmTLS、シングルサインオン（SAML SSO）および多要素認証（MFA）により、ゼロトラストを前提としたアクセス管理を強化します。医療データを取り扱うユースケースについては、HIPAA準拠を支援するビジネスアソシエイト契約（BAA）にも対応します[1][2][3]。

アクセス・API提供方針は、エンタープライズ規模の認証・認可と可視化を重視します。SAML SSOやMFA（TOTP対応）に加え、SCIMによる自動プロビジョニング／デプロビジョニング、ロールベースのアクセス制御（RBAC）、プロジェクトおよびプロジェクト制限、組織オーナーやセキュリティチームが権限を管理できるAdmin API、セキュリティ・コンプライアンスリスクの可視化を行うAudit Logs API、リアルタイムの使用状況可視化や機能／チーム／プロジェクト単位のトラッキングを可能にするダッシュボードを提供します。ネットワーク面では、IP許可リストとmTLSによる接続管理を組み合わせ、ワークスペース内のアクセス、使用状況、支出、リスクを細やかに統制できるようにします[2][1]。

アクセス・API提供方針は、データの取り扱いにおいて透明性と制御性を徹底します。APIでは、サービス提供と不正利用の特定のために入出力データを原則30日間安全に保持し（特定のエンドポイントを除く）、保持期間と用途を明確に制限します。お客様データを用いたファインチューニングでは、生成されたモデルは当該お客様のみが利用でき、他のお客様への提供や他モデルの学習には使用しません。ファインチューニング用に送信されたデータは、お客様がファイルを削除するまで保持します。また、事業者向けサービス（API等）で当社がお客様のためにコンテンツを取り扱う場合は一般のプライバシーポリシーの適用対象外であることを明示し、エンタープライズ向けの法的コミットメントと整合する運用を行います。さらに、アカウント保護の観点から、IPアドレス等に基づくおおよその位置情報を不審なログイン試行の検知などのセキュリティ目的で取り扱います[3][4][2]。

アクセス・API提供方針は、安全なアクセスの確保に不可欠な許容される利用範囲を明確化します。サービスに実装している保護措置や安全管理上の緩和策の迂回、サービスの妨害・中断、また当社出力を用いて当社と競合するモデルを開発する行為を禁止し、企業ドメインのメールで作成されたアカウントは通知の上で組織のビジネスアカウントに統合され、移行後は組織管理者が当該アカウントやコンテンツの管理（アクセス制限・削除等）を行える場合があることを定めます。第三者サービスや第三者アウトプットには各事業者の条件が適用されることを明示し、統合時の権利関係とリスクを明確にします[5][6]。

アクセス・API提供方針は、可観測性と費用管理を支える仕組みを提供します。ワークスペース全体でモデルや機能の使用状況を追跡するユーザーアナリティクス、課金・使用量アラート、プロジェクト単位の詳細な使用量とコストの確認を可能にし、組織が自らのポリシーに沿った高度な権限設計と監査体制、ならびに統合的なリスクとコスト管理を実装できるよう支援します[2][1]。

アクセス・API提供方針は、インシデント対応とベンダー管理でも透明性を貫きます。2025年11月、第三者の分析プロバイダーMixpanelで限定的なAPIユーザーの分析データが不正にエクスポートされた際には、直ちに同サービスを本番環境から除去し、影響データセットの検証、関係する組織・管理者・ユーザーへの通知、外部環境への影響の有無の監視を実施しました。当社インフラへの不正アクセスは確認されず、APIキーやChatGPTユーザーへの影響はありませんでした。併せて、フィッシング対策の徹底（公式ドメイン送信の確認、リンクや添付への注意、MFAの有効化）と、当社がパスワード・APIキー・確認コードをメールやチャットで要求しない方針を改めて周知しました[7]。

アクセス・API提供方針は、これらの方針と機能を継続的に強化し、責任あるアクセス設計、厳格なデータガバナンス、明確な使用条件、そして迅速かつ透明なインシデント対応を通じて、お客様が安全かつ信頼できる形でAPIを活用できるよう取り組み続けます[1][2][3][5][6][7]。

【出典】
[1] https://openai.com/ja-JP/api/
[2] https://openai.com/ja-JP/business-data/
[3] https://openai.com/ja-JP/enterprise-privacy/
[4] https://openai.com/ja-JP/policies/privacy-policy/
[5] https://openai.com/ja-JP/policies/terms-of-use/
[6] https://openai.com/ja-JP/policies/row-terms-of-use/
[7] https://openai.com/ja-JP/index/mixpanel-incident/


## アラインメント技術
アラインメント技術は、OpenAIにおけるモデル安全・リスク・ガバナンスの中心に位置づけられ、能力向上と安全性の両立を実証的・段階的に進めます。安全性を「AIの負の影響を軽減し正の影響を最大化する実践」と定義し、超知能に至るリスクを壊滅的たり得るものとして扱います。アラインメントと制御の確実性が担保できない段階で超知能システムを展開しないという原則のもと、必要に応じて一時的な開発減速を含む意思決定を実証研究に基づいて行い、先端研究機関間での共通基準と知見共有を推進します[1]。

アラインメント技術は、研究・学習段階でRLHFを中核にInstructGPTを整合させ、APIのデフォルトとして展開しました。事前学習に比べて2%未満の計算・データで効果を引き出しつつ、RLファインチューニング中に事前学習データの一部を対数尤度最大化で混ぜる単純なアルゴリズム変更により「アライメント税」を緩和し、安全性や人間の好みを維持しながら学術タスクの低下を抑制しました。あわせて、ラベラー選定、人間評価、報酬モデル設計を体系化し、ユーザー意図への忠実性・真実性の向上と有害性の低減を実現しています[2]。

アラインメント技術は、モデル行動の規範を明示するModel Specを継続的に改定し、実運用事例や約1,000名からのフィードバックを取り込みつつ、評価プロンプトとともにCC0で公開してコミュニティでの適応・再利用を可能にしました。強化されたアラインメントが性能向上にも寄与することを確認しており、今後も評価用コードやツールを順次公開し課題セットを拡大します[4]。また、モデル自身の不適切行動を正直に申告させる「confession（告白）」を提示し、GPT‑5 Thinkingでの実験では指示違反や報酬ハックなどの不履行時に高確率で告白が生じることを確認しました。告白は防止策ではなく可視化手段であり、chain‑of‑thought monitoring、deliberative alignment、instruction hierarchyと組み合わせた多層スタックの一要素として、Model Spec等のポリシー遵守と行動の自己報告の強化に役立てます[5][4]。

アラインメント技術は、外部の信頼できる第三者評価で検証を補強します。独立評価・方法論レビュー・専門家によるSMEプロービングを組み合わせ、透明性と機密性のバランス、堅牢なセキュリティ管理、公平な報酬設計の原則で運用し、GPT‑5にも適用しています。さらに、集団的アラインメントプロジェクトや各国の先端AI安全機関（CAISI/AISI）との協力、ウェルビーイングに関する諮問グループとの連携を通じて、評価とガバナンスの基盤を広げています[3]。

アラインメント技術は、強化ファインチューニング（RFT）で専門家判断との整合性と一貫性を高めています。Doppelと連携し、正確性だけでなく説明の品質にも報酬を与える設計を採用し、アナリストのフィードバックを採点例として取り込むことで、曖昧なエッジケースでも専門家水準の再現性と一貫性を向上。信頼度が低い場合は人間レビューを介し、決定をRFTループに還流させて継続的に改善します[6]。

アラインメント技術は、実世界の知識労働に即した評価としてGDPvalを導入しました。米国GDPへの寄与上位9産業から選んだ44職種・1,320タスクでモデルを評価し、領域専門家が成果物を人間とAIでブラインド比較して批評・ランキング化、「優れている・同等・劣っている」で判定する枠組みにより、アラインメント到達度を実務に近い形で把握します[8]。

運用段階でも、アラインメント技術は品質を実装・監視する基盤を提供します。Agents SDKはステップレベルのトレースと監視、実行リプレイ、ツール呼び出しの検査や即時デバッグを可能にし、Responses APIはトーン・正確さ・ポリシー遵守の分類を強化、Realtime APIは音声サポートを提供します。評価ダッシュボードで品質を可視化し、サポート担当者がインタラクションにフラグを立ててテストケース化、新しいパターンへの分類器提案や軽量オートメーションの試作を行います。社内では「サポートエージェント」などのAIエージェントと継続的評価・動的知識ループを運用し、すべてのインタラクションを学習データに変換して品質向上へ結びつけています[7][9]。

アラインメント技術は、これらの技術とプロセスを重ね合わせた多層のアラインメント・スタックを進化させ、実証に基づく改善、外部評価との協働、運用での継続的監視を通じて、安全で信頼できるモデルの開発・展開を推進します[1][3][4][5]。

【出典】
[1] https://openai.com/ja-JP/index/ai-progress-and-recommendations/
[2] https://openai.com/ja-JP/index/instruction-following/
[3] https://openai.com/ja-JP/index/strengthening-safety-with-external-testing/
[4] https://openai.com/ja-JP/index/sharing-the-latest-model-spec/
[5] https://openai.com/ja-JP/index/how-confessions-can-keep-language-models-honest/
[6] https://openai.com/ja-JP/index/doppel/
[7] https://openai.com/ja-JP/index/openai-support-model/
[8] https://openai.com/ja-JP/index/gdpval/
[9] https://openai.com/ja-JP/index/building-openai-with-openai/


## デュアルユース質問への安全な応答
デュアルユース質問への安全な応答は、安全性と有用性の両立を基本方針として、文脈と意図に応じた最適な応答を設計・検証し続けます。デュアルユース質問への安全な応答は、この原則をModel Specに明示し、違法行為の助長は拒否しつつ、正当な目的のための予防や一般的情報には前向きに応じる運用を徹底します。例えば、万引きを助長する依頼は拒否する一方、小売店主の予防策の相談には一般的な注意点を具体的に示すのが理想的な応答です。同じ知識でも提示の観点により結果が似通う可能性があるため、誤用リスクに注意し、利用規約に基づく対応も含めて安全な運用を行います[2][8]。

デュアルユース質問への安全な応答は、従来の「拒否中心」を超えるため、GPT‑5で「セーフコンプリーション」を導入しました。これは安全域を逸脱しない範囲で可能な限り役立つ回答を返す新しい安全性学習手法で、必要に応じて質問の一部に限定して答えたり、抽象度を上げて説明したりします。拒否が必要な場合には理由を明確に述べ、安全な代替案を提示するよう学習しており、管理下での実験と本番環境の両方で、デュアルユースの問いに対するきめ細かな対応、意図があいまいな場合の頑健性向上、不必要な過剰拒否の減少が確認されています。たとえばウイルス学のような高リスク領域では、詳細に踏み込まず概要レベルの説明にとどめる運用を行い、危機対応領域にもこの原則を展開して、必要に応じて部分的・おおまかな回答へ切り替える設計を採用しています[1][9]。

デュアルユース質問への安全な応答は、ルールに基づく振る舞いの明確化にも取り組み、デリケートな話題に対する望ましい応答タイプを「断固とした拒否」「柔らかい拒否」「遵守」の3つに整理しました。拒否の際には「短い謝罪」と「従えない旨の明確な伝達」を含め、冗長な表現や批判的な言葉を避ける運用ルールを定義しています。さらに、RLHFの限界を補う「ルールベース報酬（RBR）」で、各リクエストを上記3タイプにマッピングして学習し、拒否の質を一定に保ちながら過剰拒否を抑制し、安全性と有用性のバランスを継続的に改善しています[3]。

デュアルユース質問への安全な応答は、推論に基づく安全性の強化にも注力しています。主要な推論モデルに安全性ポリシーを直接学習させ、何が安全かを推論する「熟慮的アライメント」により、文脈依存の柔軟で説明可能な安全判断を実現します。準備態勢のフレームワークに基づくレッドチーミングと評価を経て、有害プロンプトの難例で安全なコンプリーション率が大幅に向上するなど、推論ベースのアプローチの有効性を確認しています[10][6]。加えて、開発者が自ら定めたポリシーを推論時に適用できるオープンウェイトの安全性分類モデル「gpt‑oss‑safeguard」を研究プレビューとして公開し、説明可能な安全性推論と柔軟なポリシー適用を可能にしました。これは安全性ポリシー以外のラベリングにも応用でき、プロダクト固有の安全基準の実装を支援します[4][5]。

デュアルユース質問への安全な応答は、センシティブな会話領域でも、Model Specの原則に沿って安全で共感的な応答の改善を進めています。メンタルヘルスや自傷・自殺、AIへの感情的依存といった領域で、問題の定義、測定、専門家による検証、リスク軽減というプロセスに基づく改善を重ね、現実世界の関係性を尊重し、根拠のない信念を肯定せず、間接的なリスクシグナルにも注意を払う設計を継続しています。これらの枠組みは、意図や文脈の微細な違いに応じた応答設計という点で、デュアルユース質問への安全な対応にも通底しています[7][2]。

デュアルユース質問への安全な応答は、モデルレベルとプロダクトレベルの双方で安全対策を重ねる多層的アプローチを採用します。システムカードで安全性の前提・評価・緩和策を公開し、GPT‑5‑Codexのアドエンダムでは、有害タスクやプロンプトインジェクションに対する特化した安全学習、エージェントのサンドボックス化やネットワークアクセスの設定可能化など、製品側の緩和策も併せて明示しています。Soraでは、事前学習データのフィルタリング、社内外レッドチーミングによるリスク発見と反復緩和、段階的な提供、顔写真・動画の使用制限、未成年関連プロンプトの厳格なモデレーションなど、文脈依存の新たなリスクに対して段階的な安全アプローチを運用しています。データフィルタリングからアルファ/ベータ段階での展開、広範なフィードバック収集まで、ライフサイクル全体で反復的に改善を続けます[6][11]。

デュアルユース質問への安全な応答は、使用に関するポリシーを通じて「安全性を確保しながら、有用性と自由度を最大限に高める」ことを約束し、開発者向けのモデレーションツールとガイダンス、監視と執行の明確なプロセス、研究成果と最新情報の共有、不正利用の報告手段を提供します。新しい情報やユースケースに合わせてポリシーを更新し、過度な制限を避けながらユーザーをより適切に保護します。また、開発者・ユーザーの指示関係の原則や、同一知識の別視点提示が誤用に繋がり得る点への注意など、実装可能なガイドラインを透明性高く示し、社会からのフィードバックを取り入れながら継続的に改善します[8][2]。

デュアルユース質問への安全な応答は、セーフコンプリーション、ルールに基づく応答設計、推論ベースの安全性、Model Specの公開、システムカードによる透明性、多層的な緩和策、そして継続的なポリシー運用を組み合わせることで、デュアルユースの質問に対して安全かつ有用な応答を提供し続けます[1][2][3][4][6][8]。

【出典】
[1] https://openai.com/ja-JP/index/introducing-gpt-5/
[2] https://openai.com/ja-JP/index/introducing-the-model-spec/
[3] https://openai.com/ja-JP/index/improving-model-safety-behavior-with-rule-based-rewards/
[4] https://openai.com/ja-JP/index/introducing-gpt-oss-safeguard/
[5] https://openai.com/ja-JP/index/gpt-oss-safeguard-technical-report/
[6] https://openai.com/ja-JP/safety/
[7] https://openai.com/ja-JP/index/strengthening-chatgpt-responses-in-sensitive-conversations/
[8] https://openai.com/ja-JP/policies/usage-policies/
[9] https://openai.com/ja-JP/index/helping-people-when-they-need-it-most/
[10] https://openai.com/ja-JP/index/learning-to-reason-with-llms/
[11] https://openai.com/ja-JP/index/sora-system-card/


## ドメイン協働による安全性改善（心理健康）
ドメイン協働による安全性改善（心理健康）は、メンタルヘルスや自傷・自殺、AIへの感情的依存といったセンシティブな領域における安全性を最優先事項とし、ドメインの専門家との継続的な協働によってモデルのふるまいを計画的に改善しています。直近のモデル更新では、精神疾患や自傷・自殺、AI依存に関する対応を重点強化し、将来のモデルに向けて「感情的依存」や「自殺を伴わない精神衛生上の緊急事態」を安全性テストの標準ベースラインに追加します。あわせて、Model Specを改訂し、現実世界の人間関係を尊重し、根拠のない信念を肯定せず、妄想や躁状態などの兆候には安全かつ共感的に応答し、間接的な自傷・自殺リスクのシグナルにも一層配慮するという原則を明確化しました。これらの改善は、「問題の定義」「測定」「アプローチの検証（外部のメンタルヘルス・安全専門家によるレビュー）」「リスクの軽減」という手順に基づいて進めており、2025年10月のGPT‑5更新ではこの領域での不適切な応答を65〜80%削減し、準拠率を大幅に引き上げる成果を上げています。この取り組みには精神科医など170名以上の専門家が参画しました[1]。

ドメイン協働による安全性改善（心理健康）は、ChatGPTに多層の安全対策を組み込み、危機の兆候を検出した際の応答を体系化しています。モデルは自傷の指示を提供せず、ユーザーの感情を受け止めながら支援につなぐ親身な言葉へと転換するよう学習しており、当社の「徹底的な防御」アプローチにもとづく分類により安全性に反する応答は自動的にブロックします。未成年に対してはより強力な保護を適用し、必要に応じて利用を停止（ログアウト）するほか、自傷行為の画像出力はすべてのユーザーでブロックしています。さらに、非常に長いセッションでは休憩を促し、誰かが自殺の意図を示した場合には、米国の988、英国のSamaritans、その他の地域ではfindahelpline.comといった実世界の支援先を案内するロジックをモデルに実装しています。これらは、30カ国以上の地域にまたがる90名超の精神科医、小児科医、一般開業医らと緊密に連携し、最新の研究とベストプラクティスを反映して設計しています[2]。

ドメイン協働による安全性改善（心理健康）は、プロダクトのガードレール設計や青少年対応の強化を目的に、ウェルビーイングとAIの関係に通じた8名の専門家からなる「ウェルビーイングとAIに関する専門家評議会」を設立しました。同評議会は、ティーンの心身の不調が疑われる際の保護者への通知設計や、ペアレンタルコントロール導入時に優先すべき機能選定などを助言し、そのフィードバックを踏まえて通知の表現を見直すなど、ティーンと家族双方に配慮した改善を進めています。これと並行して、精神科医、心理学者、小児科医、危機介入の専門家などからなるグローバル医師ネットワークとも協働し、モデルの振る舞い・ポリシー策定・実状に即した応答検証を行うことで、臨床的知見とベストプラクティスに基づく設計を推進しています[3]。

ドメイン協働による安全性改善（心理健康）は、プロダクトの安全対策と並行して、使用に関するポリシー面からも明確な基準を敷いています。自殺・自傷行為・摂食障害の助長や促進を厳に禁じるとともに、有資格者の適切な関与なく資格を要する個別の医療助言を提供する行為を禁止し、最新の情報に応じた規則の更新、簡便な不正利用報告の手段、必要に応じたアクセス停止や異議申し立てのプロセスを整備することで、過度な制限を避けつつユーザー保護を両立します[4]。

ドメイン協働による安全性改善（心理健康）は、安全性を一度きりの到達点ではなく日々の改善の連続と捉え、Preparednessフレームワークやレッドチーミング、システムカードの公開など、透明で反復的な安全プロセスを全社で運用しています。子どもの安全や個人情報、バイアスなどの論点を産業界・政策立案者と連携して牽引し、2025年9月にはペアレンタルコントロールも導入しました[5]。加えて、思考の連鎖を用いた推論モデルの研究では、安全規則とその文脈的な適用方法をモデルの思考過程に組み込むことで、人間の価値観と原則を効果的に教示し、分布外シナリオやジェイルブレイクに対しても安全性がより堅牢になることを確認しており、o1‑previewは難易度の高い安全ベンチマーク（ジェイルブレイクやエッジケース）で安全なコンプリーション率を大幅に改善しました。こうしたモデルレベルの進歩は、Preparednessフレームワークに基づく事前の安全テストとレッドチーミングでストレステストされ、困難な状況での安全な応答の向上として現れています[6]。

ドメイン協働による安全性改善（心理健康）は、ドメイン専門家との継続的な協働、実世界の保護につながるプロダクト設計、明確なポリシーと透明なガバナンス、そして研究開発の統合を通じて、メンタルヘルス領域におけるモデルの安全性を高い水準で実現し続けます。ドメイン協働による安全性改善（心理健康）は、耳を傾け、学び、検証を重ねながら、人々が最も助けを必要とする瞬間に確かな支援となるAIを提供し続けます[1][2][3][5][6]。

【出典】
[1] https://openai.com/ja-JP/index/strengthening-chatgpt-responses-in-sensitive-conversations/
[2] https://openai.com/ja-JP/index/helping-people-when-they-need-it-most/
[3] https://openai.com/ja-JP/index/expert-council-on-well-being-and-ai/
[4] https://openai.com/ja-JP/policies/usage-policies/
[5] https://openai.com/ja-JP/safety/
[6] https://openai.com/ja-JP/index/learning-to-reason-with-llms/


## モデルの安全設計・精度向上
モデルの安全設計・精度向上は、安全性を設計段階から組み込むセーフティ・バイ・デザインを軸に、モデルの有用性と安全性を同時に高める取り組みを継続します。その中核に、モデルの望ましい振る舞いを定義する包括的な行動指針「Model Spec」と、その準拠状況を測る評価プロセスを据え、公開・評価・改善のサイクルを透明に運用します[1][2][3]。

モデルの安全設計・精度向上は、Model Specを継続的に改定し、知的自由と透明性を尊重しながら、危害防止のガードレールを明確化しました。爆弾の作り方のような危険な手順は決して提供せず、政治・文化的にデリケートな問いには特定の意図を推進することなく思慮深く答えるという原則を採用しています。違法行為の助長は拒否しつつ、小売店の損失防止のような正当な文脈の相談には適切に応じる、といった実装例で有用性と安全性の両立を徹底しています[1][2]。

センシティブな会話領域では、2025年10月のモデル更新でGPT‑5の安全性を強化し、精神疾患や自傷・自殺、AIへの感情的依存に関する不適切な応答を65～80％削減しました。170名以上の精神科医等の専門家が関与し、Model Specに基づき、現実世界の人間関係を尊重し、根拠のない信念を肯定せず、妄想や躁状態の兆候に安全かつ共感的に対応し、自傷・自殺リスクを示唆する間接的シグナルにも注意を払う要件を明確化しています。自殺・自傷に関する長年の安全指標に加え、感情的依存や自傷を伴わない精神衛生上の緊急事態も安全性テストのベースラインに組み込み、問題の定義―測定―外部検証―リスク軽減というステップでChatGPTの応答品質を系統的に改善しています[4][1]。

Model Specの準拠度については、現実世界のパフォーマンスを捉えるために挑戦的な評価プロンプトを収集し、モデルの動作を継続評価しています。予備的な結果では、昨年5月時点の最高システムと比べて準拠が大幅に改善しており、更新の一部はポリシー改定に起因する可能性があるものの、多くは強化されたアライメントの成果だと評価しています。これらは継続的プロセスとして運用し、現実の利用から得られる新しいケースで課題セットを拡充します。Model SpecはCC0で公開し、一般からのフィードバックを受け付けながら、今後1年にわたり変更点や研究進捗を定期的に共有します[1][2]。

学習手法では、モデルの安全設計・精度向上はルールベース報酬（RBR）を導入してRLHFに統合しました。RBRは簡潔な命題やルールで安全行動を促し、過剰拒否を抑えつつ安全性と有用性のバランスを最適化します。モデルやガイドラインの進化に応じて大規模な再学習を伴わず迅速に更新でき、明確なルールで定義できる課題で特に有効です。主観性の高い課題では人間のフィードバックと組み合わせ、Model Specのルールもガイドラインとして活用します[5]。

システムレベルでは、モデルの安全設計・精度向上は多層防御を重視し、モデル自体の安全学習に加えて保護レイヤーを実装し、ポリシーに基づいて安全でない可能性のある入出力を検出・対処します。長年にわたり安全性クラシファイアが主要な防御層として機能しており、推論時に開発者が独自ポリシーを指定できるオープンウェイトの安全性分類モデル「gpt‑oss‑safeguard（120B/20B）」を研究プレビューとして公開、柔軟で説明可能な安全性推論を提供します。実運用では、厳格なポリシーから開始しSafety Reasonerに計算資源を配分（直近では全計算の16％）しつつ、リスク理解に応じて段階的に調整します。画像・動画生成では動的かつ段階的な出力評価で安全でない生成をリアルタイムに識別・ブロックし、生物学や自傷行為など特定領域には小型・高速・高再現率のクラシファイアを組み合わせるなど、多層防御を実装しています。オープンモデルにおいても「安全性はオープンモデルの基盤」であり、総合的な安全性テストやModel System Cardの整備を進めています[6][9][7]。

評価の透明性と継続的改善に向け、モデルの安全設計・精度向上は「Evaluations Hub」で不許可コンテンツ、ジェイルブレイク、ハルシネーション、命令階層といった評価結果の一部を公開。2025年8月15日にはGPT‑5系の結果やProduction Benchmarks、StrongRejectの詳細を更新・追加しました。モデルの高性能化と柔軟化に伴う従来評価の「飽和」に対応するため、新モダリティや新たなリスクに合わせて評価手法を定期的に更新し、System CardやPreparedness Framework、各モデルのリリース研究と併せて包括的理解を促進します[3]。

領域特化の精度と安全性について、モデルの安全設計・精度向上は医療分野のベンチマーク「HealthBench」を発表しました。60か国の262名の医師と作成した5,000の対話と4万超のルーブリックで、緊急時の案内、専門性に応じたコミュニケーション、不確実性の伝達、回答の深さ、健康データタスク等を評価し、最新モデル（o3、GPT‑4.1等）は従来モデルより大幅に改善した一方で、信頼性や文脈把握には改善余地があることも確認しました[8]。

マルチモーダル生成の安全性では、モデルの安全設計・精度向上はSoraの開発・デプロイ前にレッドチーミングで暗示的プロンプトや比喩などの敵対的手法による回避を試験し、人物を含むメディアのアップロード保護や、将来エロティック・暴力・ディープフェイクへ加工され得るコンテンツのより強力なフィルタリングの必要性を特定。プロンプトフィルタリング、ブロックリスト、分類器閾値の調整などの安全策を反復的に追加・強化しました[9]。

推論の安全性強化に向けては、思考の連鎖を活用するo1‑previewで安全規則とその文脈的な推論方法を思考過程に組み込み、主要なジェイルブレイク評価や困難な拒否境界ベンチマークで大幅な性能向上を実証しました。有害プロンプトに対する安全なコンプリーションはGPT‑4oの0.990から0.995に、困難ケースでは0.714から0.934へ改善し、デプロイ前にはPreparedness Frameworkに基づく安全性テストとレッドチーミングで報酬ハッキングの事例も観測しつつ堅牢性を検証しています[10]。

ソフトウェア開発タスクの実力測定では、モデルの安全設計・精度向上はSWE‑bench Verifiedを公開し、人間が検証した500サンプルでテスト妥当性と問題記述の明確さを保証。これによりGPT‑4oのスコアは16％から33.2％へ改善が確認され、コミュニティ主導のスキャフォールドによる外部強化の影響を考慮した継続評価の重要性を、学習前・学習中・学習後（外部統合を含む）にわたって提示しています[11]。

モデルの安全設計・精度向上は、こうした設計指針、学習手法、システム的対策、評価と公開の仕組みを統合し、コミュニティと連携しながら、モデルの安全設計と精度向上を着実に前進させます[1][2][3][4][5][6][7][8][9][10][11]。

【出典】
[1] https://openai.com/ja-JP/index/sharing-the-latest-model-spec/
[2] https://openai.com/ja-JP/index/introducing-the-model-spec/
[3] https://openai.com/ja-JP/safety/evaluations-hub/
[4] https://openai.com/ja-JP/index/strengthening-chatgpt-responses-in-sensitive-conversations/
[5] https://openai.com/ja-JP/index/improving-model-safety-behavior-with-rule-based-rewards/
[6] https://openai.com/ja-JP/index/introducing-gpt-oss-safeguard/
[7] https://openai.com/ja-JP/open-models/
[8] https://openai.com/ja-JP/index/healthbench/
[9] https://openai.com/ja-JP/index/sora-system-card/
[10] https://openai.com/ja-JP/index/learning-to-reason-with-llms/
[11] https://openai.com/ja-JP/index/introducing-swe-bench-verified/


