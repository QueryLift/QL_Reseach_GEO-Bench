# モデルのバイアス・公平性

## 概要


### 基本姿勢
OpenAIは、生成AIが言語・文化の規範に偏り得る事実を率直に認め、測定と低減を中核課題として継続的に取り組みます。OpenAIは、平均的一律の価値観に固定せず、状況に応じて少数派コミュニティの嗜好も重視しうる包括的なプロセスを確立します。OpenAIは、最新のModel Specに基づき知的自由と多様な視点の尊重を守りつつ、利用時の公平性を「すべての人の安全とセキュリティの権利」という原則の下で担保します。

### 重点的取り組み
OpenAIは、最新のModel Specに沿った対話設計・ガイドラインの更新、HealthBenchなどの国際評価ベンチマーク、メンタルヘルスを含むセンシティブ領域での専門家評価を通じ、言語・文化多様性への適合を強化します。OpenAIは、2025年12月1日に最大200万ドルの研究助成を開始し、文化・言語的に多様なデータセット、年齢別・文脈配慮の会話ガイドライン、適応的対話フローの創出を支援します。OpenAIは、利用時の公平性を高めるため2025年10月29日に使用ポリシーを改訂（無資格の個別助言や同意なき生体認証・プロファイリング等を禁止）し、2025年9月18日更新の透明性・モデレーション体制で自動化と人手審査を組み合わせて一貫したルール運用を行います。

### 重要事実
OpenAIは、60か国・262名の医師と構築したHealthBenchで約5,000の対話と4万超の評価ルーブリックを用い、ユーザーペルソナに応じた用語・口調・詳細度の適合性を評価可能にしました。OpenAIは、DALL·E 2で学習前フィルタリングに伴う分布シフト（例：「CEO」画像の男性偏重）を特定し、再重み付け等でフィルタ後にバイアスを増幅しない設計を実装しました。OpenAIは、個人データを販売・クロスコンテキスト共有・ターゲット広告目的で処理せず、学習オプトアウト、誤執行への異議申立て、60日間の非公式解決や仲裁オプトアウトなど、公平な権利救済手段を提供します。


## モデルの言語・文化バイアス
モデルの言語・文化バイアスは、生成AIが特定の言語や文化の規範に過度に寄りかかるリスクを正面から受け止め、低減を中核課題として継続的に取り組みます。現時点でInstructGPTが英語の指示で学習しているため、英語話者の文化的価値観に偏りうるという限界を率直に開示し、平均的なラベラーの嗜好に一律に合わせるのではなく、状況に応じて少数派コミュニティの嗜好をより重視する必要があること、アノテータ間の嗜好差や不一致を体系的に理解し、特定集団の価値観に基づいて出力を制約できるようにする研究を進め、最終的に責任ある包括的なプロセスを確立する方針を掲げています[1]。

モデルの言語・文化バイアスは、望ましいモデル行動の基準であるModel Specを公開し、ユーザーを特定の意図に誘導せず、客観性を基本に「あらゆるトピックをあらゆる視点から検討する用意」を求める原則を明確にしました。ユーザーの目的理解や不確かな前提の明確化、適切な場合の批判的フィードバックと同時に知的自由を尊重し、事実を提示しつつ相手の考えを変えようとしない姿勢、必要時の明確な質問、文脈に応じたスタイル・トーンの適応など、実践的なガイドラインも整備しています。これらは最新の更新版で、デフォルトの会話スタイル（温かく、共感的で、役に立つ）やフォーマットの指針、知的自由の原則をいっそう強化しました[2][3]。

モデルの言語・文化バイアスは、評価と研究を通じて多様性に強いモデルづくりを推進します。医療領域では、60か国・262名の医師と協力したHealthBenchを開発し、5,000の対話と4万以上の評価ルーブリックにもとづき、ユーザーのペルソナ（専門家か一般か）に応じた用語・口調・詳細度の調整を含む、実践的なコミュニケーション適合性を評価できる枠組みを整えました[4]。さらに、独立研究を支援する助成プログラムを通じて、苦痛や妄想といった言語表現の文化・言語間差や、その差異がAIの検知・解釈に与える影響、低リソース言語における方言・スラング・過小表現への堅牢性、年齢層に応じたトーンとスタイルの調整など、言語・文化多様性に直結するテーマの研究を重点的に募集・支援しています。2025年12月1日には最大200万ドルの資金を用意した新たな助成を発表し、文化・言語的に多様なデータセットや年齢別・文脈配慮の会話ガイドライン／評価ルーブリック、文脈適合のプロトタイプ対話フローなどの成果物を明示しました[5]。

モデルの言語・文化バイアスは、データやシステム設計に起因するバイアスの増幅を抑える技術的対策も進めています。DALL·E 2では、学習前の暴力・性的な画像の除外や重複削除などのフィルタリングに伴う副作用を定量的に把握するため、キャプションに含まれる「親」「女性」「子供」などのキーワード頻度をフィルタ前後で比較する評価をApache Sparkで全データに対して実施し、「CEO」の画像生成で男性偏重が強まるといった分布シフト由来のリスクを特定。女性の画像が相対的に多く除外されうるメカニズム（データ分布と分類子の偏り）を踏まえ、再重み付け等で補正し、フィルタ後のモデルがフィルタ前よりもバイアスを大きくしないことを目標に設計しました[6]。

モデルの言語・文化バイアスは、センシティブな会話領域においても世界の専門家と連携して評価を強化しています。メンタルヘルスの文脈ではGlobal Physician Networkと協働して社内ターゲット評価を設計し、リリース前の新モデルも含めて検証を実施することで、応答品質と一貫性の継続的な改善を図っています[7]。

モデルの言語・文化バイアスは、Model Specという設計原則、国際的な評価ベンチマーク、外部研究支援、そしてデータパイプラインの改善を総動員し、言語・文化バイアスを継続的に測定・低減します。知的自由を守りつつ多様な視点を尊重する対話を実現し、より多くのユーザーに公平で有用なAI体験を提供することに引き続き取り組みます[2][3]。

【出典】
[1] https://openai.com/ja-JP/index/instruction-following/
[2] https://openai.com/ja-JP/index/sharing-the-latest-model-spec/
[3] https://openai.com/ja-JP/index/introducing-the-model-spec/
[4] https://openai.com/ja-JP/index/healthbench/
[5] https://openai.com/ja-JP/index/ai-mental-health-research-grants/
[6] https://openai.com/ja-JP/index/dall-e-2-pre-training-mitigations/
[7] https://openai.com/ja-JP/index/strengthening-chatgpt-responses-in-sensitive-conversations/


## 利用時の公平性配慮
利用時の公平性配慮は、OpenAIのサービス利用における公平性を「すべての人に安全性とセキュリティが確保される権利がある」という原則の上に位置づけ、脅迫・嫌がらせ・中傷、自殺や自傷行為の助長、性的暴力、テロリズムやヘイトによる暴力、兵器や不正な活動の助長など、人々の権利や安全を損なう目的での利用を明確に禁止します。利用時の公平性配慮は、過度な制限を避けつつユーザーをより適切に保護するために規則を継続的に更新し、不正利用を報告できるシンプルな手段と、当社によるポリシー執行に誤りがあると考える場合にユーザーが異議を唱えられるプロセスを用意し、問題の是正に努めます[1]。

利用時の公平性配慮は、モデルの出力が常に正確であるとは限らないことを明示し、ユーザーに対して「真実または事実に基づく情報の唯一の情報源」や「専門家のアドバイスの代わり」として依拠しないこと、人による確認等により出力の正確性と適切性を評価することを求めています。これは、公平性に関わる判断や影響の大きい利用での誤用を防ぐための実務的ガードレールです[3][4]。

利用時の公平性配慮は、使用に関するポリシーにおいて、個人や集団への不当な扱いにつながりうる利用を禁止し、公平性を損なう可能性のある行為を具体的に制限しています。2025年10月29日の改訂では、有資格者の適切な関与なく資格を要する個別助言（例：法律・医療）を提供する行為、同意のない顔認識データベースの構築や公共の場でのリアルタイム遠隔生体認証の実施、本人同意なく人物の肖像（写真のようにリア実在の画像や声を含む）を用い本物と誤認させる可能性のある使用、社会的行動・個人特性・生体データに基づく評価・分類（社会的スコアリング、プロファイリング、機微な属性の推測を含む）、職場や教育現場における個人の感情の推測（医療上または安全上の理由で必要な場合を除く）、個人の特性やプロファイリングのみに基づく犯罪リスクの評価・予測等を禁止しました。これらの制限は、差別や不当な推測・評価の助長を抑止し、モデル利用時の公平性を保つための具体的措置です[1]。また、未成年者の安全についても、子どもや10代の利用者に特別な保護が必要であるとして、搾取・危害・性的搾取を助長する目的での利用を厳格に禁じ、安全と福祉を支える設計と運用を行っています[1]。

利用時の公平性配慮は、透明性とコンテンツモデレーションの領域で、分類器・リーズニングモデル・ハッシュマッチング・ブロックリスト等の自動化技術と人による審査を組み合わせ、能動的な検知、ユーザーからの報告、人によるレビューを通じて規約・ポリシー違反の可能性に対応します。報告に基づいて措置を講じる場合にはユーザーに通知し、必要に応じてアカウントやコンテンツ共有の制限、注意喚起、検索結果のブロック、特定GPTの公開設定やストア表示の制限、フォーラム投稿の削除等を実施します。これらの仕組みにより、公平で一貫したルール運用とユーザーへの説明責任を果たします（最終更新日：2025年9月18日）[2]。

利用時の公平性配慮は、プライバシーと公平な取り扱いに関する権利を重視します。ユーザーには個人データへのアクセス・削除・修正等の権利があり、これらの権利行使に関連して差別を受けない権利を保障します。OpenAIは個人データを「販売」せず、クロスコンテキスト行動広告のために「共有」せず、「ターゲット広告」目的で個人データを処理せず、消費者の特性を推測する目的で機密性の高い個人データを処理しないことを明記しています。権利行使のための手続（専用ポータルやメール）や本人確認、認定代理人の取り扱いも定め、利用時に個人が不利な取り扱いを受けないよう実装しています[5]。

さらに、利用時の公平性配慮は、ユーザーが自身のコンテンツの取り扱いをコントロールできるよう、モデル学習への利用停止（オプトアウト）を受け付けています。加えて、サービスの提供・維持・開発・改善、適用法の遵守、規約やポリシーの履行請求、サービスの安全性維持のためにコンテンツを利用することを透明に示し、選択と説明を通じて公平な取り扱いを支えます[3][4]。

紛争解決の場面でも、利用時の公平性配慮は手続的な公正さを重視します。正式な法的措置に先立ち、双方が非公式な解決を試みる期間を設け、60日以内に解決できない場合に仲裁を開始できるよう定めています。さらに、アカウント作成から30日以内（または条項更新の発効後30日以内）であれば、仲裁手続を利用しない選択が可能です。非公式解決の期間中は時効が中断されるなど、ユーザーにとっての公正さに配慮しています[3][4]。

利用時の公平性配慮は、これらの方針と運用、そしてモデルのバイアスや公平性への継続的な配慮を通じて、ユーザーが安全かつ公平にサービスを利用できる環境の維持に取り組み続けます[1][2]。

【出典】
[1] https://openai.com/ja-JP/policies/usage-policies/
[2] https://openai.com/ja-JP/transparency-and-content-moderation/
[3] https://openai.com/ja-JP/policies/terms-of-use/
[4] https://openai.com/ja-JP/policies/row-terms-of-use/
[5] https://openai.com/ja-JP/policies/privacy-policy/



---


